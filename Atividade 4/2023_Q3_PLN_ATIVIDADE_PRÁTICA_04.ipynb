{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "22bc86fb47924e77937b06939c2117a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_90494973e7f340cb80dd05d2d01d8b54",
              "IPY_MODEL_dacea7ee31fd465887130a52c5b7e33b",
              "IPY_MODEL_fda89f2561a34dbd8cfc1adcc28d7644"
            ],
            "layout": "IPY_MODEL_f7ab490aefaf467e83d1fb9d4bb0cd16"
          }
        },
        "90494973e7f340cb80dd05d2d01d8b54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d0debbefa24481db0cbb9b5f8f24ae2",
            "placeholder": "​",
            "style": "IPY_MODEL_20d76e4bcb5f4c0e86828d862be76a70",
            "value": "100%"
          }
        },
        "dacea7ee31fd465887130a52c5b7e33b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_676282eb36dd41ab9925aef557224cd5",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_979a4b6d55674c6699873c73d13c1fd9",
            "value": 2
          }
        },
        "fda89f2561a34dbd8cfc1adcc28d7644": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a291e45ef6204c42a2f77994483cdc72",
            "placeholder": "​",
            "style": "IPY_MODEL_b5e1dd43f74b4a25b0a2813bc2a933ab",
            "value": " 2/2 [00:00&lt;00:00,  2.68it/s]"
          }
        },
        "f7ab490aefaf467e83d1fb9d4bb0cd16": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d0debbefa24481db0cbb9b5f8f24ae2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20d76e4bcb5f4c0e86828d862be76a70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "676282eb36dd41ab9925aef557224cd5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "979a4b6d55674c6699873c73d13c1fd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a291e45ef6204c42a2f77994483cdc72": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5e1dd43f74b4a25b0a2813bc2a933ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "122b83314ad14697b70563c99322c610": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_03f21dd58db24f169169aa9f8b051124",
              "IPY_MODEL_1ca0eae33f4e44b1bd13e35093a2dfb2",
              "IPY_MODEL_9cf98ca9063e4c6d979f9654e36f16f0"
            ],
            "layout": "IPY_MODEL_3d42d3ce9e35434a8ff28f6c5b82103a"
          }
        },
        "03f21dd58db24f169169aa9f8b051124": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca33f5667d064433b7906c3723dc0e26",
            "placeholder": "​",
            "style": "IPY_MODEL_7f391c1fd72f4dba8bfa4a80c6788113",
            "value": "100%"
          }
        },
        "1ca0eae33f4e44b1bd13e35093a2dfb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d103e94fb16842bd91d99e645113bfb3",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_19a4f31f5f144e208e86149268643bd5",
            "value": 5
          }
        },
        "9cf98ca9063e4c6d979f9654e36f16f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_869d90263a7047a69a07183fc35fcc37",
            "placeholder": "​",
            "style": "IPY_MODEL_e99c76528d9f41859927361c25fd2156",
            "value": " 5/5 [01:07&lt;00:00, 17.00s/it]"
          }
        },
        "3d42d3ce9e35434a8ff28f6c5b82103a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca33f5667d064433b7906c3723dc0e26": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f391c1fd72f4dba8bfa4a80c6788113": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d103e94fb16842bd91d99e645113bfb3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19a4f31f5f144e208e86149268643bd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "869d90263a7047a69a07183fc35fcc37": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e99c76528d9f41859927361c25fd2156": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "abad2a3d42404651ac42cdde9d9848f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2f4911dce5f64f32842291524e3bfdf1",
              "IPY_MODEL_b55a988e4ad04442b278444509724531",
              "IPY_MODEL_ea6f94aa5e8149f9b1c5640f9a29ce47"
            ],
            "layout": "IPY_MODEL_8f3c9781566a49a0b43271addb418a27"
          }
        },
        "2f4911dce5f64f32842291524e3bfdf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58105d38560e47cfa329015e5f5e8693",
            "placeholder": "​",
            "style": "IPY_MODEL_b860beafd56a42c4817392a725b4ea3a",
            "value": "100%"
          }
        },
        "b55a988e4ad04442b278444509724531": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bbad4ef134094b14b907b3d8541c2e00",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_538debd46ffa4890a1e59000d95589a8",
            "value": 1
          }
        },
        "ea6f94aa5e8149f9b1c5640f9a29ce47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74be88abe6ed4c61a2be56f268436375",
            "placeholder": "​",
            "style": "IPY_MODEL_ee1471925c534a4da28405d2df144d4b",
            "value": " 1/1 [00:00&lt;00:00,  2.97it/s]"
          }
        },
        "8f3c9781566a49a0b43271addb418a27": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58105d38560e47cfa329015e5f5e8693": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b860beafd56a42c4817392a725b4ea3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bbad4ef134094b14b907b3d8541c2e00": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "538debd46ffa4890a1e59000d95589a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "74be88abe6ed4c61a2be56f268436375": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee1471925c534a4da28405d2df144d4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c6f4762a30904c8d9a3d53c88a129732": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0dd37bd71fea47409af806d2bb271183",
              "IPY_MODEL_e8d5440ddaf54122b4b3c97957be705b",
              "IPY_MODEL_ef31a2c9646d477996d054c3a7b4db88"
            ],
            "layout": "IPY_MODEL_6297ae9c540c49c48960843cf65afbd7"
          }
        },
        "0dd37bd71fea47409af806d2bb271183": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8cf419864fee42cabef2977f8e0b6ad7",
            "placeholder": "​",
            "style": "IPY_MODEL_5ead95d23be848cdac099d1b5ecfddb1",
            "value": "100%"
          }
        },
        "e8d5440ddaf54122b4b3c97957be705b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7779d3c9cbe64f6cb31ab2b487b4d34a",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_62cdb5019ab34d4a95284ef6fbda73af",
            "value": 1
          }
        },
        "ef31a2c9646d477996d054c3a7b4db88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_23c024a2f0db485f8dd287e7c608f319",
            "placeholder": "​",
            "style": "IPY_MODEL_ef4a50c0f9cd4a32957ddb0648c87349",
            "value": " 1/1 [00:00&lt;00:00,  6.83it/s]"
          }
        },
        "6297ae9c540c49c48960843cf65afbd7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8cf419864fee42cabef2977f8e0b6ad7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ead95d23be848cdac099d1b5ecfddb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7779d3c9cbe64f6cb31ab2b487b4d34a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62cdb5019ab34d4a95284ef6fbda73af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "23c024a2f0db485f8dd287e7c608f319": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef4a50c0f9cd4a32957ddb0648c87349": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1203f073b78340e783587e02f5693630": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_272a7738021c43b988e49887563de59c",
              "IPY_MODEL_29b5924b04b04855a0ba0268eaba0cc9",
              "IPY_MODEL_f4124d5dd78c499ea4f31b4553b20473"
            ],
            "layout": "IPY_MODEL_65f071fa699a4a4ca18374c422e102a5"
          }
        },
        "272a7738021c43b988e49887563de59c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf34ae246ec94a599ae14ad76fb7badf",
            "placeholder": "​",
            "style": "IPY_MODEL_693430a580b447a497abbdf030f33fc9",
            "value": "100%"
          }
        },
        "29b5924b04b04855a0ba0268eaba0cc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17373778ef874fc0ba26e6643d0f0c3e",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_87976fa2c43f4289ab233db5a6f1b61b",
            "value": 6
          }
        },
        "f4124d5dd78c499ea4f31b4553b20473": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c5181531d88b480f9c2d2631ebb6fe61",
            "placeholder": "​",
            "style": "IPY_MODEL_9ed5da709fa342e7b9c8e2d0b82950e4",
            "value": " 6/6 [01:03&lt;00:00, 15.56s/it]"
          }
        },
        "65f071fa699a4a4ca18374c422e102a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf34ae246ec94a599ae14ad76fb7badf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "693430a580b447a497abbdf030f33fc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "17373778ef874fc0ba26e6643d0f0c3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87976fa2c43f4289ab233db5a6f1b61b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c5181531d88b480f9c2d2631ebb6fe61": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ed5da709fa342e7b9c8e2d0b82950e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6c42167995cf48e18a3d0f388148c7d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_88c5c1cca5514f38a94195de6821413e",
              "IPY_MODEL_76667aee4c184eee8b94931062a5bf5f",
              "IPY_MODEL_ef78c533a8c44d7089f633ab5794dfa3"
            ],
            "layout": "IPY_MODEL_200b1f0dd9ba4c338059a8f84dfb54d6"
          }
        },
        "88c5c1cca5514f38a94195de6821413e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4472d54a10934846a2fdff87214ba7c6",
            "placeholder": "​",
            "style": "IPY_MODEL_8d3bf3bc33014abd8e83c719f6f7207a",
            "value": "100%"
          }
        },
        "76667aee4c184eee8b94931062a5bf5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4435563aedf8487da222a8db99e85dce",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ce75788c60a54fb08ee3f49315d51ab4",
            "value": 1
          }
        },
        "ef78c533a8c44d7089f633ab5794dfa3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c85dfc656f04c009ecfeb146dbfd4f4",
            "placeholder": "​",
            "style": "IPY_MODEL_ab526ae921614d44865c66de619c0af6",
            "value": " 1/1 [00:00&lt;00:00,  2.70it/s]"
          }
        },
        "200b1f0dd9ba4c338059a8f84dfb54d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4472d54a10934846a2fdff87214ba7c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d3bf3bc33014abd8e83c719f6f7207a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4435563aedf8487da222a8db99e85dce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce75788c60a54fb08ee3f49315d51ab4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8c85dfc656f04c009ecfeb146dbfd4f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab526ae921614d44865c66de619c0af6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9fa4d15156764615886a15000afef4f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d8474f39315b45d2af6e8914ecad99d6",
              "IPY_MODEL_0404f0d7f47245599b16ad103153bfbc",
              "IPY_MODEL_723f7ff061cf4c84aa9e3a65693e81dd"
            ],
            "layout": "IPY_MODEL_6e9b86f9a60443a3aa6619f9f4d993b8"
          }
        },
        "d8474f39315b45d2af6e8914ecad99d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af672877381e4f729843af6408d3f6a3",
            "placeholder": "​",
            "style": "IPY_MODEL_3e887487c39f48cb9fcbae55ea74448e",
            "value": "100%"
          }
        },
        "0404f0d7f47245599b16ad103153bfbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1dcd20492f9c404093618bfd7bf1c5a3",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_445db3bda18a4db3ace04f162f86e03d",
            "value": 1
          }
        },
        "723f7ff061cf4c84aa9e3a65693e81dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f36f13adde324d5c84af416f0aeb86e7",
            "placeholder": "​",
            "style": "IPY_MODEL_9862070d900b4977870a858c89da8d03",
            "value": " 1/1 [00:00&lt;00:00,  6.49it/s]"
          }
        },
        "6e9b86f9a60443a3aa6619f9f4d993b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af672877381e4f729843af6408d3f6a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e887487c39f48cb9fcbae55ea74448e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1dcd20492f9c404093618bfd7bf1c5a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "445db3bda18a4db3ace04f162f86e03d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f36f13adde324d5c84af416f0aeb86e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9862070d900b4977870a858c89da8d03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e8f5c6b164cd4adeac01d8c9d33b9097": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_886f7ee48b844517a582ba969e054e07",
              "IPY_MODEL_9d4a46e7c1ac483ca7c90ebb04789e0f",
              "IPY_MODEL_89fb3c53d72d4d2b89163cf00383d58e"
            ],
            "layout": "IPY_MODEL_632809c6b83e4d9fb988ace282fa43fd"
          }
        },
        "886f7ee48b844517a582ba969e054e07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e1ad40472ffd43cfbf806fc77005d5b4",
            "placeholder": "​",
            "style": "IPY_MODEL_242f217e45cc4cb99a2661e80973646c",
            "value": "100%"
          }
        },
        "9d4a46e7c1ac483ca7c90ebb04789e0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_05045ccec19347369f5af9f1f98d5a9e",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c22167bbf50b4e89afc553f03ba22814",
            "value": 1
          }
        },
        "89fb3c53d72d4d2b89163cf00383d58e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a5b00bcf4c2e4f94951afa9e917748b1",
            "placeholder": "​",
            "style": "IPY_MODEL_76407c53c741405aaf4c483e5fcf6b54",
            "value": " 1/1 [00:00&lt;00:00,  2.35it/s]"
          }
        },
        "632809c6b83e4d9fb988ace282fa43fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1ad40472ffd43cfbf806fc77005d5b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "242f217e45cc4cb99a2661e80973646c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "05045ccec19347369f5af9f1f98d5a9e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c22167bbf50b4e89afc553f03ba22814": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a5b00bcf4c2e4f94951afa9e917748b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76407c53c741405aaf4c483e5fcf6b54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pedrovmjm/PLN/blob/main/Atividade%204/2023_Q3_PLN_ATIVIDADE_PR%C3%81TICA_04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6QILOdpOjwv"
      },
      "source": [
        "# **Processamento de Linguagem Natural [2023.Q3]**\n",
        "Prof. Alexandre Donizeti Alves"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8m67OOx9MX_3"
      },
      "source": [
        "### **ATIVIDADE PRÁTICA 04 [Uso da API da OpenAI com técnicas de PLN]**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Gk0nHKabBT-"
      },
      "source": [
        "A **ATIVIDADE PRÁTICA 04** deve ser feita utilizando o **Google Colab** com uma conta sua vinculada ao Gmail. O link do seu notebook, armazenado no Google Drive, além do link de um repositório no GitHub e os principais resultados da atividade, devem ser enviados usando o seguinte formulário:\n",
        "\n",
        "> https://forms.gle/GzwCq3R7ExtE9g9a8\n",
        "\n",
        "\n",
        "**IMPORTANTE**: A submissão deve ser feita até o dia 20/11 (segunda-feira) APENAS POR UM INTEGRANTE DA EQUIPE, até às 23h59. Por favor, lembre-se de dar permissão de ACESSO IRRESTRITO para o professor da disciplina de PLN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7hJlilKM485"
      },
      "source": [
        "### **EQUIPE**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**POR FAVOR, PREENCHER OS INTEGRANDES DA SUA EQUIPE:**\n",
        "\n",
        "\n",
        "**Integrante 01:**\n",
        "\n",
        "`Por favor, informe o seu nome completo e RA:`\n",
        "Pedro Victor Marcelino Jordão Motta RA: 11201921599\n",
        "\n",
        "**Integrante 02:**\n",
        "\n",
        "`Por favor, informe o seu nome completo e RA:`\n",
        "Luccas Vinicius de Faveri Tortorelli Cardoso 11201920991"
      ],
      "metadata": {
        "id": "tnIArN0QY-Ek"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **LIVRO**\n",
        "---"
      ],
      "metadata": {
        "id": "6yExhaebs-nD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Processamento de Linguagem Natural - Conceitos, Técnicas e Aplicações em Português.`\n",
        "\n",
        ">\n",
        "\n",
        "Disponível gratuitamente em:\n",
        "  \n",
        "  > https://brasileiraspln.com/livro-pln/1a-edicao/.\n",
        "\n",
        "\n",
        "**POR FAVOR, PREENCHER OS CAPITULOS SELECIONADOS PARA A SUA EQUIPE:**\n",
        "\n",
        "`Primeiro capítulo: ` 8\n",
        "\n",
        "`Segundo capítulo:` 18\n",
        "\n"
      ],
      "metadata": {
        "id": "DjJM_qhEZRy6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtjgWQRzNphL"
      },
      "source": [
        "### **DESCRIÇÃO**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementar um `notebook` no `Google Colab` que faça uso da **API da OpenAI** aplicando, no mínimo, 3 técnicas de PLN. As técnicas devem ser aplicadas nos 2 (DOIS) capítulos do livro **Processamento de Linguagem Natural - Conceitos, Técnicas e Aplicações em Português**.\n",
        "\n",
        ">\n",
        "\n",
        "**RESTRIÇÃO**: É obrigatório usar o *endpoint* \"*`Chat Completions`*\".\n",
        "\n",
        ">\n",
        "\n",
        "As seguintes técnicas de PLN podem ser usadas:\n",
        "\n",
        "*   Correção Gramatical\n",
        "*   Classificação de Textos\n",
        "*   Análise de Sentimentos\n",
        "*   Detecção de Emoções\n",
        "*   Extração de Palavras-chave\n",
        "*   Tradução de Textos\n",
        "*   Sumarização de Textos\n",
        "*   **Similaridade de Textos**\n",
        "*   **Reconhecimento de Entidades Nomeadas**\n",
        "*   **Sistemas de Perguntas e Respostas**\n",
        "\n",
        ">\n",
        "\n",
        "Os capítulos devem ser os mesmos selecionados na **ATIVIDADE PRÁTICA 02**. Para consultar os capítulos, considere a seguinte planilha:\n",
        "\n",
        ">\n",
        "\n",
        "> https://docs.google.com/spreadsheets/d/1ZutzQ3v1OJgsgzCvCwxXlRIQ3ChXNlHNvB63JQvYsbo/edit?usp=sharing\n",
        "\n",
        ">\n",
        ">\n",
        "\n",
        "**IMPORTANTE:** É obrigatório usar o e-mail da UFABC. Não é permitido alterar os capítulos já selecionados.\n",
        "\n"
      ],
      "metadata": {
        "id": "fXTwkiiGs2BV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **CRITÉRIOS DE AVALIAÇÃO**\n",
        "---\n"
      ],
      "metadata": {
        "id": "gWsBYQNtxmum"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Serão considerados como critérios de avaliação as técnicas usadas e a criatividade envolvida na aplicação das mesmas.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5iHdx4BXYruQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **IMPLEMENTAÇÃO**\n",
        "---"
      ],
      "metadata": {
        "id": "nw09lujGvfjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Instalação de Libs**"
      ],
      "metadata": {
        "id": "PHibEXVgfagg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalação da biblioteca Faiss para operações eficientes de busca e análise em conjuntos grandes de vetores.\n",
        "# A versão '-cpu' indica que a instalação é para CPUs, apropriada quando não se utiliza aceleração de hardware especializada, como GPUs.\n",
        "!pip install faiss-cpu\n"
      ],
      "metadata": {
        "id": "RyUailD5vi9E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16172df6-096e-485a-fcf1-ea9b6affeca1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.7.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalação da versão específica 0.28.1 da biblioteca OpenAI.\n",
        "# Fixar a versão pode ser útil para manter consistência em projetos e evitar problemas de compatibilidade.\n",
        "!pip install openai==0.28.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHbOJOPfGZuh",
        "outputId": "53722ecc-0d03-4606-ea6a-038b2563c28a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai==0.28.1 in /usr/local/lib/python3.10/dist-packages (0.28.1)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28.1) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28.1) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28.1) (3.8.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.1) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.1) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalação da versão específica 0.0.329 da biblioteca Langchain.\n",
        "# Fixar a versão pode ser útil para manter consistência em projetos e evitar problemas de compatibilidade.\n",
        "!pip install langchain==0.0.329\n"
      ],
      "metadata": {
        "id": "YnD7Jb1Yk1o6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a1c66d6-24ec-4711-ea77-8196766c3cc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain==0.0.329 in /usr/local/lib/python3.10/dist-packages (0.0.329)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.329) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.329) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.329) (3.8.6)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.329) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.329) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.329) (0.6.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.329) (1.33)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.52 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.329) (0.0.63)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.329) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.329) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.329) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.329) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.329) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.329) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.329) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.329) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.329) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.329) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain==0.0.329) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain==0.0.329) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain==0.0.329) (1.1.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.329) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.329) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain==0.0.329) (2.4)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.0.329) (4.5.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.329) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.329) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.329) (3.0.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.329) (23.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.329) (1.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalação da biblioteca tiktoken, que fornece uma contagem de tokens para textos no formato do OpenAI GPT.\n",
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QeGqr0mfunRK",
        "outputId": "1bbf427a-1d69-4ca5-9f65-568b56caa18d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.5.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Importação de todas as Libs para a tarefa**"
      ],
      "metadata": {
        "id": "5nhATSWffqiA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importação de bibliotecas necessárias para o script.\n",
        "from bs4 import BeautifulSoup  # Para realizar o parsing de HTML.\n",
        "import requests  # Para fazer requisições HTTP.\n",
        "import re  # Para manipulação de expressões regulares.\n",
        "import time  # Para manipulação de tempo.\n",
        "import json  # Para manipulação de dados em formato JSON.\n",
        "import openai  # Biblioteca OpenAI para tarefas de linguagem natural.\n",
        "from google.colab import files  # Para manipulação de arquivos no ambiente Google Colab.\n",
        "import textwrap  # Para facilitar a formatação de texto.\n",
        "from langchain.text_splitter import CharacterTextSplitter  # Módulo para dividir texto em caracteres.\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings  # Módulo para incorporar vetores utilizando OpenAI.\n",
        "from langchain.vectorstores import FAISS  # Módulo para armazenamento eficiente de vetores utilizando FAISS.\n",
        "from langchain.llms import OpenAI  # Módulo para modelos de linguagem utilizando OpenAI.\n",
        "import httpx  # Cliente HTTP assíncrono."
      ],
      "metadata": {
        "id": "bjjMDnq7GcbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Definindo a chave da API**"
      ],
      "metadata": {
        "id": "qsYudP9qf0M_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fazer upload do arquivo de texto\n",
        "upload_arquivo = files.upload()\n",
        "\n",
        "# obter o nome do arquivo\n",
        "nome_arquivo = list(upload_arquivo.keys())[0]\n",
        "\n",
        "# ler o conteúdo do arquivo\n",
        "with open(nome_arquivo, 'r') as file:\n",
        "   chave_api = file.read()\n",
        "\n",
        "# definir a chave da API\n",
        "openai.api_key = chave_api"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "-2F9Z5JzLaZP",
        "outputId": "239abca5-9d4a-4ced-8923-28b8c9760c75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ba4590f2-861b-42c0-a6cb-c1643a0d3b5b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ba4590f2-861b-42c0-a6cb-c1643a0d3b5b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving apiOpenai.txt to apiOpenai (2).txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Funções para a tarefa de resumir**"
      ],
      "metadata": {
        "id": "rtgYwIilf7n8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definição da função para obter texto de um item.\n",
        "def getText(item):\n",
        "    # Mensagem de início da captura do capítulo.\n",
        "    print(f\"Iniciando a captura do capítulo: {item[0]}\")\n",
        "\n",
        "    # Faz uma requisição HTTP para a URL do item e obtém o conteúdo HTML.\n",
        "    response = requests.get(item[1])\n",
        "\n",
        "    # Utiliza BeautifulSoup para fazer o parsing do conteúdo HTML.\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Encontra o elemento principal no HTML com a classe 'content' e o id 'quarto-document-content'.\n",
        "    main_element = soup.find('main', {'class': 'content', 'id': 'quarto-document-content'})\n",
        "\n",
        "    # Verifica se o elemento principal foi encontrado.\n",
        "    if main_element:\n",
        "        # Obtém o texto do elemento principal em minúsculas e sem normalização.\n",
        "        texto = main_element.get_text().lower()\n",
        "        textoOffNormalize = main_element.get_text()\n",
        "\n",
        "    # Mensagem indicando sucesso na conexão e coleta do texto.\n",
        "    print(\"Conexão e coleta do texto com sucesso\")\n",
        "\n",
        "    # Cria um dicionário com os dados do capítulo.\n",
        "    json_data = {\n",
        "        \"Capitulo\": item[0],\n",
        "        \"texto\": texto,\n",
        "        \"textoOffNormalize\": textoOffNormalize,\n",
        "    }\n",
        "\n",
        "    # Retorna o dicionário com os dados do capítulo.\n",
        "    return json_data\n"
      ],
      "metadata": {
        "id": "A8vOQjR7G4-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definição da função para quebrar um texto em chunks.\n",
        "def quebraTexto(text):\n",
        "    # Tamanho máximo de cada chunk (em caracteres).\n",
        "    tamanho_chunk = 16384\n",
        "    chunks = []  # Lista para armazenar os chunks resultantes.\n",
        "    chunk_atual = \"\"  # String para construir o chunk atual.\n",
        "\n",
        "    # Itera sobre as frases do texto.\n",
        "    for sentence in text.split(\".\"):\n",
        "        # Verifica se adicionar a frase atual ao chunk não excede o tamanho máximo.\n",
        "        if len(chunk_atual) + len(sentence) < tamanho_chunk:\n",
        "            chunk_atual += sentence + \".\"  # Adiciona a frase ao chunk atual.\n",
        "        else:\n",
        "            chunks.append(chunk_atual.strip())  # Adiciona o chunk atual à lista.\n",
        "            chunk_atual = sentence + \".\"  # Inicia um novo chunk com a frase atual.\n",
        "\n",
        "    # Verifica se há algum chunk não processado ao final.\n",
        "    if chunk_atual:\n",
        "        chunks.append(chunk_atual.strip())\n",
        "\n",
        "    # Retorna a lista de chunks.\n",
        "    return chunks\n"
      ],
      "metadata": {
        "id": "0udXmjAVIcz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definição da função para resumir um trecho de texto utilizando a API da OpenAI.\n",
        "def resumir_trecho(texto, indice, max_retries=3, delay_between_retries=5):\n",
        "    # Loop para realizar tentativas de resumo.\n",
        "    for tentativa in range(max_retries + 1):\n",
        "        chunk = texto  # Inicializa o chunk com o texto fornecido.\n",
        "\n",
        "        # Lista de mensagens a serem enviadas para a API da OpenAI.\n",
        "        mensagens = []\n",
        "        # Mensagem de sistema explicando o propósito da IA no resumo de texto.\n",
        "        mensagens.append({'role': 'system', 'content': \"Você é uma inteligência artificial que irá resumir pedaços de texto de um livro sobre Processamento de Linguagem Natural, portanto, você deve capturar explicações e pontos importantes para o entendimento e mantendo uma linearidade no resumo.\"})\n",
        "        # Mensagem do usuário contendo o chunk de texto a ser resumido.\n",
        "        mensagens.append({'role': 'user', 'content': chunk})\n",
        "        print(mensagens)\n",
        "\n",
        "        # Chamada à API da OpenAI para obter um resumo do texto.\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo-16k-0613\",\n",
        "            messages=mensagens,\n",
        "            max_tokens=2048,\n",
        "            temperature=0.4,\n",
        "        )\n",
        "\n",
        "        # Verifica se a resposta da API contém escolhas válidas.\n",
        "        if \"choices\" in response and response[\"choices\"]:\n",
        "            summary = response[\"choices\"][0][\"message\"][\"content\"]\n",
        "            return summary  # Retorna o resumo obtido.\n",
        "        else:\n",
        "            # Caso a tentativa não seja bem-sucedida, imprime mensagem de falha.\n",
        "            print(f\"Falha na tentativa {tentativa + 1}.\")\n",
        "            # Verifica se há mais tentativas disponíveis.\n",
        "            if tentativa < max_retries:\n",
        "                print(f\"Tentando novamente em {delay_between_retries} segundos...\")\n",
        "                time.sleep(delay_between_retries)  # Aguarda antes de tentar novamente.\n",
        "            else:\n",
        "                print(\"Número máximo de tentativas atingido. Abortando.\")\n",
        "                return None  # Retorna None caso todas as tentativas falhem.\n"
      ],
      "metadata": {
        "id": "y7TDPa4QJ5EP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definição da função para visualizar resumos de trechos.\n",
        "def visualizar_resumo(tuplaResumo):\n",
        "    largura_maxima = 200  # Largura máxima para formatação do texto.\n",
        "\n",
        "    # Itera sobre os resumos na tupla.\n",
        "    for i, value in enumerate(tuplaResumo):\n",
        "        # Quebra o texto do resumo em linhas de acordo com a largura máxima.\n",
        "        texto_quebrado = textwrap.wrap(value[1], largura_maxima)\n",
        "\n",
        "        # Imprime o número do trecho e o resumo formatado.\n",
        "        print(\"Resumo do trecho\", value[0], \":\")\n",
        "        for linha in texto_quebrado:\n",
        "            print(linha)\n",
        "        print('\\n')  # Adiciona uma linha em branco entre os resumos."
      ],
      "metadata": {
        "id": "J08p__9LNAnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Resgate do texto do capítulo 8 e 18**"
      ],
      "metadata": {
        "id": "JMZiyxhBjcXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lista de URLs a serem processadas.\n",
        "urls = [\n",
        "    (8, \"https://brasileiraspln.com/livro-pln/1a-edicao/parte5/cap8/cap8.html\"),\n",
        "    (18, \"https://brasileiraspln.com/livro-pln/1a-edicao/parte8/cap18/cap18.html\")\n",
        "]\n",
        "\n",
        "# Lista para armazenar os resultados da função getText para cada URL.\n",
        "texto = []\n",
        "\n",
        "# Itera sobre as URLs e obtém o texto de cada capítulo.\n",
        "for i in urls:\n",
        "    texto.append(getText(i))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_0cGzwGG-Jk",
        "outputId": "3a13455c-1768-4606-ef0a-087a0ed96e43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iniciando o captura do capitulo:8\n",
            "Conexão e coleta do texto com sucesso\n",
            "Iniciando o captura do capitulo:18\n",
            "Conexão e coleta do texto com sucesso\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Aplicação da Tarefa de Resumo de texto**"
      ],
      "metadata": {
        "id": "x2vA6pETjoCk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtendo os chunks do texto do capítulo 8 usando a função quebraTexto.\n",
        "chunks_8 = quebraTexto(texto[0]['textoOffNormalize'])"
      ],
      "metadata": {
        "id": "ZYsx5gITJw9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtendo os chunks do texto do capítulo 18 usando a função quebraTexto.\n",
        "chunks_18 = quebraTexto(texto[1]['textoOffNormalize'])"
      ],
      "metadata": {
        "id": "AUTFtBs7iyno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lista para armazenar os resumos do Capítulo 8.\n",
        "resumo_Capitulo_8 = []\n",
        "\n",
        "# Itera sobre os chunks do Capítulo 8 e obtém os resumos.\n",
        "for i, value in enumerate(chunks_8):\n",
        "    resumo = resumir_trecho(value, i)\n",
        "\n",
        "    # Verifica se o resumo foi bem-sucedido.\n",
        "    if resumo is not None:\n",
        "        print(f\"\\n\\nResumo do trecho {i} foi bem-sucedido. Segue o resumo: {resumo}\\n\\n\")\n",
        "        resumo_Capitulo_8.append((i, resumo))\n",
        "    else:\n",
        "        print(f\"Falha no resumo do trecho {i}. Você pode tentar novamente ou lidar com o contexto de falha.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwAjIkgPqbVf",
        "outputId": "ab3ada05-3bb4-44ce-bb57-b5d386a32bf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'role': 'system', 'content': 'Você é uma inteligência artificial que irá resumir pedaços de texto de um livro sobre Processamento de Linguagem Natural, portanto, você deve capturar explicações e pontos importantes para o entendimento e mantendo uma linearidade no resumo.'}, {'role': 'user', 'content': '8\\xa0 E o significado?\\n\\n\\n\\n\\n\\nCláudia Freitas \\n\\n\\n\\nPublicado em:\\n\\n26/09/2023\\n\\n\\n\\n\\nPDF\\n\\nSemântica lida com o sentido do que é comunicado por meio da linguagem (em oposição ao que é comunicado por imagens ou sons não verbais, por exemplo). Assim, a semântica estuda o significado de palavras e frases. Mas a simplicidade relativa ao que é semântica acaba aí. Nos estudos linguísticos, a semântica é conhecida como “um domínio de investigação de limites movediços” e para o qual não há jargões bem estabelecidos (Ilari; Geraldi, 1985, p. 6). A questão “o que é o significado de uma palavra?” (e também o de uma frase) é um dos problemas nucleares da investigação semântica, e sua resposta irá depender da perspectiva teórica adotada. Essa característica é diferente de outras áreas do conhecimento, como a zoologia, por exemplo, em que não há controvérsia sobre o que é um animal.\\nNo PLN, diferentes maneiras de conceber o significado se manifestam em diferentes abordagens para o tratamento do sentido, como veremos nos Capítulos 9 e 10.\\nMas… sabemos o que é significado e os dicionários – objetos que contêm o significado das palavras – não só existem, como são úteis. Então, por que tanta dificuldade? Por que “limites movediços”?\\nPara ilustrar essa ideia, vamos fazer uma analogia entre a observação – e descrição – do significado de uma palavra e a observação de uma onda, como narrada em “Palomar na praia”, de um livro de Ítalo Calvino. Trata-se de um capítulo curto, e alguns trechos são transcritos abaixo. A história gira em torno de alguém – Palomar – tentando conhecer algo – uma onda – de forma maximamente objetiva. Palomar, aliás, além de nome do protagonista, é o nome de um observatório astronômico que, durante muito tempo, ostentou o maior telescópio do mundo1. Vamos à história, apresentada no Quadro\\xa08.1.\\n\\nQuadro 8.1 Trechos da história “Palomar na praia” de Ítalo Calvino.\\n\\n\\n\\n\\n\\n\\n\\nO senhor Palomar está de pé na areia e observa uma onda. Não que esteja absorto na contemplação das ondas. Não está absorto, porque sabe bem o que faz: quer observar uma onda e a observa. (…). Em suma, não são “as ondas” que ele pretende observar, mas uma simples onda e pronto: no intuito de evitar as sensações vagas, ele predetermina para cada um de seus atos um objetivo limitado e preciso.\\nO senhor Palomar vê uma onda apontar na distância, crescer, aproximar-se, mudar de forma e de cor, revolver-se sobre si mesma, quebrar-se, desfazer-se. A essa altura poderia convencer-se de ter levado a cabo a operação a que se havia proposto e ir-se embora. Contudo, isolar uma onda da que se lhe segue de imediato e que parece às vezes suplantá-la ou acrescentar-se a ela e mesmo arrastá-la é algo muito difícil, assim como separá-la da onda que a precede e que parece empurrá-la em direção à praia, quando não dá até mesmo a impressão de voltar-se contra ela como se quisesse fechá-la. (…).\\nEm suma, não se pode observar uma onda sem levar em conta os aspectos complexos que concorrem para formá-la e aqueles também complexos a que essa dá ensejo. Tais aspectos variam continuamente, decorrendo daí que cada onda é diferente de outra onda; mas da mesma maneira é verdade que cada onda é igual a outra onda, mesmo quando não imediatamente contígua ou sucessiva; enfim, são formas e sequências que se repetem, ainda que distribuídas de modo irregular no espaço e no tempo. Como o que o senhor Palomar pretende fazer neste momento é simplesmente ver uma onda, ou seja, colher todos os seus componentes simultâneos sem descurar de nenhum, seu olhar se irá deter sobre o movimento da água que bate na praia a fim de poder registrar os aspectos que a princípio não havia captado (…).\\n(…) Foi uma dessas línguas baixas de areia que o senhor Palomar escolheu como ponto de observação, porque as ondas nelas batem obliquamente de uma parte e de outra, e ao cavalgarem por cima da superfície semi-submersa vão encontrar-se com as que chegam da outra parte. (…).\\nO senhor Palomar está procurando agora limitar seu campo de observação; se tem presente um quadrado de, digamos, dez metros de praia por dez metros de mar, pode levantar um inventário de todos os movimentos de ondas que ali se repetem com frequência variada dentro de um dado intervalo de tempo. A dificuldade está em fixar os limites desse quadrado, porque, por exemplo, se ele considera como o lado mais distante de si a linha em relevo de uma onda que avança, essa linha ao aproximar-se dele irá, erguendo-se, ocultar de sua vista tudo o que está atrás; e eis que o espaço tomado para exame se destaca e ao mesmo tempo se comprime. (…)\\nContudo, o senhor Palomar não perde o ânimo e a cada momento acredita haver conseguido observar tudo o que poderia ver de seu ponto de observação, mas sempre ocorre alguma coisa que não tinha levado em conta. Prestar atenção em um aspecto faz com que este salte para o primeiro plano, invadindo o quadro, como em certos desenhos diante dos quais basta fecharmos os olhos e ao reabri-los a perspectiva já mudou. (…).\\nO vento estaria mudando? É pena que a imagem que o senhor Palomar havia conseguido organizar com tanta minúcia agora se desfigure, se fragmente e se perca. Só conseguindo manter presentes todos os aspectos juntos, ele poderia iniciar a segunda fase da operação: estender esse conhecimento a todo o universo.\\nBastaria não perder a paciência, coisa que não tarda a acontecer. O senhor Palomar afasta-se ao longo da praia, com os nervos tensos como havia chegado e ainda mais inseguro de tudo.\\nCALVINO, Ítalo. Palomar. São Paulo: Companhia das Letras, 1994. p.7-11.\\n\\n\\n\\n\\n\\nO que vemos, por trás da simplória tarefa de observação de uma única onda, é a dificuldade de Palomar diante de um objeto que se transforma continuamente durante a própria atividade de observação. Ainda que Palomar defina, de modo preciso, seu objetivo e seu objeto (“observar uma simples onda e pronto”) e busque uma abrangência descritiva (“Colher todos os seus componentes simultâneos sem descurar de nenhum”), é difícil, na observação, isolar o objeto de suas “adjacências”, reduzir as diferentes instâncias do objeto a uma essência comum (“sempre ocorre alguma coisa que não tinha levado em conta”), controlar a subjetividade, suspender as pressões externas (“bastaria não perder a paciência”), encontrar o ponto de vista superior ou ideal (“Foi uma dessas línguas baixas de areia que o sr. Palomar escolheu como ponto de observação”). Enfim, definitivamente, Palomar não é bem sucedido em sua empreitada, por mais simples que esta parecesse inicialmente.\\nDe volta à semântica, podemos nos imaginar como Palomar na tentativa de observar o significado de uma única palavra. Podemos escolher a palavra “quente”, e teremos uma “sopa quente”, um “dia quente” e uma “cerveja quente”. Estamos tratando da mesma temperatura, do mesmo significado, o que há em comum em todos eles, e que os define? Podemos escolher outra palavra, “medo”, em “medo de altura”, “medo de perder o emprego”, “medo de barata”, “medo do mar”, “medo de sofrer”. Estamos falando exatamente do mesmo “medo”? Qual o significado exato, preciso, de “medo”? Se estamos diante de uma mesma palavra, e de uma palavra que sabemos usar, não seria esperado que soubéssemos definir, de forma clara e precisa, seu significado? Qual o significado (ou significados) de “tomar”, tomando como exemplos combinações como “tomar um susto”, “tomar um porre”, “tomar cuidado”, “tomar um suco”, “tomar remédios”, “tomar uma decisão”, “tomar conta”, “tomar um tombo”, “tomar ciência”, “tomar porrada”, “tomar dois banhos”, “tomar um susto” etc.\\nÉ exatamente este tipo de dificuldade que justifica a existência, nos estudos linguísticos, de duas grandes perspectivas que irão problematizar o que é o significado. São perspectivas concorrentes, e de um modo bastante simplificado podemos chamá-las de representacionistas ou essencialistas, por um lado, e de pragmáticas (ou, pragmáticas radicais), por outro (Martins, 2000, 2004). No PLN, estas visões se manifestam em diferentes maneiras de lidar com o significado: usando técnicas simbólicas (veja Capítulo\\xa09) ou usando representações distribuídas (veja Capítulo\\xa010), respectivamente.\\nA perspectiva representacionista/essencialista é a visão hegemônica, estando presente em boa parte dos estudos linguísticos e no senso comum – e, até recentemente, em boa parte do PLN também.\\nNesta visão, palavras seriam como “substitutos” de entidades extralinguísticas, entidades externas à linguagem (entidades mentais, reais ou virtuais). As palavras, nessa perspectiva, importam pouco, importando mesmo as ideias (as entidades extralinguísticas) que elas representam. Significado e palavra são, assim, entidades distintas, ainda que relacionadas (falamos “do significado das palavras”, por exemplo), e a relação entre elas é hierárquica, com a entidade significado se sobrepondo à entidade palavra (ou à palavra e seus sinônimos), que apenas fornece matéria/forma para “hospedar” o significado.\\nAinda de acordo com esta visão, apesar da multiplicidade de usos e contextos que podem existir associados a uma mesma palavra (por exemplo “tomar”, ou “quente”, ou ainda “liberdade”, “violência”, “aprender”, “significado”, “compreensão”) a comunicação é possível porque esses diferentes usos estão associados a uma essência comum (a entidade extralinguística), e por isso reconhecemos a palavra como sendo a mesma em diferentes situações. A associação entre a palavra (ou a palavra e seus sinônimos) e sua essência/ideia/conteúdo/conceito/significado, por sua vez, é guiada por regras. Aprender uma língua, aqui, é aprender a estabelecer a conexão entre a palavra e a entidade extralinguística que ela representa (e diferentes línguas irão variar quanto às palavras usadas para representar estes conceitos/ideias). Este conceito/ideia/significado, que algo é separado da palavra, é um “objeto” extralinguístico (do mundo mental, real, virtual) estável e com contornos bem definidos – mas que, por sua vez, também será descrito por meio de palavras.\\nPodemos agrupar sob esta visão – apresentada aqui de maneira muito simplificada – uma série de correntes teóricas que, de alguma maneira, compartilham a ideia de que a estabilidade do significado (e a compreensão) é o resultado da representação de algo que lhe é exterior.\\nAinda segundo esta visão, os significados das palavras são, de certo modo, o que o dicionário diz. O fato de dicionários representarem os significados de maneira objetiva, estável e discreta (vemos isso na maneira pela qual as acepções estão claramente separadas e numeradas), faz parecer que os significados das palavras se organizam “naturalmente” assim2. No PLN, reconhecemos esta maneira de lidar com o significado em recursos como wordnets, por exemplo, que são bases de dados lexicais que contêm “nomes, verbos, adjetivos e advérbios agrupados em conjuntos de sinônimos cognitivos, cada um representando um conceito distinto”3 (grifo meu). Uma apresentação do que se pode fazer partindo desse ponto de vista, e de por que ele continua tendo espaço no PLN, está no Capítulo\\xa09.\\nJá do ponto de vista pragmático (ou, mais precisamente, pragmático radical)4, – e fazendo igualmente uma apresentação bastante simplificada – o significado de uma palavra é decorrência de situações concretas (e não o correspondente a uma entidade extralinguística), e situações concretas são variáveis. Nesta visão, os vários usos de uma palavra não se organizam em torno de um núcleo semântico comum (a entidade extralinguística), garantidor da estabilidade do que elas significam. A estabilidade do significado será sempre provisória, e o significado dependerá do uso, do contexto, do tempo, do espaço, de quem fala … A comunicação se dá no risco (isto é, pode dar certo ou não, podemos nos entender ou não), e os mal-entendidos existem, estão aí – não são um desvio ou uma falha, são parte do jogo. O que determina se compreendemos o significado de um enunciado linguístico é o fato de a manifestação dessa compreensão (um comportamento) ser considerada adequada no contexto em que é produzida. Por exemplo, a um enunciado como “Está quente aqui”, seriam manifestações legítimas ações como abrir a janela ou respostas como “Não acho” ou “Por que não tira o casaco?”, entre outras. Mas dificilmente aceitaríamos como manifestações de compreensão do enunciado “Está quente aqui” dar uma cambalhota ou uma resposta como “Prefiro melão”5. Aprender uma língua, aqui, é aprender a tomar parte nas atividades humanas, um aprendizado que nunca se completa.\\nAssim como uma onda, os limites do significado de uma palavra não têm – aliás, podem não ter, pois não se trata de uma exigência – a precisão ou os limites definidos, necessários à formalização que sempre se buscou fazer. Segundo esta visão, o significado é flexível e maleável, não havendo uma “essência”, algo que perpassa todos os usos, e sobre o qual seja possível se sustentar, se estabilizar. No PLN, esta visão se alinha às representações distribuídas (veja Capítulo\\xa010)6.\\nSabemos que uma mesma palavra pode aparecer em contextos diferentes – desde contextos completamente distintos, como “banco” e “manga”, até contextos ligeiramente diferentes, como os exemplos de “quente”, “medo” ou “tomar”, que já vimos7. Nesse caso, e considerando os modelos de representações distribuídas mais complexos e dinâmicos, cada forma “quente” ou “medo” será representada de uma maneira – e por isso nesses casos falamos de vetores contextuais (contextual word embeddings). Nos vetores estáticos, que irão representar de uma única maneira as várias formas “quente” ou “medo”, o alinhamento à visão não-representacionista/não essencialista se mantém, uma vez que não há uma fonte (ou entidade) externa que determina o significado da palavra. Vetores produzidos a partir de conjuntos de dados diferentes irão levar a representações diferentes8.\\nDurante muito tempo, a semântica computacional esteve ancorada em visões essencialistas-representacionistas (ou simbólicas), como ilustram os capítulos “Semantics” de dois compêndios da área de PLN: Jurafsky; Martin (2023) e Mitkov (2003). No entanto, trabalhos de PLN que dialogam claramente com perspectivas não-essencialistas também não são novidade, como Kilgarriff (1997); Kilgarriff (2003) e Brewster; Wilks (2004), por exemplo.\\nEntre as técnicas simbólicas e as representações distribuídas existem ainda os datasets (ou corpora) com anotação semântica (veja Capítulo\\xa014), uma terceira maneira de lidar com o significado no PLN. Se, por um lado, tais datasets se alinham às abordagens probabilísticas, uma vez que podem ser usados como fonte para o aprendizado de máquina (para o aprendizado do significado), por outro lado, a atividade de anotação de significado se alinha às abordagens representacionistas. Neste tipo de anotação (também chamada de anotação de word senses), cada palavra (ou segmento de texto) é anotada com informação relativa ao significado de acordo com o contexto específico em que aparece no corpus. A informação relativa ao significado, por sua vez, vem de fontes externas (como dicionários, wordnets, verbnets e framenets) e a tarefa de anotação pode ser descrita como um trabalho de desambiguação, pois consistiria em selecionar, dentre os vários sentidos possíveis de uma palavra, aquele usado no contexto da frase. O que a anotação faz, deste modo, é criar uma representação estável entre a palavra e o seu significado, no contexto em que está sendo usada. Cada ocorrência de uma palavra poderá estar associada a um significado diferente (e aqui vemos uma aproximação com abordagens pragmáticas), desde que este significado esteja presente no inventário de significados usado na anotação (aqui vemos uma aproximação com abordagens representacionistas). Para as pessoas responsáveis pela anotação, a principal dificuldade está na escolha do sentido adequado conforme o contexto, uma vez que os sentidos frequentemente se sobrepõem, como as ondas observadas por Palomar.\\nPor exemplo, tomando a palavra “trabalho” destacada no parágrafo anterior, a tarefa consiste em escolher, dentre opções listadas no quadro abaixo, retiradas do dicionário Caldas-Aulete online9, aquela adequada ao contexto (se a anotação usasse o inventário de uma wordnet como fonte, o inventário de significados poderia ser diferente10).\\n\\nQuadro 8.'}]\n",
            "\n",
            "\n",
            "Resumo do trecho 0 foi bem-sucedido. Segue o resumo: A semântica estuda o significado das palavras e frases na linguagem. No entanto, a definição do significado de uma palavra é um problema complexo e depende da perspectiva teórica adotada. Existem duas principais abordagens no estudo do significado: a representacionista/essencialista e a pragmática. A abordagem representacionista vê as palavras como substitutos de entidades extralinguísticas e busca uma essência comum nos diferentes usos da palavra. Já a abordagem pragmática entende que o significado é variável e depende do contexto e da situação. No processamento de linguagem natural (PLN), essas diferentes perspectivas se refletem em técnicas simbólicas e representações distribuídas. Além disso, existem os datasets com anotação semântica, que buscam criar uma representação estável entre palavras e seus significados em contextos específicos. Essas abordagens têm impacto no desenvolvimento de recursos como wordnets e na anotação de significados em corpora. \n",
            "\n",
            "\n",
            "[{'role': 'system', 'content': 'Você é uma inteligência artificial que irá resumir pedaços de texto de um livro sobre Processamento de Linguagem Natural, portanto, você deve capturar explicações e pontos importantes para o entendimento e mantendo uma linearidade no resumo.'}, {'role': 'user', 'content': '2 Acepções da palavra trabalho conforme dicionário\\n\\n\\n\\n\\n\\n\\n\\n\\nEmprego da força física ou intelectual para realizar alguma coisa\\nAplicação dessas forças como ocupação profissional: Seu trabalho é de gari.\\nLocal onde isso se realiza: Mora longe do trabalho.\\nEsmero, cuidado que se emprega na confecção ou elaboração de uma obra\\nA confecção, elaboração ou composição de uma obra\\nObra realizada: Essa cômoda é um belo trabalho de marcenaria.\\nGrande esforço; TRABALHÃO; TRABALHEIRA\\nExercício para treino: A professora passou muito trabalho para casa.\\nAção contínua de uma força da natureza e seu efeito: O trabalho do vento resulta na erosão eólia.\\nMed. Fenômeno orgânico que se opera no interior dos tecidos (trabalho inflamatório; trabalho de cicatrização)\\nResultado do funcionamento de uma máquina, um aparelho etc.: o trabalho de uma pá mecânica.\\nObrigação ou responsabilidade; DEVER; ENCARGO: Seu trabalho é protegê-lo do assédio da imprensa.\\nEcon. Conjunto das atividades humanas empregado na produção de bens: O capital e o trabalho são os pilares da economia.\\nTarefa a ser realizada: Contratou-o para um trabalho temporário.\\n\\n\\n\\n\\n\\nFonte: (Freitas, 2022)\\n\\nParece que as acepções 1, 2 e 14 são aceitáveis no contexto da frase, o que já é um problema se precisamos escolher apenas um sentido, e por isso não é exagero dizer que as pessoas responsáveis pela anotação se sentem como Palomar na tentativa de isolar uma onda.\\nCorpora anotados com este tipo de informação são escassos, e um dos motivos é justamente a dificuldade de isolar o significado/conteúdo/essência das palavras enquanto estão sendo efetivamente usadas.\\nO estudo de Baker; Fellbaum; Passonneau (2017) tentou entender por que, com este tipo de anotação, era tão difícil conseguir uma boa concordância entre anotadores (veja Capítulo\\xa014), isto é, era tão difícil que as pessoas concordassem quanto à escolha do significado utilizado. Afinal, a tarefa é simples: associar cada palavra ao seu significado, e, se sabemos a nossa língua, sabemos o significado das palavras que usamos. No estudo, diferentes pessoas deveriam anotar as mesmas palavras, nas mesmas frases, considerando o mesmo inventário de sentidos. Os resultados indicaram uma variação bem maior que o previsto11. Vamos lembrar que, na anotação, os significados precisam ser vistos como unidades discretas e de conteúdo estável – uma necessidade de ordem prática que se alinha harmoniosamente com visões representacionistas, mas que não encontra respaldo em visões pragmáticas.\\nO fato de representações distribuídas terem levado a resultados positivos no PLN não deve ser visto como argumento contrário às técnicas simbólicas. São maneiras diferentes de lidar com o sentido das palavras. Como tirar o melhor proveito destas diferentes visões e abordagens, no PLN, é uma das questões que se coloca. O que temos visto é a limitação de cada uma delas, tomada individualmente. Se consideramos o significado como uma entidade estável, como lidar com as mudanças, que inclusive podem ser capturadas pelos dicionários (dicionários, recentemente, mudaram a definição da palavra “família”12)? Por outro lado, se consideramos a instabilidade e a dependência dos dados, como evitar vieses indesejados, como a associação entre os sentidos, por exemplo, de “paraguaio” e “de baixa qualidade”, quando dizemos “uísque paraguaio”?\\nOs próximos capítulos aprofundam cada uma dessas maneiras de trabalhar com o significado no PLN.\\n\\n\\nBAKER, C.; FELLBAUM, C.; PASSONNEAU, R. Semantic Annotation of MASC. Em: Handbook of Linguistic Annotation. [s.l.] Springer Netherlands, 2017. p. 699–717.\\n\\n\\nBREWSTER, C.; WILKS, Y. Ontologies, taxonomies, thesauri:learning from texts. (M. Deegan, Ed.)Proceedings of Use of Computational Linguistics in the Extraction of Keyword Information from Digital Library Content Workshop. Anais...2004. Disponível em: <http://www.cbrewster.com/papers/KeyWord_FMO.pdf>\\n\\n\\nFREITAS, C. Linguística Computacional. [s.l.] Editora Parábola, 2022.\\n\\n\\nILARI, R.; GERALDI, J. W. Semântica. [s.l.] Ética, 1985.\\n\\n\\nJURAFSKY, D.; MARTIN, J. H. Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition. 3rd. ed. USA: Prentice Hall PTR, 2023.\\n\\n\\nKILGARRIFF, A. I Don’t Believe in Word Senses. Computers and the Humanities, 1997.\\n\\n\\nKILGARRIFF, A. Thesauruses for Natural Language Processing. Proceedings of Natural Language Processing and Knowledge Engineering. Anais...2003. Disponível em: <https://www.kilgarriff.co.uk/Publications/2003-K-Beijing-thes4NLP.pdf>\\n\\n\\nMARTINS, H. Sobre a estabilidade do significado em Wittgenstein. Veredas, v. 4, n. 2, p. 19–42, 2000.\\n\\n\\nMARTINS, H. Três Caminhos na Filosofia da Linguagem. Em: Introdução à Linguística. Volume III. [s.l.] Editora Cortez, 2004.\\n\\n\\nMITKOV, R. The Oxford handbook of Computational Linguistics. [s.l.] Oxford University Press, 2003.\\n\\n\\n\\n\\n\\nDevo à Helena Franco Martins a apresentação deste texto como alegoria tanto para as tentativas de apreensão do significado como para a crise relativa ao conhecimento/ciência.↩︎\\nPor trás de dicionários estão lexicógrafos e decisões editoriais.↩︎\\nhttp://wordnet.princeton.edu/↩︎\\nDentre as linhas de investigação pragmáticas há as que poderiam ser também enquadradas em um paradigma representacionista. Isto porque algumas correntes da pragmática recomendam a análise das propriedades da prática da comunicação como maneira de fornecer uma explicação do que são as línguas e os significados. Por isso a especificação indicando a “radicalidade” da visão que será apresentada.↩︎\\nMas mesmo estas poderiam ser aceitas se assim fosse previamente estipulado.↩︎\\nMas não se alinha à busca do algoritmo capaz de fornecer a representação distribuída “correta”, ou “verdadeira”.↩︎\\nÉ importante notar que, diferentemente do que supõe o senso comum, os casos de “banco” ou “manga”, apesar de fartamente citados como exemplos de ambiguidade, estão longe de ser prototípicos. Pelo contrário, são raros os casos em que dois sentidos se apresentam tão claramente distintos. O mais comum são casos como “quente” ou “medo”.↩︎\\nMas mesmo representações distribuídas podem ser associadas a visões representacionistas, quando se assume que tais representações são úteis apenas enquanto não encontramos a forma (ou a representação) correta de uma palavra.↩︎\\nhttps://www.aulete.com.br/trabalho↩︎\\nAqui é possível consultar a OpenWordNet-PT para os significados de “trabalho”: https://www.openwordnet-pt.org/search?search_field=all&term=trabalho↩︎\\nA seção “Anotações Semânticas” de Freitas (2022) traz um levantamento dos principais estudos sobre anotação semântica e seus desafios, bem como uma apresentação linguística da alternativa oferecida pelas representações distribuídas para o tratamento do significado.↩︎\\nDicionários mudam definição de família https://www.metropoles.com/vida-e-estilo/comportamento/dicionario-houaiss-muda-significado-da-palavra-familia↩︎\\n\\n\\n.'}]\n",
            "\n",
            "\n",
            "Resumo do trecho 1 foi bem-sucedido. Segue o resumo: O termo \"trabalho\" possui diversas acepções de acordo com o dicionário. Essas acepções incluem o emprego da força física ou intelectual para realizar algo, a ocupação profissional, o esmero na confecção de uma obra, o grande esforço, a ação contínua de uma força da natureza, o fenômeno orgânico que ocorre nos tecidos, o resultado do funcionamento de uma máquina, a obrigação ou responsabilidade, o conjunto de atividades humanas empregadas na produção de bens e a tarefa a ser realizada. A anotação semântica dessas diferentes acepções é um desafio, pois é difícil isolar o significado das palavras enquanto estão sendo usadas. Estudos têm mostrado a dificuldade de se obter concordância entre anotadores quanto à escolha do significado utilizado. O uso de representações distribuídas tem sido uma abordagem promissora para lidar com o sentido das palavras, mas ainda há limitações em cada uma das abordagens existentes. O próximo capítulo irá aprofundar essas diferentes maneiras de trabalhar com o significado no Processamento de Linguagem Natural. \n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lista para armazenar os resumos do Capítulo 18.\n",
        "resumo_Capitulo_18 = []\n",
        "\n",
        "# Itera sobre os chunks do Capítulo 18 e obtém os resumos.\n",
        "for i, value in enumerate(chunks_18):\n",
        "    resumo = resumir_trecho(value, i)\n",
        "\n",
        "    # Verifica se o resumo foi bem-sucedido.\n",
        "    if resumo is not None:\n",
        "        print(f\"\\n\\nResumo do trecho {i} foi bem-sucedido. Segue o resumo: {resumo}\\n\\n\")\n",
        "        resumo_Capitulo_18.append((i, resumo))\n",
        "    else:\n",
        "        print(f\"Falha no resumo do trecho {i}. Você pode tentar novamente ou lidar com o contexto de falha.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIcJTiY_tKvQ",
        "outputId": "be4095f3-1b7e-4c7c-bc3e-b42fa103618c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'role': 'system', 'content': 'Você é uma inteligência artificial que irá resumir pedaços de texto de um livro sobre Processamento de Linguagem Natural, portanto, você deve capturar explicações e pontos importantes para o entendimento e mantendo uma linearidade no resumo.'}, {'role': 'user', 'content': '18\\xa0 Tradução Automática\\nAbordagens e Avaliação\\n\\n\\n\\n\\n\\nSheila Castilho \\nHelena de Medeiros Caseli \\n\\n\\n\\nPublicado em:\\n\\n26/09/2023\\n\\n\\n\\n\\nPDF\\n\\n18.1 Introdução\\nA tradução automática (TA), também conhecida como tradução de máquina (em inglês, machine translation ou MT), refere-se à tradução de um texto eletrônico por um computador de uma língua para outra sem intervenção humana. Nesse sentido, convencionou-se chamar de língua (ou texto) fonte a língua de partida (origem) e língua (ou texto) alvo a língua de chegada (destino ou saída). Além de envolver a análise e interpretação (NLU) da língua-fonte e a geração (NLG) da língua-alvo, há a premissa fundamental de gerar uma saída que seja semanticamente equivalente (transmite o mesmo significado) à entrada.\\nNos últimos anos, a TA evoluiu significativamente com o avanço de modelos estatísticos e neurais. Atualmente, ela é amplamente utilizada em todo o mundo por governos, indústria da tradução, consumidores finais e em pesquisas em uma variedade de aplicações.\\nOs primeiros sistemas bem-sucedidos de TA datam do final dos anos 1950 e início dos anos 19601, com os experimentos de Georgetown. No entanto, é possível encontrar referências a tentativas de tradução automática no século XVII (Hutchins, 2001). Desde então, diferentes abordagens para a TA foram desenvolvidas, incluindo abordagens baseadas em regras, exemplos, estatísticas e, mais recentemente, a TA neural, apresentadas brevemente nas diversas seções deste capítulo.\\nHoje em dia, a TA desempenha um papel importante não apenas no âmbito comercial, mas também no âmbito social e político. Ela é amplamente utilizada em diversas aplicações de comunicação, que incluem2:\\n\\nTexto para texto: o usuário insere um texto-fonte e obtém uma versão traduzida em formato de texto;\\nTexto para fala: o usuário insere um texto-fonte e obtém uma versão falada na língua-alvo;\\nFala para texto: o usuário fala no idioma fonte e obtém uma versão traduzida em formato de texto;\\nFala para fala: o usuário fala no idioma fonte e obtém uma versão falada na língua-alvo;\\nImagem (de palavras) para texto: o usuário insere uma imagem contendo um texto e obtém a tradução desse texto.\\n\\nDevido à ampla utilização da TA atualmente, seu impacto pode ser observado em nossa sociedade. Por essa razão, a avaliação da TA (tema da Seção\\xa018.3) tornou-se mais importante, visando garantir a qualidade da tradução. Além da avaliação, este capítulo descreve as principais abordagens para a TA (Seção\\xa018.2). No decorrer deste capítulo, alguns conceitos-chave são explicados para que você possa acompanhar os desenvolvimentos.\\n\\n\\n18.2 Abordagens\\nA tradução automática pode ser realizada de diversas maneiras, desde a mais simples (tradução direta), que envolve a tradução palavra-a-palavra (ou sequência de palavras), até a mais utilizada na atualidade, que é a tradução baseada em redes neurais artificiais (tradução neural). Na trajetória entre a tradução direta e a tradução neural, explicaremos também abordagens intermediárias, como a baseada em regras, a tradução por interlíngua e a tradução estatística.\\nPara tanto, vamos traduzir a sentença “A casa do meu avô é linda.” para o inglês. Como isso aconteceria de acordo com as abordagens mais utilizadas (ou tradicionais) na tradução automática?\\n\\n18.2.1 Tradução direta\\nNa tradução direta ocorre o mapeamento direto de palavras-fonte para palavras-alvo sem passar por outros níveis de análise3. Assim, no nosso exemplo, os caracteres seriam combinados em palavras e cada uma seria mapeada diretamente para seu equivalente em inglês usando, por exemplo, um léxico bilíngue. Utilizando o léxico bilíngue disponível no github do MUSE4 e a tradução palavra-a-palavra, nossa saída seria como apresentado em Exemplo\\xa018.1:\\n\\nExemplo 18.1 \\xa0\\n\\nEntrada: A casa do meu avô é linda.\\nSaída: _ house _ my granddad _ beautiful .\\n\\n\\nonde as palavras para as quais não se encontrou um equivalente no léxico consultado foram substituídas por _5.\\nVejam que neste processo de tradução não há nenhum processamento referente às línguas envolvidas, uma vez que o resultado é obtido via casamento de padrão, seguido da substituição de uma palavra por outra com base em uma lista de pares de palavras.\\nObviamente a abordagem de tradução direta apresenta diversas limitações, como não ser capaz de lidar com a estrutura (sintaxe) da língua, que, como pode ser visto no Capítulo\\xa06, é fundamental para o tratamento adequado da língua. A tradução direta foi uma das primeiras abordagens a serem investigadas e não é mais utilizada nos tradutores atuais.\\n\\n\\n18.2.2 Tradução Automática Baseada em Regras\\nComo o nome sugere, os sistemas de TA baseados em regras (em inglês, Rule-based Machine Translation ou RBMT) são sistemas baseados em conhecimento desenvolvidos por meio da especificação de regras linguísticas, que levam em consideração morfologia, sintaxe e semântica das línguas envolvidas, além dos léxicos bilíngues de ambos os idiomas de origem e de destino6. Essas regras e léxicos são formuladas e criados manualmente por especialistas em linguagem.\\nA partir desses recursos, a RBMT é capaz de realizar mapeamentos mais complexos como os que consideram a sintaxe das línguas fonte e alvo. Assim, no nosso exemplo, uma possível regra que poderia ser aplicada é a que indica a inversão da posição do sujeito que possui a casa com o uso do “apóstrofo s” gerando “My grandfather’s house is beautiful.” ao invés de “The house of my grandfather is beautiful.”\\nAssim, vê-se que o processamento automático necessário para a tradução baseada em regras é um pouco mais complexo do que aquele aplicado na tradução direta, uma vez que agora é preciso saber o papel de cada palavra na sentença-fonte, ou seja, saber que “casa” é um substantivo e que “do” é uma combinação da preposição “de” e determinante (artigo definido “o”) para que a regra apresentada em Quadro\\xa018.1 seja aplicada corretamente. A regra especifica que sempre que for encontrado, na sequência-fonte, um substantivo (<SUB>) seguido da preposição (<PREP>) “de” combinada (+) com os artigos (<DET>) “a” ou (|) “o” deve-se gerar como saída o apóstrofo (’) seguido de “s” e o substantivo equivalente na língua-alvo. O símbolo “=>” separa o que deve ser considerado na língua-fonte (à esquerda) e o que deve ser gerado na língua-alvo (à direita).\\n\\nQuadro 18.1 Exemplo de regra para a tradução automática baseada em regras\\n\\n\\n\\n\\n\\n\\n\\n<SUB> <PREP/de>+<DET/[a|o]> => ’s <SUB>\\n\\n\\n\\n\\nPara se determinar o “papel” de cada palavra na sentença o processamento necessário é o da etiquetagem morfossintática ou, do inglês, part-of-speech tagging descrito no Capítulo\\xa04.\\nApesar de realizar um processamento automático um pouco mais complexo, a desvantagem da tradução baseada em regras não está aí, mas sim na necessidade de mapear o conhecimento linguístico em regras corretas, genéricas e abrangentes o suficiente para que sejam aplicáveis a vários exemplos. Vejam que esse mapeamento envolve, necessariamente, o conhecimento da língua-fonte, da língua-alvo, e de como o processo de tradução de uma para a outra deve ocorrer. Além de um processo trabalhoso, a geração de regras é também limitada, pois, como a língua está em constante mudança, o conjunto de regras gerado tem que ser constantemente atualizado e revisado. Além disso, a tradução de/para outra língua necessita de um novo conjunto de regras. Isso porque a tradução por transferência entre duas línguas requer que a representação do conhecimento extraído da língua-fonte, e que vai ser mapeado para a língua-alvo, seja capaz de abrigar todas as características de ambas as línguas, tornando-a específica para aquele par. Analogamente, o desenvolvedor tem que ter muito conhecimento de ambas as línguas ou a equipe deve contar com linguistas/tradutores, o que torna os sistemas caros de se implementar. Além disso, a saída dos sistemas de regras pode apresentar pouca fluência, já que as traduções são fornecidas por meio de regras.\\nA grande vantagem dos sistemas de regras é que, como não são necessários textos bilíngues para seu treinamento, esses sistemas são excelentes para traduções de idiomas com recursos limitados. Além disso, esses sistemas permitem que o desenvolvedor tenha um controle maior, sendo possível identificar exatamente onde estão os problemas, e a saída (texto-alvo) é relativamente previsível. Tanto regras quanto léxicos podem ser refinados e personalizados, com a adição de mais (novas) regras e entradas bilíngues para aprimorar a tradução. Outro ponto interessante, é que o conhecimento é legível por seres humanos o que facilita a manutenção. Os sistemas de TA baseados em regras foram os primeiros sistemas comerciais de TA na década de 1970 e abriram caminho para mais pesquisas em TA após o relatório ALPAC, que cortou os fundos (Seção\\xa018.3.1).\\n\\n\\n18.2.3 Tradução por interlíngua\\nMas será que esse conceito de transferência entre línguas não pode ser estendido para um número maior de línguas, ou seja, considerando um cenário multilíngue? Sim, essa é a ideia da tradução por interlíngua, que se propõe a usar uma língua intermediária – metalíngua – que é independente das línguas envolvidas na TA e ao mesmo tempo é capaz de representar informações de qualquer outra língua. Essa metalíngua, de natureza artificial, é não ambígua e, portanto, mais simples de processar do que qualquer linguagem natural. Assim, o processo de tradução entre duas línguas quaisquer é composto de duas etapas de traduções supostamente mais simples: uma realizada entre a língua-fonte e a metalíngua, e outra realizada entre esta metalíngua e a língua-alvo.\\nMas será que essa abordagem simplifica o processo? Sim. Para ficar claro, imagine o seguinte cenário onde existem documentos escritos em n línguas distintas e queremos traduzi-los de e para quaisquer dessas línguas. Poderíamos construir arranjos de n tradutores7 automáticos para cada par de línguas (n1-n2, n1-n3, n1-n4, …, n2-n1, n2-n3, n2-n4, …) seguindo o modelo de transferência (tradução automática baseada em regras). Mas a tradução por interlíngua mostra-se mais vantajosa. Podemos definir uma metalíngua e dividir a tarefa em n grupos de especialistas/tradutores, cada um responsável por uma única língua, li (de preferência sua língua materna). Caberá a cada grupo construir um tradutor (lembre-se: bem mais simples!), ou codificador, de li para a metalíngua, e outro, decodificador, da metalíngua para li. Evidentemente todos os grupos compartilham o mesmo conhecimento sobre a língua intermediária.\\nConsiderando que cada grupo fará sua parte, ao final, as possíveis combinações desses módulos de tradução darão origem a todos os tradutores almejados. Por exemplo, para traduzir li para lj, juntamos o módulo codificador de li com o módulo decodificador para lj. Terão sido construídos 2n módulos de traduções mais simples, portanto, menos esforço do que o exigido para os tradutores bilíngues. Uma outra consequência é a possibilidade de se avaliar as traduções por meio da tradução inversa, já que os módulos independentes permitem a tradução nos dois sentidos.\\nEsse ideal foi compartilhado algumas vezes, no passado, por vários grupos de pesquisa acadêmica, mas infelizmente a prática evidenciou vários problemas. O maior deles é, segundo os críticos, a ingenuidade em se acreditar possível criar uma linguagem capaz de representar o significado de todas as outras, portanto, universal. Um outro problema – ou decorrente deste – é a adoção unânime de uma dada língua intermediária pelos grupos de línguas distintas, onde cada um deles reivindica alterações e adaptações, escancarando a inexistência de um núcleo verdadeiramente universal. Por essas e outras razões é que esse modelo não substituiu o modelo por transferência bilíngue.\\nNo final dos anos 1990, o português brasileiro estava representado, pelo NILC8, numa iniciativa da ONU para construção de tradutores para as línguas mais faladas no mundo, o Projeto UNL9. Esse projeto tinha por objetivo o desenvolvimento de um sistema multilíngue de tradução automática baseada numa interlíngua de natureza semântica – a Universal Networking Language (UNL) – desenvolvida por pesquisadores vinculados à Universidade das Nações Unidas, órgão da ONU, em Tóquio10.\\nO paradigma linguístico (baseado em regras e interlíngua), no qual o conhecimento linguístico é explicitamente mapeado em recursos como regras, dominou o cenário da tradução automática até a década de 1980, quando abordagens baseadas em corpus (empíricas) surgiram. Aliadas à motivação de tentar superar as limitações da tradução baseada em regras, essas abordagens foram impulsionadas por dois fatores: (1) os avanços no hardware necessário para processamentos computacionais mais pesados, e (2) a disponibilidade maior de recursos bilingues, em especial os corpus paralelos. As próximas seções tratam das abordagens baseadas em corpus: a tradução baseada em exemplos, a tradução estatística e a tradução neural.\\n\\n\\n18.2.4 Tradução Automática Baseada em Exemplos\\nOs sistemas de TA baseada em exemplos (do inglês, Example-based Machine Translation ou EBMT), também conhecidos como tradução por analogia, estão frequentemente associados à publicação do artigo de Nagao (1984), no qual o autor propõe um modelo baseado na imitação de exemplos de tradução de frases semelhantes, buscando utilizar a ideia de aprender a traduzir a partir de exemplos existentes (Koehn, 2020). Os sistemas de exemplos utilizam informações extraídas (sequências de palavras) de exemplos em corpora bilíngues de pares de tradução, alinhados em nível de sentença, ao qual convencionou-se chamar de corpora paralelos.\\nPor meio dessa abordagem, exemplos como os ilustrados no Quadro\\xa018.2 serviriam de base para o sistema aprender traduções de trechos de texto, como as ilustradas em Quadro\\xa018.3.\\n\\nQuadro 18.2 Exemplos para a tradução baseada em exemplos\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nA casa é muito bonita.\\nThe house is very beautiful.\\n\\n\\nMeu avô foi internado ontem.\\nMy grandfather was hospitalized yesterday.\\n\\n\\nEu comprei uma jaqueta linda.\\nI bought a beautiful jacket.\\n\\n\\n\\n\\n\\n\\n\\n\\nQuadro 18.3 Trechos aprendidos\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\na casa\\nthe house\\n\\n\\nmeu avô\\nmy grandfather\\n\\n\\nlinda\\nbeautiful\\n\\n\\n\\n\\n\\n\\n\\nA partir dos trechos aprendidos, o sistema baseado em exemplos seria capaz de combiná-los para, a partir da entrada, gerar a saída apresentada em Exemplo\\xa018.2:\\n\\nExemplo 18.2 \\xa0\\n\\nEntrada: A casa do meu avô é linda.\\nSaída: The house _ my grandfather _ beautiful.\\n\\n\\nDevido ao uso de diferentes métodos e técnicas, de acordo com Hutchins (2005, p. 63): “não parece haver um consenso claro sobre o que é ou o que não é um sistema de exemplos”. Para Carl; Way (2003, p. xix), uma definição analítica dos sistemas de exemplos era difícil, pois, segundo ele, tais sistemas “assumem uma posição entre os sistemas baseado em regras e os estatísticos” ao utilizar abordagens tanto baseadas em regras quanto orientadas por dados11. No entanto, tanto os sistemas baseados em exemplos como os estatísticos se enquadram no paradigma de TA baseada em corpus. Enquanto alguns autores veem os sistemas baseados em exemplos como um paradigma em si mesmo, outros consideram os sistemas estatísticos como um tipo de sistema de exemplos, já que os primeiros sistemas estatísticos surgiram no final da década de 1980.\\n\\n\\n18.2.5 Tradução Automática Estatística\\nOs sistemas de TA estatísticos (em inglês, Statistical Machine Translation ou SMT) foram apresentados pela primeira vez por Brown et al. (1988); no entanto, a ideia de usar métodos estatísticos para traduções automáticas foi introduzida pela primeira vez por Weaver em 1949 (Brown et al., 1988, p. 71). Desde a primeira publicação de Brown et al., a equipe da IBM desenvolveu para a empresa o primeiro sistema estatístico funcional e houve um aumento drástico na pesquisa em TA estatística na área.\\nA ideia geral dos sistemas estatísticos é usar modelos estatísticos para extrair pares de tradução de corpora bilíngues. Podem ser encontradas três abordagens principais para a TA estatística:\\n\\nTA estatística baseada em palavras (Word-based Statistical Machine Translation): alinha12 palavras individuais no texto-fonte a palavras no texto-alvo e calcula a probabilidade da tradução. Também permite a exclusão e inserção de palavras.'}]\n",
            "\n",
            "\n",
            "Resumo do trecho 0 foi bem-sucedido. Segue o resumo: A tradução automática (TA) é a tradução de um texto eletrônico de uma língua para outra sem intervenção humana. A TA evoluiu significativamente nos últimos anos com o avanço de modelos estatísticos e neurais. Atualmente, é amplamente utilizada em diversas aplicações de comunicação. Existem diferentes abordagens para a TA, incluindo a tradução direta, baseada em regras, por interlíngua, baseada em exemplos e estatística. A tradução direta mapeia palavras-fonte para palavras-alvo sem análise adicional, mas apresenta limitações. A tradução baseada em regras utiliza regras linguísticas para realizar mapeamentos mais complexos, considerando a sintaxe das línguas envolvidas. A tradução por interlíngua utiliza uma língua intermediária para representar informações de outras línguas. A tradução baseada em exemplos utiliza exemplos de traduções existentes para aprender a traduzir trechos de texto. A tradução estatística utiliza modelos estatísticos para extrair pares de tradução de corpora bilíngues. \n",
            "\n",
            "\n",
            "[{'role': 'system', 'content': 'Você é uma inteligência artificial que irá resumir pedaços de texto de um livro sobre Processamento de Linguagem Natural, portanto, você deve capturar explicações e pontos importantes para o entendimento e mantendo uma linearidade no resumo.'}, {'role': 'user', 'content': 'TA estatística baseada em frases (em inglês, Phrase-based Statistical Machine Translation ou PBSMT): alinha frases (não frases linguísticas, mas fragmentos de frases e palavras) no texto-fonte a frases no texto-alvo, comparando frases e seus vizinhos frasais ao considerar uma tradução. Essas frases também são chamadas de n-gramas, que são sequências contínuas de n palavras em sequência, ou seja, um unigrama é uma palavra, um bigrama são duas palavras, um trigrama são três palavras etc. A TA estatística baseada em frases é o tipo de abordagem estatística mais utilizado.\\nTA estatística baseada em sintaxe: esses modelos traduzem unidades sintáticas usando árvores sintáticas geradas por analisadores sintáticos (Capítulo\\xa06).\\n\\nIndependente da estratégia escolhida, na tradução estatística a probabilidade determina como um texto-fonte deve ser traduzido para um texto-alvo. De acordo com a estratégia escolhida, essa probabilidade pode ser calculada considerando apenas palavras ou também sequências de palavras (frases). Essas frases são sequências de tokens (não necessariamente palavras) como “a casa do” ou “linda .” (onde o ponto final faz parte da frase). Seja considerando apenas palavras ou frases, a tradução é realizada com base em dois modelos computacionais: (1) um modelo de tradução que especifica como mapear texto-fonte em texto-alvo e (2) um modelo de língua que especifica como gerar um texto-alvo fluente. Desse modo, o modelo de tradução tenta maximizar a acurácia da tradução, enquanto o modelo de língua tenta maximizar a fluência da sentença gerada na língua-alvo (Seção\\xa018.3).\\nPara tentar tornar esses conceitos menos abstratos, vamos retomar nosso exemplo da sentença “A casa do meu avô é linda.”. Nesse caso, o modelo de tradução poderia ser baseado em probabilidades de tradução de frases como os gerados para o corpus FAPESP (Aziz; Specia, 2011) com o auxílio do Moses13, como ilustrado na Tabela\\xa018.1.\\n\\n\\n\\nTabela\\xa018.1: Exemplos de frases fonte e alvo, e suas probabilidades, presentes em um modelo de tradução gerado a partir do Corpus FAPESP (Aziz; Specia, 2011).\\n\\n\\n\\n\\n\\n\\n\\n\\nid\\nfrase-fonte (português)\\nfrase-alvo (inglês)\\nprobabilidade\\n\\n\\n\\n\\n1\\na casa do\\n’ house\\n0.0207779\\n\\n\\n2\\na casa do\\nthe house from\\n0.0623338\\n\\n\\n3\\na casa\\nthe house\\n0.297619\\n\\n\\n4\\na casa\\nthe home\\n0.0646474\\n\\n\\n5\\ndo meu\\nof my\\n0.0813954\\n\\n\\n6\\ndo meu\\nthat of my\\n0.191576\\n\\n\\n7\\nmeu avô\\nmy grandfather\\n0.662453\\n\\n\\n8\\nmeu avô , meu pai , eu\\nmy grandfather , my father , me\\n0.0623338\\n\\n\\n9\\navô\\ngrandfather\\n0.916667\\n\\n\\n10\\né\\nis\\n0.611613\\n\\n\\n11\\né\\né\\n0.794943\\n\\n\\n12\\nlinda\\nbeautiful\\n0.0389678\\n\\n\\n13\\nlinda\\npretty\\n0.00259724\\n\\n\\n\\n\\n\\nCom base nas probabilidades da Tabela\\xa018.1, diversas opções de tradução poderiam ser geradas como as apresentadas no Exemplo\\xa018.3.\\n\\nExemplo 18.3 \\xa0\\n\\nThe house from my grandfather is beautiful.14\\nThe house that of my grandfather is pretty.15\\nThe home of my grandfather é beautiful.16\\n\\n\\nE qual dessas sentenças o sistema escolheria como saída? Isso depende de alguns fatores que não vamos detalhar aqui, mas podemos dizer que o modelo de linguagem (Capítulo\\xa015) tem um papel fundamental na seleção da melhor sentença candidata. Nesse caso, o modelo de linguagem diz qual é a melhor sentença com base na probabilidade de ela ser encontrada na língua-alvo, ou melhor, no corpus de treinamento usado para gerar o modelo de língua-alvo.\\nAssim, tanto o modelo de tradução quanto o modelo de linguagem são treinados a partir de corpus. No caso da tradução estatística, as probabilidades são determinadas contando-se as frequências de ocorrência das palavras em grandes quantidades de textos (os corpora) paralelos. Um exemplo de corpus paralelo português-inglês é o coletado por Aziz; Specia (2011), o Corpus FAPESP17, que serviu de base para a geração das probabilidades apresentadas na Tabela\\xa018.1. Para a geração do modelo de tradução, a probabilidade de uma frase em português ser traduzida para uma frase em inglês é calculada com base na co-ocorrência dessas frases no corpus paralelo. Para a geração do modelo de língua, a probabilidade de uma frase em inglês é calculada com base na ocorrência dessa frase na parte em inglês do corpus paralelo (ou de outro corpus monolíngue na língua-alvo).\\nGeralmente os modelos de tradução e de língua consideram frases de um tamanho máximo definido como parâmetro do treinamento. Quanto maior o tamanho da frase, maior é o contexto sendo considerado e, como consequência, mais coerente poderá ser a sentença gerada (Jurafsky; Martin, 2023). Porém, quanto maior o tamanho da frase, maiores serão o tempo e a quantidade de processamento necessários para realizar o treinamento dos modelos.\\nAs vantagens dos modelos estatísticos são que, com mais dados utilizados no treinamento dos sistemas, não apenas a qualidade geral aumenta, mas, com o uso de um modelo de linguagem, as traduções estatísticas ganharam fluência em relação às abordagens anteriores. Além disso, a TA estatística permitiu um uso mais eficiente de recursos humanos e de dados. As desvantagens dessa abordagem são o custo de criação de corpora paralelos, especialmente para idiomas com recursos limitados. Além disso, a TA estatística tende a ter dificuldade com pares de idiomas com ordem de palavras diferentes.\\nEm 2007, o sistema open-source PBSMT mais famoso, desenvolvido por Koehn et al. (2007), foi lançado: o Moses SMT toolkit. Ao mesmo tempo, o Google lançou seu famoso Google Tradutor com abordagens estatísticas. Vale ressaltar que os modelos estatísticos conseguiram obter grande sucesso devido ao “aumento do poder de computação e armazenamento de dados, juntamente com a disponibilidade cada vez maior de recursos de texto digital como consequência do crescimento da Internet” (Koehn, 2009, p. 18). Devido à eficiência e precisão da abordagem estatística em relação às anteriores, ela se tornou a abordagem mais amplamente utilizada naquela época. Sistemas de tradução estatística baseada em frases (PBSMT) como os de Koehn; Och; Marcu (2003) e Och; Ney (2004) eram o estado da arte até serem sucedidos pela tradução neural, a partir de 2015. De fato, a estratégia por trás do tradutor automático do Google foi a PBSMT por uma década (aproximadamente de 2006/2007 até 2016/2017)18. Atualmente, o Google e praticamente todos os sistemas de tradução online, bem como pesquisas nesta área usam a tradução neural (neural machine translation, NMT) ou algum sistema híbrido (estatístico e neural).\\n\\n\\n18.2.6 Tradução Automática Neural\\nOs sistemas de TA neural (em inglês, Neural Machine Translation ou NMT) foram introduzidos pela primeira vez na década de 1990 com alguns artigos sugerindo como redes neurais poderiam ser usadas para TA19 (Way; Forcada, 2018). No entanto, a quantidade dos dados usados para treinar esses modelos não era suficiente para produzir resultados razoáveis e, além disso, “a complexidade computacional envolvida excedia em muito os recursos computacionais daquela época, e, portanto, a ideia foi abandonada por quase duas décadas” (Koehn, 2020, p. 39).\\nEm geral, os modelos neurais consistem na construção de redes neurais end-to-end que mapeiam textos paralelos alinhados e são treinados para maximizar a probabilidade de uma sequência alvo Y, dada uma frase de origem X, sem informações linguísticas externas adicionais (Castilho et al., 2017b). Os sistemas neurais podem ser construídos com apenas uma rede em vez de uma sequência de tarefas separadas, como seu predecessor (a tradução estatística).\\nCom a publicação de resultados impressionantes em avaliação automática (Bahdanau; Cho; Bengio, 2015; Bojar et al., 2016; Sennrich; Haddow; Birch, 2016), os sistemas neurais geraram grande expectativa, especialmente porque a indústria de tradução busca melhorar a qualidade da TA para minimizar custos (Moorkens, 2017). A adoção dos sistemas neurais nos últimos anos tem sido extensiva, com um número crescente de provedores de TA e grupos de pesquisa concentrando seus esforços e recursos no desenvolvimento e implantação de sistemas NMT (Castilho et al., 2019).\\nNa tradução neural, redes neurais artificiais são usadas para fazer a tradução de uma sentença-fonte para uma sentença-alvo. Uma rede neural artificial pode ser entendida como uma composição de diversas unidades de processamento (os neurônios artificiais) conectadas entre si, em camadas. Cada unidade de processamento recebe uma entrada numérica e gera uma saída numérica. A saída é calculada de acordo com os “pesos” (\\\\(w\\\\)) e as “entradas” (\\\\(x\\\\)) associados à unidade e uma função que determina como a saída deve ser calculada. Por exemplo, vamos supor que um neurônio artificial seja governado pela função \\\\(x^2\\\\). Nesse caso, se a entrada para esse neurônio for o número \\\\(2\\\\) então a saída será \\\\(4\\\\), se for \\\\(3\\\\) a saída será \\\\(9\\\\), se for \\\\(-1\\\\) a saída será \\\\(1\\\\) e assim por diante. Os pesos são usados para ajustar o aprendizado do neurônio e são uma das partes mais importantes da definição de uma rede neural artificial.\\nNa tradução neural, diversas camadas de neurônios são usadas para aprender como traduzir uma sentença-fonte em uma sentença-alvo a partir de um corpus paralelo. Esse aprendizado geralmente demanda muito poder computacional20 e tempo, pois é realizado a passos pequenos em diversos ciclos de processamento21 das sentenças paralelas. E qual é a principal diferença metodológica da tradução neural para a estatística? Na NMT toda a sentença-fonte é considerada no aprendizado, de uma vez, e nos dois sentidos (da esquerda para a direita e da direita para a esquerda), ou seja, não há a quebra em frases como ocorria na PBSMT, nem a divisão clara entre modelo de tradução e modelo de língua. Dessa forma, a tradução gerada por um sistema NMT tende a ser mais fluente e natural, como ilustrado na Figura\\xa018.122.\\n\\n\\nFigura\\xa018.1: Tradução gerada pelo Google tradutor (tradução neural)\\n\\n\\n\\nOs modelos de tradução neural baseiam-se fortemente em duas tecnologias que se tornaram bastante usuais em PLN: embeddings e modelo de atenção. As embeddings (Capítulo\\xa010), são formas de representação de unidades lexicais (geralmente palavras) nas quais as unidades são mapeadas para vetores em um espaço de n (100, 300 ou mais) dimensões. Ao representar palavras como vetores densos notou-se que é possível mapear características linguísticas (morfológicas, sintáticas e semânticas) nesse espaço vetorial. Por exemplo, na Figura\\xa018.2 é possível observar a proximidade semântica da palavra “avô” com outras palavras a partir das word embeddings do NILC23 como “pai”, “tio”, “sobrinho” etc.\\n\\n\\nFigura\\xa018.2: Vizinhos mais próximos da palavra “avô” obtidos via consulta às word embeddings do NILC geradas usando o GloVe e dimensão 300.\\n\\n\\n\\nUsando as word embeddings bilíngues português e inglês do MUSE24 é possível observar as similaridades entre línguas como ilustrado na Figura\\xa018.3, onde as palavras em português aparecem em vermelho e as palavras em inglês, em azul. Essas word embeddings são usadas como forma de representação da língua nos modelos neurais de tradução automática.\\n\\n\\nFigura\\xa018.3: Visualização, em duas dimensões, das palavras em português (em vermelho) das palavras que ocorrem na sentença de exemplo e algumas possíveis traduções para o inglês (em azul).\\n\\n\\n\\nAssim, a tradução neural não se baseia na combinação dos modelos de tradução e de língua, como faz a tradução estatística, mas sim em um modelo sequencial que prediz uma palavra por vez. O potencial deste modelo sequencial está na maneira como ele prediz as palavras: considerando toda a sentença-fonte e também o que já foi produzido para a sentença-alvo. Desde sua proposição, a modelagem sequencial neural passou por várias arquiteturas, indo desde as redes neurais recorrentes (em inglês, recurrent neural network ou RNN) usadas para codificação (em inglês, encoder) e decodificação (em inglês, decoder) até os mecanismos de atenção (em inglês, attention mechanism) (Bahdanau; Cho; Bengio, 2015) que permitem ao decodificador focar em partes específicas da sentença de entrada em seu processo de geração da saída.\\nNesse momento, os Transformers (Vaswani et al., 2017) são o estado da arte na tradução. A Figura\\xa018.4 ilustra a tradução da sentença de exemplo, em português, para inglês usando Transformers25. Nessa ilustração, quanto mais clara (amarelo, verde claro, azul claro etc.) a célula que une a linha da palavra em inglês com a coluna da palavra em português, maior a “força” da relação entre elas. Por exemplo, observa-se uma forte relação entre “my” e “meu”, “grandfather” e “avô”, “home” e “casa”, e “beautiful” e “linda”.\\n\\n\\nFigura\\xa018.4: Visualização de um modelo de atenção usado para traduzir a sentença de exemplo.\\n\\n\\n\\nContudo, assim como todas as demais abordagens, a tradução neural também tem suas limitações. Uma delas é que, diferentemente da PBSMT onde é possível “olhar” para os modelos aprendidos e entender o que foi usado na tradução (como as frases da Tabela\\xa018.1), a tradução neural é considerada uma caixa-preta (black box): entender o que pode ter sido considerado para gerar a tradução depende de desvendar uma visualização do modelo de atenção (como o da Figura\\xa018.4), já que as previsões dos modelos neurais consistem em milhões de parâmetros. Isso dificulta a extensão dos modelos previamente treinados e coloca em dúvida a robustez do sistema. Além disso, por ser uma abordagem relativamente nova, a tradução neural ainda enfrenta alguns desafios, como o desempenho ruim em condições fora do domínio e para idiomas com recursos limitados.\\nAlém disso, é possível observar que as estratégias de tradução baseadas em corpus são fortemente influenciadas pelo corpus usado no treinamento. Por exemplo, os modelos estatísticos só terão a capacidade de traduzir uma palavra se ela tiver ocorrido um número significativo de vezes no corpus de treinamento, caso contrário não haverá uma frase correspondente contendo essa palavra e o sistema não conseguirá gerar uma tradução completa para a sentença original. No caso da tradução neural, isso é um pouco amenizado pelo uso de embeddings de unidades menores (em inglês, subword units) do que as palavras, as quais conseguem aproximar palavras desconhecidas às possíveis correspondências conhecidas26. Por exemplo, se o “a” for esquecido no “linda” da Figura\\xa018.1 o Google tradutor consegue gerar a mesma saída, sem problema. O tratamento de palavras e subwords é abordado no Capítulo\\xa04.\\nOutro ponto a se observar é que os sistemas neurais precisam de um corpus maior e de melhor qualidade do que os estatísticos, pois eles são rápidos em memorizar exemplos mal-formados (Khayrallah; Koehn, 2018). Por isso, para algumas línguas com menos recursos (em inglês, low-resourced languages) os sistemas estatísticos ainda podem apresentar um desempenho melhor do que alguns sistemas neurais. E, por esse motivo, muitas pesquisas atuais têm enfatizado o desenvolvimento de técnicas de aumento de dados (em inglês, data augmentation) para sistemas neurais.\\nApesar disso, os sistemas neurais são o estado da arte na área de tradução automática (agora em 2023), apresentando, especialmente, uma fluência muito superior aos sistemas estatísticos, o que dificulta a avaliação humana da tradução, a qual deve ser mais cuidadosa aos erros de acurácia.\\nAtualmente, existem várias técnicas de aprendizado profundo para os sistemas neurais, diferentes ramos, orientações de pesquisa e tendências27.\\n\\n\\n\\n18.3 Avaliação da Tradução Automática\\nDevido à importância da tradução no mundo globalizado de hoje, o interesse na avaliação da qualidade da tradução (AQT – do inglês, Translation Quality Assessment ou TQA) cresceu a ponto de a avaliação da TA (ATA) se tornar um subcampo em rápido crescimento dentro da TA. No entanto, como a tradução é um processo multifacetado que envolve fatores cognitivos, linguísticos, sociais, culturais e técnicos, definir e medir a qualidade da tradução também reflete essa complexidade (Castilho et al., 2018). A AQT tem sido um tópico muito discutido em estudos de tradução, tecnologia da tradução e na indústria de tradução e localização, mas ainda não há muito consenso sobre o que é e como ela deve ser feita (Castilho et al., 2018).\\nAqui, faremos a distinção entre a AQT e a ATA: enquanto a AQT abrange a avaliação tanto das traduções humanas quanto das traduções automáticas, a ATA se concentra exclusivamente na avaliação da qualidade dos sistemas de TA.'}]\n",
            "\n",
            "\n",
            "Resumo do trecho 1 foi bem-sucedido. Segue o resumo: A tradução automática estatística baseada em frases (PBSMT) alinha fragmentos de frases e palavras no texto-fonte a frases no texto-alvo, utilizando modelos de tradução e de língua para determinar a melhor tradução. Esses modelos são treinados a partir de grandes quantidades de textos paralelos, e a probabilidade de uma frase ser traduzida é calculada com base na co-ocorrência das frases no corpus paralelo. A PBSMT foi amplamente utilizada até a introdução da tradução automática neural (NMT) em 2015. A NMT utiliza redes neurais artificiais para traduzir sentenças, considerando toda a sentença-fonte de uma vez e usando embeddings e modelos de atenção para melhorar a qualidade da tradução. No entanto, a NMT também possui limitações, como a dificuldade de interpretação dos modelos e o desempenho ruim em condições fora do domínio e para idiomas com recursos limitados. A avaliação da tradução automática é um campo em crescimento, mas ainda não há consenso sobre como medir a qualidade da tradução. \n",
            "\n",
            "\n",
            "[{'role': 'system', 'content': 'Você é uma inteligência artificial que irá resumir pedaços de texto de um livro sobre Processamento de Linguagem Natural, portanto, você deve capturar explicações e pontos importantes para o entendimento e mantendo uma linearidade no resumo.'}, {'role': 'user', 'content': 'Nesta Seção, iremos definir a avaliação da TA, apresentar diferentes abordagens e discutir algumas das avaliações mais influentes na sua história, destacando a importância de realizar a avaliação dos sistemas de TA.\\n\\n18.3.1 O que é Avaliação da Tradução Automática?\\nA avaliação da Tradução Automática (ATA) é a prática de analisar a saída de tradução de um sistema (ou sistemas) de TA e julgar a qualidade dessa tradução com base em critérios estabelecidos. As abordagens para a ATA incluem avaliação automática, usando métricas automáticas Seção\\xa018.3.3, ou avaliação manual humana Seção\\xa018.3.4, realizada por pessoas, e às vezes uma combinação das duas. O fluxograma da Figura\\xa018.5 foi proposto por Doherty et al. (2018) para ajudar educadores e tradutores nos vários tipos de ATA.\\n\\n\\nFigura\\xa018.5: Fluxograma da ATA baseado em Doherty et al. (2018)\\n\\n\\n\\nDevido à falta de consenso no campo dos estudos de tradução e na indústria da tradução sobre o que constitui uma “boa” tradução, várias abordagens para a avaliação da TA surgiram. Portanto, a ATA pode ser realizada de várias maneiras diferentes, com várias abordagens, e não existe um único método que seja suficiente para abordar todos os propósitos de avaliação (Hovy; King; Popescu-Belis, 2002).\\nTradicionalmente, a avaliação da TA foi dividida em dois paradigmas: avaliação glass-box (caixa de vidro) e avaliação black-box (caixa-preta). Enquanto a primeira se preocupada com “a qualidade de um sistema com base em suas propriedades internas” (Dorr et al., 2011, p. 744) e foi amplamente utilizada com sistemas baseados em regras (Seção\\xa018.2.2), a última “mede a qualidade de um sistema apenas com base em sua saída, sem considerar os mecanismos internos do sistema de tradução” (ibid). As abordagens de avaliação black-box são o foco desta Seção.\\nA avaliação desempenha um papel essencial na TA, pois fornece informações sobre o funcionamento do sistema, quais as partes são eficazes e quais as áreas que precisam de melhorias. No entanto, a avaliação é um problema complexo, pois não existe uma única tradução correta para uma determinada fonte, e “pode haver várias traduções corretas possíveis” (ibid).\\nPara julgar a qualidade de uma determinada saída de TA, é necessário definir o que a “qualidade” significa para essa tarefa de tradução especificamente. Mas quem define o que é qualidade da tradução? A resposta para essa pergunta, definindo o que constitui uma tradução “boa” ou “ruim”, tem sido e ainda é amplamente debatida nos diferentes campos relacionados à tradução. Para O’Brien et al. (2011, p. 55), a qualidade está relacionada à opinião do cliente, mas na indústria da tradução, a avaliação de qualidade “é gerenciada por intermediários na cadeia de fornecimento e demanda”, geralmente usando uma abordagem única para todos (em inglês, one size fits all). Para a ATA, a avaliação de qualidade depende do uso pretendido da tradução. Portanto, qualidade pode significar uma tradução fluente que pareça ter sido escrita por um falante nativo; ou uma tradução precisa que transmita todo o significado expresso no texto de origem; ou talvez qualidade signifique uma tradução que seja ao mesmo tempo fluente e adequada. Além disso, qualidade pode ser definida como uma tradução fácil de ser pós-editada por alguns, enquanto outros podem definir qualidade como uma tradução que os usuários finais possam usar. E alguns podem ainda querer que todos esses critérios sejam atendidos para se ter uma tradução de qualidade, enquanto outros podem definir qualidade da tradução com outros critérios totalmente diferentes.\\nEm resumo, ao avaliar a saída da TA, essencialmente estamos tentando determinar graus de “qualidade” e “não qualidade” dessa tradução para um público específico, uma vez que um critério de qualidade que seja crucial em determinada cenário, pode ser irrelevante em outros (Dorr et al., 2011).\\nNesse sentido, alguns projetos de tradução que merecem referência são:\\n\\nALPAC Report (1966): examinou a qualidade e eficácia dos sistemas de TA nos Estados Unidos. O relatório concluiu que a TA não era útil devido à baixa qualidade, o que resultou em uma redução significativa no financiamento para o processamento de linguagem natural (PLN) e TA naquela época. O relatório também desencorajou discussões sobre avaliação dentro da comunidade de PLN por muitos anos.\\nDARPA Initiative (1992): uma das primeiras iniciativas na avaliação de TA nos anos 1990. A metodologia de avaliação era baseada em julgamentos humanos, onde os avaliadores atribuíram uma pontuação de 1 a 5 para as frases traduzidas automaticamente, comparando-as ao texto original e a traduções de referência humanas produzidas por tradutores profissionais.\\nFEMTI (2002): foi um esforço para padronizar o processo de avaliação de TA, organizando os métodos existentes de ATA e relacionando-os com o propósito e contexto dos sistemas, fornecendo “diretrizes para a seleção de características de qualidade a serem avaliadas, dependendo da tarefa esperada, dos usuários e das características de entrada de um sistema de tradução automática” (Estrella; Popescu-Belis; King, 2009).\\nGALE (2005): teve como objetivo desenvolver e aplicar tecnologias de software para TA de grandes volumes de fala e texto em várias línguas. Foi estabelecido um protocolo de avaliação que utilizava métodos automáticos, humanos, baseados em tarefas e semi-automáticos, com foco na taxa de erro de tradução mediada por humanos (HTER).\\nEuroMatrix (2006): financiado pela Comissão Europeia, esta iniciativa construiu sistemas de TA estatísticos e híbridos para diversos idiomas europeus. A avaliação dos sistemas de tradução automática incluiu avaliações automáticas com base em onze métricas, incluindo BLEU, METEOR, Translate Error Rate, Word Error Rate, entre outros (Seção\\xa018.3.3); e avaliações humanas que consistiram em classificações de fluência e adequação, ranking, tempo de leitura, tempo de pós-edição, teste de preenchimento de lacunas, clareza e informatividade (“Euromatrix. Survey of Machine Translation Evaluation”, 2007).\\nWMT (2006): começou como o Workshop de Tradução Automática Estatística e progrediu para a Conferência de Tradução Automática em 2016, tem sido uma grande contribuinte para o avanço da pesquisa em avaliação de TA. Ao longo dos anos, a WMT tem utilizado diferentes metodologias de avaliação. Desde 2008, a WMT realiza uma tarefa compartilhada, ou esforço colaborativo, (em inglês, shared task) de avaliação que compara uma ou mais traduções de referência com as saídas dos sistemas de TA e tem utilizado, principalmente, a métrica BLEU, embora essa tenha sido superada por diversas métricas diferentes. Em relação à avaliação manual, a WMT experimentou e implementou uma ampla gama de metodologias, incluindo julgamentos de fluência e adequação (2006 e 2007), diferentes avaliações de classificação (2007-2016), pós-edição e compreensão de frases (2009-2010), e avaliação direta e conjuntos de testes (desde 2016). Além disso, a shared task da WMT vem realizando avaliações em nível de documento desde 2019.\\nTAUS (2005): fundada em 2005 como uma rede de dados linguísticos com um extenso repositório de dados e uma rede de engenharia de linguagem humana, a TAUS tem estado na vanguarda das tentativas de estabelecer indicadores para a avaliação da qualidade de tradução, desenvolvendo o Framework de Avaliação Dinâmica de Qualidade (DQF) em 2011. O modelo é uma estrutura de avaliação baseada em diferentes tipos de conteúdo, propósito do uso, ferramentas, processos e outros aspectos.\\nQTLaunchPad (2012): projeto colaborativo financiado pela União Europeia (2012-2014) que reuniu e forneceu dados e ferramentas para a avaliação da qualidade de tradução, além de métricas de qualidade compartilhadas para tradução humana e automática (Uszkoreit; Lommel, 2013). O projeto contribuiu para o projeto de TA QT21 (2015-2018), cujo objetivo era desenvolver uma avaliação aprimorada. O QTLaunchPad desenvolveu o framework de Métricas de Qualidade Multidimensionais (MQM), que descreve e define métricas personalizadas de qualidade de tradução para avaliar a qualidade de textos traduzidos. MQM é amplamente utilizado como um padrão para métricas de avaliação humana em MTEval. Como parte do QT21, o MQM e o DQF (da TAUS) foram harmonizados, e atualmente, a Tipologia de Erros DQF da TAUS é um subconjunto reconhecido do MQM.\\n\\n\\n\\n18.3.2 A importância de uma avaliação replicável\\nComo visto neste Capítulo, a avaliação da TA tem sido realizada desde o surgimento da própria TA. No entanto, o relatório ALPAC tornou a avaliação quase um tópico proibido na comunidade de PLN (Paroubek; Chaudiron; Hirschman, 2007), e até hoje, a avaliação de TA, especialmente a avaliação humana, é considerada “desnecessária” em alguns momentos. Não é raro encontrar artigos de pesquisa usando uma única métrica para avaliar seu sistema (geralmente pontuações BLEU – ver Seção\\xa018.3.3, e ainda mais comum são os artigos de pesquisa com avaliação humana muito limitada e falha, e até mesmo artigos de pesquisa sem avaliação humana alguma (Marie; Fujita; Rubino, 2021).\\nO avanço dos sistemas de TA gerou muita expectativa na comunidade, especialmente porque a indústria de tradução buscava uma melhoria na qualidade da TA para reduzir custos (Moorkens, 2017). À medida que os sistemas de TA neural se tornaram líderes no mercado, com uma clara melhoria na qualidade das traduções, surgiram reivindicações exageradas de que esses sistemas estavam “preenchendo as lacunas entre a tradução humana e a tradução automática” (Wu et al., 2016). Diante disso, pesquisadores na área de avaliação de TA alertaram a comunidade para ter cautela e não fazer promessas exageradas, além de enfatizarem a necessidade de mais pesquisas para lidar com as limitações dos sistemas de TA e realizar avaliações humanas mais abrangentes (Castilho et al., 2017b).\\nNo entanto, as afirmações exageradas continuaram, com alguns declarando que seu sistema de TA neural havia atingido a “paridade humana” (em inglês, human parity) (Hassan et al., 2018) e outros alegando que a TA é um problema “resolvido” com uma qualidade de tradução “quase perfeita”. Como resposta, a fim de verificar essas alegações, dois estudos independentes (Toral et al., 2018) e (Läubli; Sennrich; Volk, 2018) reavaliaram os dados utilizados por Hassan et al. (2018) e descobriram que a escolha dos avaliadores, o contexto linguístico e a criação de traduções de referência têm um impacto significativo na avaliação de qualidade, apontando para falhas nas práticas atuais em avaliação de TA.\\nComo resultado, os pesquisadores de avaliação TA passaram a se envolver em discussões mais aprofundadas sobre a necessidade de aprimorar continuamente a metodologia de avaliação, “com o objetivo de superar as limitações das métricas automáticas e das abordagens humanas, evitar superestimativas na capacidade da TA e explicar os resultados aparentemente contraditórios da TA de forma mais abrangente” (Castilho et al., 2019, p. 2). Além disso, os pesquisadores começaram a buscar diretrizes para avaliar a “paridade humana” na avaliação da TA (Läubli et al., 2020).\\nEntão, quem precisa de avaliação de TA? Em resumo, todos os campos diferentes relacionados à tradução. Algumas questões precisam ser abordadas para que a avaliação seja confiável, tais como:\\n\\nO que a qualidade significa nesta avaliação? Como vimos, há um grande debate sobre a definição de qualidade, então, antes de avaliar um sistema de TA, o pesquisador precisa definir o que seria uma tradução “boa” e “ruim” nesse cenário.\\nQue tipo de sistema está sendo avaliado? Dependendo do par de idiomas, os sistemas estatísticos podem mostrar um problema específico. Já os sistemas neurais são conhecidos por serem fluentes e conhecer a similaridade entre palavras, o que torna mais difícil detectar erros.\\nQual é o objetivo a ser alcançado com esta avaliação? Saber por que o sistema de TA está sendo avaliado é importante para decidir que tipo de avaliação precisa ser realizada. Por exemplo, se alguém quer saber se as implementações realizadas em um sistema de TA feito para lidar com expressões multipalavras resultaram em traduções corretas dessas expressões, uma análise linguística aprofundada da saída seria mais apropriada do que avaliar a fluência da saída.\\n\\nComo podemos ver, a avaliação de TA é um problema complexo, e não é surpresa que tenha se estabelecido como um campo independente. Com os avanços na qualidade dos sistemas de TA atuais, a comunidade de avaliação precisa estar atualizada com os procedimentos que possam fornecer avaliações mais sólidas, capazes de confirmar ou refutar as alegações feitas, como afirmou Carl Sagan: “Alegações extraordinárias exigem evidências extraordinárias”.\\n\\n\\n18.3.3 Métricas Automáticas\\nComo vimos anteriormente, a ATA assume uma complexidade intrínseca devido a uma multiplicidade de fatores. Tipicamente, encontramos duas vertentes de avaliação: a avaliação automática e a avaliação humana (manual), ocasionalmente mescladas para compor uma abordagem híbrida. Nesta Seção, abordaremos as métricas automáticas mais predominantes, reconhecendo a sua influência no domínio da TA.\\nAs métricas automáticas pioneiras empregadas na TA tiveram origem em outras áreas da PLN. Por exemplo, temos a Word Error Rate (WER), introduzida por Su; Wu; Chang (1992), que originou-se do campo de reconhecimento da fala (Capítulo\\xa02). Por sua vez, a ROUGE, desenvolvida por Lin (2004), teve sua origem na sumarização automática. Outra métrica muito usada anteriormente foi a F-measure, empregada em recuperação de informação (Capítulo\\xa016) e em diversas outras áreas do PLN, também encontrou aplicabilidade nesse contexto.\\nAs Métricas de Avaliação Automática (MAA) são adotadas na TA quando se busca evitar a intervenção humana direta (Figura\\xa018.5). As MAAs atuam como programas computacionais, recebendo as traduções de um sistema de TA e as traduções de referência (TR) como entrada, e produzindo uma pontuação numérica que reflete a similaridade entre as traduções de TA e TR.\\nA maioria das MAAs é classificada como métricas de referência (em inglês, reference-based metrics), exigindo a disponibilidade da TR, isto é, a tradução humana do texto em avaliação, a fim de serem empregadas como ponto de comparação. No entanto, abordagens mais recentes incorporam modelos de linguagem pré-treinados para medir a semelhança entre uma tradução gerada e um conjunto de referências. Nesse contexto, essas métricas medem a similaridade entre as representações semânticas das palavras e as frases presentes tanto nas traduções geradas quanto nas referências, utilizando recursos linguísticos capturados durante o pré-treinamento desses modelos, em detrimento de uma comparação direta com traduções humanas específicas.\\nAs MAAs preponderantes na área da TA são aquelas que operam sobre características lexicais e dispensam a necessidade de treinamento (em inglês, untrained). Essas métricas, geralmente baseadas em similaridades (em inglês, matching) e diferença de edições (em inglês, edit distance) entre o resultado da TA e a TR, avaliam a sobreposição entre a hipótese (resultado da TA) e a TR. Tal avaliação contempla tanto a precisão quanto a abrangência dos elementos lexicais (Lee et al., 2023). Duas vertentes são identificadas entre as métricas lexicais: as word-based (baseadas em palavras), que analisam as similaridades entre palavras; e as character-based (baseadas em caracteres), que investigam a similaridade entre caracteres.\\nAs métricas lexicais word-based mais amplamente empregadas permitem medir tanto a similaridade dos n-grama quanto a distância de edição (edit distance). Dentre as métricas baseadas em n-grama, destacam-se as amplamente conhecidas BLEU (Papineni et al., 2002), METEOR (Banerjee; Lavie, 2005) e NIST (Doddington, 2002). Por outro lado, as métricas que calculam a distância de edição e que têm destaque são TER (e HTER) (Snover et al., 2006) e WER (Su; Wu; Chang, 1992). Vale mencionar a singularidade da métrica chrF (Popović, 2015), a qual, além de ser character-based, também mede a similaridade dos n-gramas.\\nMais recentemente, métricas treinadas em modelos baseados em redes neurais usando a arquitetura Transformer foram propostas.'}]\n",
            "\n",
            "\n",
            "Resumo do trecho 2 foi bem-sucedido. Segue o resumo: A avaliação da Tradução Automática (TA) é a prática de analisar a qualidade da tradução gerada por um sistema de TA, utilizando critérios estabelecidos. Existem abordagens automáticas e manuais para a avaliação, e muitas vezes uma combinação das duas é utilizada. A avaliação desempenha um papel essencial na melhoria dos sistemas de TA, fornecendo informações sobre seu desempenho e áreas que precisam de melhorias.\n",
            "\n",
            "A avaliação da TA pode ser realizada de várias maneiras diferentes, devido à falta de consenso sobre o que constitui uma \"boa\" tradução. Tradicionalmente, a avaliação da TA foi dividida em avaliação \"caixa de vidro\" e \"caixa-preta\". A avaliação \"caixa de vidro\" se concentra nas propriedades internas do sistema de TA, enquanto a avaliação \"caixa-preta\" avalia apenas a saída do sistema, sem considerar seus mecanismos internos.\n",
            "\n",
            "A avaliação da TA tem uma longa história, com várias avaliações influentes ao longo dos anos. O relatório ALPAC (1966) examinou a qualidade dos sistemas de TA nos Estados Unidos e resultou em uma redução significativa no financiamento para o processamento de linguagem natural e TA na época. A iniciativa DARPA (1992) foi uma das primeiras iniciativas na avaliação de TA e utilizou julgamentos humanos para avaliar as traduções automáticas. Outras avaliações importantes incluem o projeto GALE (2005), EuroMatrix (2006), WMT (2006), TAUS (2005) e QTLaunchPad (2012).\n",
            "\n",
            "A avaliação de TA é importante para todos os campos relacionados à tradução, mas é um problema complexo devido à falta de consenso sobre o que constitui uma tradução de qualidade. Além disso, a avaliação automática de TA é realizada por meio de métricas automáticas, como BLEU, METEOR e TER, que comparam as traduções geradas com traduções de referência. Essas métricas têm sido aprimoradas ao longo dos anos, mas ainda apresentam limitações.\n",
            "\n",
            "É importante que a avaliação de TA seja replicável e baseada em evidências. No entanto, muitas vezes a avaliação humana é considerada desnecessária e alguns estudos utilizam apenas uma métrica para avaliar seus sistemas. Com os avanços na qualidade dos sistemas de TA, é necessário aprimorar continuamente a metodologia de avaliação e evitar promessas exageradas sobre a capacidade da TA.\n",
            "\n",
            "Em resumo, a avaliação da TA é uma prática essencial para melhorar os sistemas de tradução automática. Existem diferentes abordagens e métricas para avaliar a qualidade da tradução, mas é importante considerar o contexto e definir o que constitui uma tradução de qualidade em cada cenário específico. \n",
            "\n",
            "\n",
            "[{'role': 'system', 'content': 'Você é uma inteligência artificial que irá resumir pedaços de texto de um livro sobre Processamento de Linguagem Natural, portanto, você deve capturar explicações e pontos importantes para o entendimento e mantendo uma linearidade no resumo.'}, {'role': 'user', 'content': 'Dentre essas, há as métricas supervisionadas (supervised-metrics) e as não-supervisionadas, dependendo da técnica de aprendizado, ambas categorias com word-embeddings e contextual-embeddings (Lee et al., 2023). Entre as métricas não-supervisionadas mais recorrentes, destacam-se MEANT (word embedding) (Lo; Wu, 2011), BERTscore (Zhang et al., 2020), Yisi (Lo, 2019) e BARTscore (Yuan; Neubig; Liu, 2021) (contextual-embedding). Entre as supervisionadas, estão a BEER (Stanojevic; Sima’an, 2014) e BLEND (Ma et al., 2017) (ambas word-embeddings), BERT for MTE (Shimanaka; Kajiwara; Komachi, 2019), BLEURT (Sellam; Das; Parikh, 2020) e COMET (Rei et al., 2020).\\nAs vantagens das MAAs são que elas são eficientes, econômicas e fornecem avaliações consistentes, ou seja, se a métrica for computada para a mesma tradução várias vezes, todas elas vão dar o mesmo resultado. No entanto, uma preocupação é a dependência exclusiva das similaridades entre a saída do sistema e as referências. Primeiramente, não há somente uma única tradução correta para um texto, sendo assim, o significado do texto pode ser traduzido de várias maneiras. Mas seriam todas as tradução “igualmente boas”? E o que “boa” significa nesse determinado contexto da tradução? Nesse caso, usar múltiplas TRs seria essencial para se ter uma avaliação mais justa. Segundo, as MAAs não oferecem insights detalhados sobre erros de tradução, pontos fortes e fragilidades de um sistema. Elas não dizem o que funciona no sistema, o que precisa ser melhorado; sendo o único objetivo medir a semelhança com a(s) referência(s), e consequentemente, as melhorias específicas decorrentes de modificações no sistema de tradução permanecem obscuras. E finalmente, o sistema com uma pontuação menor pode ser melhor na prática do que um sistema com uma pontuação mais alta. Enquanto as MAAs servem como ferramentas quantitativas valiosas, elas não revelam completamente as complexidades da qualidade da tradução. Uma abordagem mais abrangente, combinando MAAs com avaliações humanas e análises qualitativas, oferece uma compreensão mais profunda do desempenho dos sistemas de TA.\\nEmbora as MAAs não se revelem apropriadas para mensurar a qualidade final dos sistemas, impulsionam o avanço da pesquisa em TA, uma vez que podem ser empregadas de forma constante durante o desenvolvimento e a implementação desses sistemas. Em essência, as MAAs são medidas úteis na comparação entre sistemas de TA ou de versões de um mesmo sistema de TA, mas são limitadas na predição da qualidade da tradução.\\n\\n\\n18.3.4 Métricas Humanas\\nO processo de avaliar a qualidade da TA por meio da intervenção humana é essencial. Embora as MAAs proporcionem uma avaliação quantitativa, a avaliação humana oferece uma visão mais detalhada e uma análise mais ampla de fenômenos linguísticos complexos subjacentes ao desempenho dos sistemas de tradução, sendo assim imprescindível em uma compreensão mais abrangente dos sistemas de TA.\\nA avaliação humana pode ser feita através de diversos paradigmas, sendo os mais comuns o paradigma de fluência-adequação e pós-edição. A abordagem de ranqueamento de segmentos (em inglês, ranking) também é comumente empregada para a comparação dos sistemas de tradução, e possibilita a avaliação comparativa de diversos sistemas, fornecendo insights sobre a eficácia relativa de suas saídas. Igualmente, a anotação de erros, sob a forma de marcações específicas, oferece um feedback valioso sobre os sistemas em análise.\\nOutras abordagens incluem métricas secundárias, tais como legibilidade, compreensibilidade e usabilidade das traduções resultantes. Vale mencionar métricas centradas no usuário, que são avaliadas com testes de compreensão, por exemplo, que podem ser utilizados para aferir não apenas a fidelidade à tradução, mas também a transmissão efetiva da mensagem subjacente. Essa abordagem complementar permite uma apreciação mais holística da eficácia das traduções geradas.\\nImportante ressaltar que a avaliação humana é realizada por uma variedade de avaliadores, incluindo tanto profissionais quanto amadores. Essa diversidade de perspectivas pode contribuir para uma avaliação mais robusta e representativa da real eficácia dos sistemas de TA se empregados na mesma avaliação. Porém, também pode resultar em conclusões diferentes se a avaliação é feita só com um ou com o outro (só amadores ou só profissionais, por exemplo). Essa Seção vai abordar as métricas humanas mais comuns na avaliação de TA, assim como a importância de se calcular a concordância entre anotadores.\\n\\n18.3.4.1 Adequação: Fidelidade Semântica\\nA adequação (em inglês, “accuracy” ou “fidelity”), ou também “acurácia” ou “exatidão”, é uma métrica importante na avaliação humana da TA. Ela foca na fidelidade semântica entre o texto-fonte e a tradução, mostrando a profundidade e a precisão da transferência de significado, permitindo determinar se a mensagem que está sendo transmitida e seu sentido foram preservados de maneira precisa e fiel.\\nEssencialmente, a adequação investiga até que ponto a tradução transmite o significado do texto de origem para o texto-alvo. Nesse contexto, uma escala Likert28 é utilizada para classificar o nível de transferência semântica, onde geralmente uma pergunta é feita para o avaliador: “Até que ponto a tradução transfere o significado do texto-fonte para o texto-alvo?”. Como resposta, o avaliador pode escolher uma opção em uma escala que varia desde “Nada” até “Tudo”, com os graus intermediários de “Pouco” e “Muito”.\\nAs limitações da adequação é que ela não fornece informações sobre a fluência da tradução, deixando uma lacuna importante na avaliação. Em determinados casos, o foco reside exclusivamente no significado da sentença de origem, tornando a fluência uma preocupação secundária. Além disso, a adequação não oferece detalhes precisos sobre os erros presentes na tradução, o que pode dificultar a identificação de pontos específicos para a melhoria do sistema.\\n\\n\\n18.3.4.2 Fluência: Naturalidade e Estrutura\\nA fluência, ou “inteligibilidade” (em inglês, “fluency” ou “intelligibility”), é outra métrica importante na avaliação humana da TA, que se preocupa com a naturalidade e a estrutura do texto-alvo, revelando o grau de fluência e adaptabilidade da saida da TA às normas linguísticas e socioculturais da língua-alvo.\\nEssa avaliação foca diretamente no texto-alvo, priorizando a avaliação da gramática e dos aspectos estruturais da tradução. Essencialmente, ela investiga o quão natural e fluido é o fluxo do texto-alvo dentro do contexto da língua-alvo, considerando suas normas linguísticas e socioculturais específicas. Uma característica distintiva da dimensão de fluência é que ela pode ser avaliada independentemente do texto-fonte, uma vez que se concentra exclusivamente no resultado final da tradução.\\nTambém medida numa escala Likert, uma pergunta típica dirigida ao avaliador poderia ser: “Quão fluente está o texto-alvo, ou seja, como está o fluxo e a naturalidade do texto-alvo no contexto da língua-alvo e suas normas linguísticas e socioculturais em um dado contexto?”. A escala Likert pode variar desde “Sem fluência” até “Nativo”, incluindo os graus intermediários de “Pouca fluência” e “Quase nativo”.\\nEssa métrica oferece informação sobre a naturalidade da tradução e se ela soa natural e fluída para um falante nativo ou se exibe características de “linguagem quebrada”, prejudicando assim a experiência de leitura e compreensão.\\nAssim como a adequação, a avaliação da fluência também apresenta limitações. Ela não proporciona informações sobre a adequação da tradução, pois o foco está exclusivamente na fluência da sentença de destino, tornando a adequação uma preocupação secundária. Além disso, a fluência também não oferece detalhes precisos sobre os erros presentes na tradução.\\nPor esse motivo, é comum que as avaliações de adequação e fluência caminhem juntas, uma vez que é mais intuitivo avaliar uma em relação à outra. No entanto, há momentos em que pode ser necessário priorizar uma em detrimento da outra. Documentações técnicas, por exemplo, podem demandar uma maior ênfase na adequação, priorizando a transmissão precisa do significado.\\nAlgumas considerações sobre o uso de escalas Likert na avaliação humana são importantes. Vale a pena ressaltar que as escalas Likert podem apresentar complexidades na sua aplicação. Diversos tipos podem ser utilizados, como escalas numéricas, de janela deslizante (em inglês, sliding window) e afirmações (concordo/discordo). Embora essas escalas sejam facilmente compreensíveis e quantificáveis, elas carregam uma natureza subjetiva, pois falham em medir as atitudes reais dos respondentes, levantando a questão: qual a diferença exata entre uma pontuação 5 e uma 4? (Um erro? Dois erros?). Além disso, a presença de um número par de opções pode levar os participantes a escolherem o centro, demonstrando a delicadeza desse tipo de avaliação.\\n\\n\\n18.3.4.3 Ranqueamento: Hierarquia de Traduções\\nO ranqueamento (em inglês, ranking) tem como propósito classificar e comparar duas ou mais traduções, com o intuito de estabelecer uma hierarquia de qualidade entre elas. As comparações podem ser efetuadas tanto entre as traduções geradas por diferentes sistemas de TA quanto entre traduções geradas por humanos. Essa abordagem permite identificar nuances de qualidade, ressaltando as distinções entre as opções.\\nAlém disso, o ranqueamento pode incorporar a possibilidade de empates, quando duas ou mais traduções são avaliadas como equivalentes em qualidade, sendo categorizadas como “igualmente boas” ou “igualmente ruins”. A categorização desses empates enriquece a análise, oferecendo insights sobre o grau de qualidade comparativa.\\nNo âmbito de diagnósticos, esse tipo de classificação oferece a capacidade de indicar melhorias ao comparar o sistema avaliado com uma linha de base (baseline). Essa perspectiva não somente permite avaliar o progresso alcançado mas também identificar áreas específicas de aprimoramento, promovendo a constante evolução do sistema de TA.\\nUma aplicação prática do ranqueamento é a seleção do sistema mais adequado para um projeto específico. A análise hierárquica das traduções permite a escolha embasada no desempenho, assegurando que a tradução atenda de forma eficiente aos requisitos e objetivos do projeto em questão.\\nO uso do ranqueamento apresenta suas limitações a serem consideradas. Ele não oferece uma avaliação refinada, não detalhando os erros presentes nas traduções. Quando empates não são permitidos, traduções igualmente boas ou ruins podem ser classificadas de maneira diferente, ressaltando uma inconsistência na hierarquia.\\n\\n\\n18.3.4.4 Anotação de Erros: Taxonomias\\nA anotação de erros se destaca como um método essencial para identificar e classificar imperfeições presentes em textos traduzidos. Diversas taxonomias foram propostas para essa finalidade, como Vilar et al. (2006), Font Llitjós; Carbonell; Lavie (2005), Federico et al. (2014), Costa et al. (2015), DQF de TAUS (O’Brien et al., 2011) e MQM (Lommel; Melby, 2018) by QT212 (Doherty et al., 2013). Para o português brasileiro, Martins (2014) e Martins; Caseli (2015) trazem a adaptação das categorias de erros de Popovic; Burchardt (2011) e Vilar et al. (2006) para traduções português-inglês.\\nAs tipologias de erros frequentemente abrangem uma série de aspectos, incluindo palavras ausentes (em inglês, missing words), palavras adicionadas (em inglês, added/extra words), ordem errada das palavras (em inglês, word order), traduções literais (em inglês, literal translation), traduções erradas (em inglês, mistranslation), palavras incorretas (em inglês, incorrect words), formas inadequadas (em inglês, incorrect form), pontuação inadequada (em inglês, punctuation), entre outros, que podem incluir outras subcategorias específicas.\\nA adoção de taxonomias de erros na ATA oferece diversos benefícios, tais como identificar tipos específicos de erros nas saídas de TA, fornecer relatórios detalhados de erros para o aprimoramento dos sistemas, e fornecer informações aos clientes sobre a qualidade da tradução. Além disso, provedores de serviços linguísticos utilizam taxonomias e avaliações de severidade29 para monitorar o trabalho de tradutores. A anotação de erro também ajuda a investigar as relações entre tipos específicos de erros e as preferências de usuários ou pós-editores, bem como avaliar o impacto de diferentes tipos de erros em várias etapas do processo de pós-edição.\\nContudo, entre as principais limitações dessa estratégia, destaca-se que a anotação manual de erros é um processo caro e demorado, demandando um investimento significativo de tempo. Além disso, essa avaliação nem sempre é uma tarefa simples, especialmente quando se trata de diferenciar entre categorias como “tradução literal” e “tradução errada”.\\nOutra complexidade está associada à dependência da língua. Diferentes idiomas possuem particularidades que podem tornar a identificação e a classificação de erros uma tarefa mais desafiadora. Também é relevante considerar que a eficácia da anotação de erros pode variar de acordo com o tipo de abordagem de TA: enquanto ela pode ser mais adequada para sistemas de tradução baseados em regras (RBMT), pode não ser tão precisa para sistemas de tradução estatística (SMT) ou de tradução neural (NMT). Nesse contexto, a seleção da abordagem de avaliação mais adequada torna-se um ponto de reflexão. Além disso, a falta de consenso entre avaliadores é uma questão importante, frequentemente requerendo treinamento e prática para alcançar um nível satisfatório de concordância (Capítulo\\xa014).\\n\\n\\n18.3.4.5 Pós-Edição na Avaliação de TA\\nA pós-edição (PE) (do inglês, post-editing) é definida como “a correção da saída da tradução automática bruta por um tradutor humano, de acordo com instruções e critérios de qualidade específicos” (O’Brien, 2011, p. 197). A PE emerge como uma ferramenta fundamental na avaliação de sistemas de TA, oferecendo uma perspectiva mais aprofundada sobre o esforço envolvido nesse processo. A medição desse esforço pode ser abordada de diferentes perspectivas (Krings, 2001), proporcionando uma visão mais abrangente do desempenho do sistema e do impacto da TA no fluxo de trabalho.\\n\\nEsforço Temporal: mede o ritmo de pós-edição. Avaliando o tempo gasto na pós-edição por palavras por segundos é possível compreender a velocidade desse processo. Nesse contexto, uma eficiência temporal maior, ou seja, menos tempo gasto na pós-edição, pode indicar uma melhor qualidade da saída da TA, influenciando a produtividade.\\nEsforço Técnico: mede o número de operações de edição realizadas, como inserções (insertions), remoções (deletions), e trocas (shifts). Nesse sentido, a métrica hTER (Seção\\xa018.3.3) é frequentemente utilizada como uma estimativa aproximada do esforço técnico. Uma menor quantidade de edições necessárias está diretamente correlacionada a uma melhor qualidade da TA, uma vez que está ligada ao tempo de esforço e, consequentemente, à produtividade.\\nEsforço Cognitivo: pode ser medido por meio de diferentes abordagens, incluindo o rastreamento ocular (eye-tracking). A redução desse esforço cognitivo durante o processo de pós-edição é indicativa de uma qualidade superior da TA, e tal esforço tem sido correlacionado a outras métricas de avaliação humana.\\n\\nA utilização da PE na avaliação da TA é motivada por diversos fatores. Além de avaliar a utilidade do sistema de TA em produção, ela permite identificar erros comuns e gerar novos dados de treinamento ou teste. Contudo, é importante ressaltar que as medidas de esforço de PE tendem a variar entre avaliadores novatos (estudantes) e profissionais, bem como entre o público em geral e profissionais experientes.\\nAlguns trabalhos com pós-edição com o portguês incluem: De Sousa; Aziz; Specia (2011); Almeida (2013), Castilho et al. (2014), Moorkens et al. (2015), Castilho et al. (2017a), Silva et al. (2017), Castilho; Resende; Mitkov (2019), Castilho; Resende (2022). Há também os trabalhos que investigaram a automatização do processo de pós-edição para o português: Caseli; Inácio (2020).\\n\\n\\n18.3.4.'}]\n",
            "\n",
            "\n",
            "Resumo do trecho 3 foi bem-sucedido. Segue o resumo: As métricas de avaliação automática (MAAs) são usadas para medir a qualidade da tradução automática (TA) de forma rápida e eficiente. Existem duas categorias de métricas: supervisionadas e não supervisionadas, que podem ser baseadas em word-embeddings ou contextual-embeddings. As MAAs são úteis para comparação entre sistemas de TA, mas têm limitações na predição da qualidade da tradução final.\n",
            "\n",
            "A avaliação humana é essencial para uma compreensão mais detalhada e abrangente dos sistemas de TA. Ela pode ser feita através de diferentes paradigmas, como fluência-adequação, pós-edição, ranqueamento e anotação de erros. Além disso, métricas secundárias, como legibilidade, compreensibilidade e usabilidade, podem ser consideradas.\n",
            "\n",
            "A adequação é uma métrica que avalia a fidelidade semântica entre o texto-fonte e a tradução. A fluência, por sua vez, avalia a naturalidade e a estrutura do texto-alvo. O ranqueamento permite estabelecer uma hierarquia de qualidade entre diferentes traduções, enquanto a anotação de erros identifica e classifica os erros presentes nas traduções.\n",
            "\n",
            "A pós-edição é uma ferramenta importante na avaliação de sistemas de TA, pois oferece uma visão mais aprofundada do esforço envolvido no processo de tradução. O esforço pode ser medido em termos de tempo, operações de edição e esforço cognitivo. A pós-edição também pode ser usada para gerar novos dados de treinamento ou teste.\n",
            "\n",
            "É importante considerar que a avaliação humana pode variar dependendo dos avaliadores e das línguas envolvidas. Além disso, a anotação de erros é um processo caro e demorado, e a escolha da abordagem de avaliação mais adequada é um ponto de reflexão. \n",
            "\n",
            "\n",
            "[{'role': 'system', 'content': 'Você é uma inteligência artificial que irá resumir pedaços de texto de um livro sobre Processamento de Linguagem Natural, portanto, você deve capturar explicações e pontos importantes para o entendimento e mantendo uma linearidade no resumo.'}, {'role': 'user', 'content': '6 Considerações Finais sobre Avaliação Humana na Avaliação de TA\\nAo explorar a avaliação humana na avaliação de TA, encontramos uma série de questões metodológicas e pragmáticas que merecem reflexão:\\n\\nPrecisamos sempre avaliar tanto a adequação quanto à fluência?\\nQuantos avaliadores são necessários e quais as competências linguísticas eles devem possuir?\\nDevemos envolver tradutores, linguistas ou especialistas no domínio? O viés em cada escolha é um ponto a ser considerado.\\nQuantos pontos devem ser incluídos em uma escala Likert de avaliação?\\nQual o grau de concordância entre avaliadores (Capítulo\\xa014) e a consistência nas avaliações individuais?\\n\\nConsiderações pragmáticas também emergem, incluindo o custo associado aos avaliadores, à geração de textos de referência traduzidos por humanos, e à qualidade dessas referências. O tempo investido, a baixa concordância intra e inter-avaliadores, e a questão de saber se o objetivo da avaliação é apenas avaliar melhorias em um sistema são fatores preponderantes.\\n\\n\\n\\n18.3.5 Avaliação dependente de contexto\\nComo vimos nas seções anteriores, a avaliação de TA começou desde o princípio da área com projetos como DARPA, que avaliaram a qualidade das traduções com métricas humanas. À medida que a área avançou, métricas automáticas foram gradualmente incorporadas, inicialmente provenientes de outros domínios do PLN, e posteriormente desenvolvidas especificamente para o campo da TA.\\nE apesar de ambas as MAAs e as métricas humanas serem o estado da arte na avaliação, à medida que os sistemas neurais de TA evoluíram, tornando-se mais complexos e produzindo traduções de maior qualidade, surgiu a necessidade de uma avaliação mais abrangente e rigorosa que levasse em conta fatores diversos. Adicionalmente, com o aumento nos esforços direcionados à incorporação de contexto nos sistemas de TA neurais, viu-se a necessidade de também se ter uma avaliação com contexto, uma vez que, os resultados obtidos na avaliação com sentenças eram limitados, pois ela não é capaz de identificar as melhorias desses sistemas (Läubli; Sennrich; Volk, 2018; Toral et al., 2018). Ademais, as MAAs subestimam a qualidade dos sistemas NMT (Shterionov et al., 2018), e a credibilidade dessas métricas para sistemas em nível de documento também tem sido objeto de críticas (Smith, 2017). Diante disso, surgiu a necessidade de avaliar a TA considerando um contexto mais amplo, possibilitando uma análise mais abrangente do contexto em questão. Entretanto, a metodologia para essa avaliação ainda está em sua fase inicial e poucos estudos foram realizados nesse sentido.\\nEm 2018, alegações de “paridade humana” (em inglês, human parity) na qualidade da TA (Hassan et al., 2018) foram rebatidas por Toral et al. (2018) e Läubli; Sennrich; Volk (2018), os quais apontaram que essa paridade não se replicava quando se considerava o contexto ou outros fatores, como a direção da tradução ou a experiência do anotador.\\nA Conferência de Tradução Automática (WMT), realizada desde 2006 e que até o ano de 2019 restringiu suas avaliações à análise de frases individuais, começou sua primeira tentativa de conduzir avaliações humanas em nível de documento no domínio de notícias no ano de 2019 (Barrault et al., 2019), em resposta às críticas apresentadas por Toral et al. (2018) e Läubli; Sennrich; Volk (2018). Adotando uma abordagem direta de avaliação (Graham et al., 2016), a conferência solicitou que avaliadores de multidão30 atribuíssem uma pontuação (de 0 a 100) a cada sentença. Os avaliadores foram instruídos a avaliar: (i) textos completos, (ii) segmentos individuais consecutivos na ordem original, e (iii) frases individuais selecionadas aleatoriamente. No ano subsequente, na edição WMT20, houve uma mudança de abordagem, expandindo o âmbito de avaliação para abranger artigos completos, demandando dos avaliadores a análise de segmentos específicos enquanto visualizavam o documento completo, bem como a avaliação da tradução do conteúdo (Barrault et al., 2020).\\nCastilho (2020) e Castilho (2021) testou as diferenças na concordância inter-anotadores (CIA) entre duas metodologias de avaliação: (i) uma centrada em sentenças individuais e (ii) outra com contexto, para o português brasiliero. No estudo de Castilho (2020), tradutores avaliaram a saída de TA considerando critérios de fluência, adequação (usando uma escala Likert), ranqueamento e anotação de erros. Essa avaliação foi conduzida em duas configurações distintas onde os tradutores atribuíram: (i) uma pontuação para cada sentença isolada, e (ii) uma pontuação para o documento como um todo. Os resultados demonstraram que os níveis de CIA para a metodologia em nível de documento atingiram níveis negativos, enquanto a satisfação dos tradutores com essa metodologia foi bastante reduzida. No entanto, esse enfoque evitou situações de avaliação incorreta (misevaluation) que são recorrentes quando se analisam sentenças isoladamente.\\nContinuando esse trabalho, Castilho (2021) modifica a configuração em nível de documento e repete o experimento com mais tradutores, onde ela compara a CIA na avaliação de (i) sentenças únicas aleatórias, (ii) avaliação de sentenças individuais em que os tradutores têm acesso à fonte completa e à saída de TA, e (iii) avaliação de documentos completos. Os resultados mostraram que uma metodologia em que os tradutores avaliam sentenças individuais no contexto de um documento gera um bom nível de CIA em comparação com a metodologia de sentença única aleatória, enquanto uma metodologia em que os tradutores atribuem uma pontuação por documento mostra um nível muito baixo de CIA. A autora afirma que atribuir uma nota por sentença no contexto evita casos de avaliação incorreta que são extremamente comuns nas configurações de avaliação de frases aleatórias. Além disso, a autora postula que o maior acordo de CIA na configuração de sentença única aleatória ocorre porque “os avaliadores tendem a aceitar a tradução quando a adequação é ambígua, mas a tradução está correta, especialmente se for fluente” (Castilho, 2021, p. 42), e afirma que o método de avaliação de sentença única aleatória deve ser evitado, pois o problema de avaliação incorreta é especialmente problemático ao avaliar a qualidade de sistemas NMT, uma vez que eles apresentam um nível aprimorado de fluência.\\nApós isso, em Castilho (2022) foi demonstrado que o contexto necessário para resolver questões de avaliação é influenciado pelo domínio, sem parecer estar intrinsecamente ligado ao comprimento das sentenças presentes no corpus envolvendo os idiomas inglês, português, irlandês, chinês e alemão. Em consequência disso, a pesquisa de Castilho et al. (2023) revelou que o impacto da extensão do contexto não parece influenciar significativamente os resultados, porém a estruturação da pontuação desempenha um papel crucial. Isso se deve ao fato de que sentenças conectadas tendem a gerar resultados mais diversos, com abordagens mais acuradas para resolver ambiguidades lexicais quando comparadas aos cenários de pontuação normais. Além disso, o estudo apontou que os sistemas GPT demonstraram proporcionar traduções mais precisas do que os sistemas de Tradução Automática.\\nDiante desse panorama, a avaliação de TA com contexto encontra-se em sua infância, com diversas questões em aberto. O futuro da avaliação de TA deve considerar se as métricas automatizadas e as avaliações humanas atuais conseguem capturar de forma realista a qualidade dos sistemas de nível de documento, e se é necessário modificar ou criar novas abordagens. Especial atenção deve ser dada aos modelos de linguagem como o GPT, conhecidos por gerar traduções fluentes e coesas, uma vez que a avaliação deve incorporar precisão da informação, fidelidade ao conteúdo original e coerência global, evitando a introdução de informações imprecisas ou divergentes.\\nAdemais, a avaliação de documentos traduzidos não deve se limitar a métricas automáticas, mas também usar a avaliação humana. Os avaliadores humanos desempenham um papel crucial em identificar nuances de qualidade que as métricas automáticas podem não capturar, como aspectos culturais, ambiguidades e sutilezas linguísticas. Portanto, a combinação de métricas automáticas com avaliações humanas se mostra uma abordagem fundamental para obter uma compreensão abrangente da qualidade da tradução de documentos.\\n\\n\\n\\n18.4 O Futuro da Tradução Automática\\nO futuro da TA parece muito promissor. Com a globalização e a internet, mais conteúdo é criado todos os dias, e, portanto, estão surgindo cada vez mais casos de uso nos quais a TA pode ser útil (Way, 2018). Segundo a “Slator 2019 Language Industry Market Report” (2019, p. 14), “a TA está bem encaminhada para se tornar a tecnologia mais importante para aprimorar a produtividade dos tradutores humanos”.\\nO aumento impressionante na qualidade com o surgimento da TA neural (NMT) em comparação com seu antecessor, o PSMT, foi exagerado pela mídia (Läubli; Sennrich; Volk, 2018; Toral et al., 2018), mas é incontestável que a NMT tenha sido, de fato, uma mudança de paradigma na área. No entanto, o entusiasmo em torno da TA diminuiu, com empresas de tradução de grande e médio porte relatando que, embora o uso da TA tenha aumentado, os benefícios percebidos têm se estabilizado (Sarah Hickey, 2020) em termos de grandes avanços na qualidade.\\nNo entanto, com o aumento da qualidade, é possível abordar uma variedade maior de tipos de documentos e públicos. Isso significa que há muito espaço para personalização de sistemas de TA projetados para casos de uso e contextos específicos, melhorando a precisão. Para a TAUS (2020, p. 16), a NMT será “aplicada de forma útil em ambientes de tradução de fala” e, na verdade, em todo discurso falado, já que lida melhor com conteúdo gerado pelo usuário. Além disso, “a NMT ajudará na expansão adicional de tecnologias de tradução de fala [...] disponíveis principalmente como sistemas monolíngues baseados em inglês, [...] transformando-os em sistemas multilíngues”, o que implicará “muitas mudanças profundas e caras”.\\nMais recentemente, no fim de 2022, os modelos de linguagem em larga escala (Capítulo\\xa015), como o GPT-3 da OpenAI31, têm desempenhado um papel importante no campo da tradução automática e prometem desempenhar um papel ainda maior no futuro. Esses modelos surgiram com o avanço das redes neurais e do aprendizado profundo. Desde sua introdução, os LLMs têm sido amplamente utilizados na TA, proporcionando melhorias significativas na qualidade e na fluidez das traduções geradas. Eles têm sido capazes de lidar com nuances linguísticas, contexto e ambiguidades, resultando em traduções mais precisas e naturais. Com o contínuo avanço da tecnologia, espera-se que os LLMs sejam capazes de melhorar a personalização das traduções, adaptando-se a estilos de escrita específicos e preferências individuais.\\nNo entanto, embora os LLMs tenham apresentado avanços significativos na área da TA, ainda existem desafios a serem superados. A qualidade da tradução depende de vários fatores, como a disponibilidade de dados de treinamento de alta qualidade e a compreensão do contexto e nuances linguísticas. Além disso, os LLMs podem ser sensíveis a preconceitos (bias) presentes nos dados de treinamento, resultando em traduções imprecisas ou enviesadas.\\nSegundo a pesquisa da CSA, “a pós-edição como serviço diminuirá ao longo do tempo, sendo substituída pela tradução automática adaptativa em software de tradução mais dinâmico” (p.\\xa023), e haverá uma demanda crescente por linguistas profissionais que possam interagir com a saída da tradução automática “Slator 2019 Language Industry Market Report” (2019, p. 22). Além disso, o relatório da Slator afirma que agora, com os altos níveis de qualidade e a ampla disponibilidade de ferramentas gratuitas de tradução automática, os clientes corporativos esperam mais do que uma tradução automática “apenas boa” e estão buscando “soluções personalizadas, adaptadas ao seu conteúdo, que possam ser adaptadas para seus fluxos de trabalho e preferências estilísticas específicas” “Slator 2019 Language Industry Market Report” (2019, p. 22).\\nVale ressaltar que todos os relatórios afirmam que a maioria da indústria “ainda não espera que a qualidade da tradução automática atinja os níveis da tradução humana em um futuro próximo” TAUS (2020, p. 16), e, portanto, tanto a tradução humana quanto a interação humana com a tradução automática ainda são altamente demandadas.\\nComo podemos ver, os sistemas de TA estão atingindo níveis de qualidade significativamente altos e, por isso, estão sendo cada vez mais utilizados em diversas áreas de negócio. Com a TA se tornando ubíqua em nosso dia a dia, a necessidade de testar a qualidade desses sistemas se tornou essencial (Castilho et al., 2019). Há muito espaço para a TA melhorar, e, portanto, uma boa prática na avaliação da TA é essencial para evitar afirmações exageradas e fornecer aos usuários um feedback honesto.\\n\\n\\nALMEIDA, G. DE. Translating the post-editor: an investigation of post-editing changes and correlations with professional experience across two Romance languages. 2013. Disponível em: <https://api.semanticscholar.org/CorpusID:60255248>\\n\\n\\nAZIZ, W.; SPECIA, L. Fully Automatic Compilation of a Portuguese-English Parallel Corpus for Statistical Machine Translation. STIL 2011. Anais...Cuiabá, MT: 2011.\\n\\n\\nBAHDANAU, D.; CHO, K.; BENGIO, Y. Neural Machine Translation by Jointly Learning to Align and Translate. (Y. Bengio, Y. LeCun, Eds.)3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings. Anais...San Diego, California.: 2015. Disponível em: <http://arxiv.org/abs/1409.0473>\\n\\n\\nBANERJEE, S.; LAVIE, A. METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments. (J. Goldstein et al., Eds.)Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization. Anais...Ann Arbor, Michigan: Association for Computational Linguistics, jun. 2005. Disponível em: <https://aclanthology.org/W05-0909>\\n\\n\\nBARRAULT, L. et al. Findings of the 2019 Conference on Machine Translation (WMT19). Proceedings of WMT. Anais...Florence, Italy: 2019.\\n\\n\\nBARRAULT, L. et al. Findings of the 2020 Conference on Machine Translation (WMT20). Proceedings of the Fifth Conference on Machine Translation. Anais...Online: Association for Computational Linguistics, nov. 2020. Disponível em: <https://www.aclweb.org/anthology/2020.wmt-1.1>\\n\\n\\nBOJAR, O. et al. Findings of the 2016 Conference on Machine Translation. Proceedings of the First Conference on Machine Translation. Anais...Berlin, Germany: Association for Computational Linguistics, 2016.\\n\\n\\nBROWN, P. et al. A statistical approach to language translation. Proceedings of the 12th conference on Computational linguistics -. Anais...Budapest, Hungry: Association for Computational Linguistics, 1988. Disponível em: <http://portal.acm.org/citation.cfm?doid=991635.991651>. Acesso em: 10 jun. 2020\\n\\n\\nCARL, M.; WAY, A. (EDS.). Recent Advances in Example-Based Machine Translation. [s.l.] Springer Netherlands, 2003.\\n\\n\\nCASELI, H. DE M.; INÁCIO, M. NMT and PBSMT Error Analyses in English to Brazilian Portuguese Automatic Translations. Proceedings of the Twelfth Language Resources and Evaluation Conference. Anais...Marseille, France: European Language Resources Association, 2020. Disponível em: <https://aclanthology.org/2020.lrec-1.446>\\n\\n\\nCASTANO, A.; CASACUBERTA, F. A connectionist approach to machine translation. 5th European Conference on Speech Communication and Technology (Eurospeech 1997). Anais...ISCA, set. 1997. Disponível em: <http://dx.doi.org/10.21437/eurospeech.1997-50>\\n\\n\\nCASTILHO, S. et al. Does post-editing increase usability? A study with Brazilian Portuguese as Target Language. Proceedings of the 17th annual conference of the European association for machine translation. Anais...2014.\\n\\n\\nCASTILHO, S. et al. A comparative quality evaluation of PBSMT and NMT using professional translators. Proceedings of Machine Translation Summit XVI: Research Track. Anais...a2017.\\n\\n\\nCASTILHO, S. et al. Is Neural Machine Translation the New State of the Art? The Prague Bulletin of Mathematical Linguistics, v. 108, n. 1, p. 109–120, jun. b2017.\\n\\n\\nCASTILHO, S. et al.'}]\n",
            "\n",
            "\n",
            "Resumo do trecho 4 foi bem-sucedido. Segue o resumo: A avaliação humana na avaliação de Tradução Automática (TA) levanta várias questões metodológicas e pragmáticas, como a necessidade de avaliar tanto a adequação quanto à fluência, o número e as competências dos avaliadores, a inclusão de tradutores, linguistas ou especialistas no domínio, a definição do número de pontos em uma escala de avaliação e a concordância entre avaliadores. Além disso, considerações pragmáticas, como o custo associado aos avaliadores e à geração de textos de referência traduzidos por humanos, também são importantes.\n",
            "\n",
            "A avaliação dependente de contexto na TA tem se mostrado necessária para avaliar sistemas neurais mais complexos e que levam em conta o contexto. As métricas automáticas e as métricas humanas são o estado da arte na avaliação, mas a avaliação com contexto é necessária para uma análise mais abrangente. A avaliação em nível de documento também tem sido explorada, com a Conferência de Tradução Automática (WMT) realizando avaliações humanas em nível de documento. Estudos têm mostrado que a avaliação de sentenças individuais no contexto de um documento gera um bom nível de concordância inter-anotadores.\n",
            "\n",
            "O futuro da TA é promissor, com a tecnologia se tornando cada vez mais importante para aprimorar a produtividade dos tradutores humanos. A qualidade da TA tem melhorado significativamente com o surgimento da TA neural, mas ainda há desafios a serem superados, como preconceitos nos dados de treinamento. Modelos de linguagem em larga escala, como o GPT-3, têm desempenhado um papel importante e prometem melhorar a personalização das traduções. A interação humana com a tradução automática continuará sendo importante, e a avaliação da qualidade dos sistemas de TA é essencial para evitar afirmações exageradas e fornecer feedback honesto.\n",
            "\n",
            "Em resumo, a avaliação humana na avaliação de TA levanta questões metodológicas e pragmáticas importantes. A avaliação dependente de contexto e em nível de documento tem se mostrado necessária para uma análise mais abrangente. O futuro da TA é promissor, com melhorias na qualidade e a possibilidade de personalização dos sistemas. A interação humana continua sendo crucial, e a avaliação da qualidade dos sistemas de TA é essencial. \n",
            "\n",
            "\n",
            "[{'role': 'system', 'content': 'Você é uma inteligência artificial que irá resumir pedaços de texto de um livro sobre Processamento de Linguagem Natural, portanto, você deve capturar explicações e pontos importantes para o entendimento e mantendo uma linearidade no resumo.'}, {'role': 'user', 'content': 'Approaches to Human and Machine Translation Quality Assessment. Em: Translation Quality Assessment: From Principles to Practice. Machine Translation: Technologies e Applications. [s.l.] Springer International Publishing, 2018. v. 1p. 9–38.\\n\\n\\nCASTILHO, S. et al. Editors’ foreword to the special issue on human factors in neural machine translation. Machine Translation, v. 33, n. 1–2, p. 1–7, maio 2019.\\n\\n\\nCASTILHO, S. On the Same Page? Comparing IAA in Sentence and Document Level Human MT Evaluation. Proceedings of the Fifth Conference on Machine Translation. Anais...Association for Computational Linguistics, nov. 2020. Disponível em: <https://www.aclweb.org/anthology/2020.wmt-1.137>\\n\\n\\nCASTILHO, S. Towards Document-Level Human MT Evaluation: On the Issues of Annotator Agreement, Effort and Misevaluation. Proceedings of the Workshop on Human Evaluation of NLP Systems. Anais...Association for Computational Linguistics, abr. 2021. Disponível em: <https://www.aclweb.org/anthology/2021.humeval-1.4>\\n\\n\\nCASTILHO, S. How Much Context Span is Enough? Examining Context-Related Issues for Document-level MT. Proceedings of the Language Resources and Evaluation Conference. Anais...Marseille, France: European Language Resources Association, 2022. Disponível em: <https://aclanthology.org/2022.lrec-1.323>\\n\\n\\nCASTILHO, S. et al. Translation Systems Care for Context? What About a GPT Model? Proceedings of the 24th Annual Conference of the European Association for Machine Translation. Anais...Tampere, Finland: EAMT, 2023. Disponível em: <https://events.tuni.fi/uploads/2023/06/11678752-proceedings-eamt2023.pdf>\\n\\n\\nCASTILHO, S.; RESENDE, N. Post-Editese in Literary Translations. Information, v. 13, n. 2, p. 66, 2022.\\n\\n\\nCASTILHO, S.; RESENDE, N.; MITKOV, R. What Influences the Features of Post-editese? A Preliminary Study. Proceedings of the Human-Informed Translation and Interpreting Technology Workshop (HiT-IT 2019). Anais...Varna, Bulgaria: Incoma Ltd., Shoumen, Bulgaria, set. 2019. Disponível em: <https://aclanthology.org/W19-8703>\\n\\n\\nCHALMERS, D. J. Syntactic transformations on distributed representations. Connectionist Natural Language Processing: Readings from Connection Science, p. 46–55, 1992.\\n\\n\\nCHRISMAN, L. Learning recursive distributed representations for holistic computation. Connection Science, v. 3, n. 4, p. 345–366, 1991.\\n\\n\\nCOSTA, A. et al. A linguistically motivated taxonomy for Machine Translation error analysis. Machine Translation, v. 29, n. 2, p. 127–161, 2015.\\n\\n\\nDE SOUSA, S. C.; AZIZ, W.; SPECIA, L. Assessing the post-editing effort for automatic and semi-automatic translations of DVD subtitles. Proceedings of the International Conference Recent Advances in Natural Language Processing 2011. Anais...2011.\\n\\n\\nDODDINGTON, G. Automatic Evaluation of Machine Translation Quality Using N-Gram Co-Occurrence Statistics. Proceedings of the Second International Conference on Human Language Technology Research. Anais...: HLT ’02.San Francisco, CA, USA: Morgan Kaufmann Publishers Inc., 2002.\\n\\n\\nDOHERTY, S. et al. Mapping the industry I: Findings on translation technologies and quality assessment. QTLaunchPad – Mapping the Industry I: Findings on Translation Technologies and Quality Assessment. Anais...GALA, 2013. Disponível em: <http://doras.dcu.ie/19474/1/Version_Participants_Final.pdf>. Acesso em: 11 nov. 2015\\n\\n\\nDOHERTY, S. et al. On Education and Training in Translation Quality Assessment. Em: MOORKENS, J. et al. (Eds.). Translation Quality Assessment: From Principles to Practice. Cham: Springer International Publishing, 2018. p. 95–106.\\n\\n\\nDORR, B. et al. Machine translation evaluation and optimization. Em: Handbook of Natural Language Processing and Machine Translation: DARPA Global Autonomous Language Exploitation. [s.l.] Springer, 2011. p. 745–843.\\n\\n\\nESTRELLA, P.; POPESCU-BELIS, A.; KING, M. The FEMTI guidelines for contextual MT evaluation: principles and resources. Em: WALTER DAELEMANS; VÉRONIQUE HOSTE (Eds.). Evaluation of translation Technology. Linguistica Antverpiensia new Series- themes em Translation Studies. [s.l: s.n.].\\n\\n\\nEuromatrix. Survey of Machine Translation Evaluation. [s.l.] Statistical; Hybrid Machine Translation Between All European Languages. Euromatrix, dez. 2007.\\n\\n\\nFEDERICO, M. et al. Assessing the Impact of Translation Errors on Machine Translation Quality with Mixed-effects Models. Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Anais...Doha, Qatar: Association for Computational Linguistics, out. 2014. Disponível em: <https://aclanthology.org/D14-1172>\\n\\n\\nFONT LLITJÓS, A.; CARBONELL, J. G.; LAVIE, A. A framework for interactive and automatic refinement of transfer-based machine translation. Proceedings of the 10th EAMT Conference: Practical applications of machine translation. Anais...Budapest, Hungary: European Association for Machine Translation, 2005. Disponível em: <https://aclanthology.org/2005.eamt-1.13>\\n\\n\\nFORCADA, M. L.; ÑECO, R. P. Recursive hetero-associative memories for translation. International Work-Conference on Artificial Neural Networks. Anais...Springer, 1997.\\n\\n\\nGRAHAM, Y. et al. Is all that Glitters in Machine Translation Quality Estimation really Gold? Proceedings of COLING 2016: Technical Papers. Anais...Osaka, Japan: The COLING 2016 Organizing Committee, dez. 2016. Disponível em: <https://www.aclweb.org/anthology/C16-1294>\\n\\n\\nHASSAN, H. et al. Achieving Human Parity on Automatic Chinese to English News Translation. arXiv preprint 1803.05567, 2018.\\n\\n\\nHOVY, E.; KING, M.; POPESCU-BELIS, A. An introduction to MT evaluation. Proceedings of Machine Translation Evaluation: Human Evaluators meet Automated Metrics. Workshop at the LREC 2002 Conference. Las Palmas, Spain. Anais...2002.\\n\\n\\nHUTCHINS, J. Towards a definition of example-based machine translation., Proceedings of Second Workshop on Example-Based Machine Translation; Anais...2005.\\n\\n\\nHUTCHINS, W. Machine Translation: A Concise History. Journal of Translation Studies: Special Issue on The Teaching of Computer-aided Translation, v. 13, p. 1–2, 2010.\\n\\n\\nHUTCHINS, W. J. Machine translation over fifty years. Histoire, Epistemologie, Langage, v. XXII, n. 1, p. 7–31, 2001.\\n\\n\\nINÁCIO, M. L.; CASELI, H. DE M. Word Embeddings at Post-Editing. (P. Quaresma et al., Eds.)Computational Processing of the Portuguese Language. Anais...Cham: Springer International Publishing, 2020.\\n\\n\\nJURAFSKY, D.; MARTIN, J. H. Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition. 3rd. ed. USA: Prentice Hall PTR, 2023.\\n\\n\\nKHAYRALLAH, H.; KOEHN, P. On the Impact of Various Types of Noise on Neural Machine Translation. Proceedings of the 2nd Workshop on Neural Machine Translation and Generation. Anais...Melbourne, Australia: Association for Computational Linguistics, jul. 2018. Disponível em: <https://aclanthology.org/W18-2709>\\n\\n\\nKOEHN, P. et al. Moses: Open Source Toolkit for Statistical Machine Translation. Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions. Anais...Prague, Czech Republic: Association for Computational Linguistics, jun. 2007. Disponível em: <https://aclanthology.org/P07-2045>\\n\\n\\nKOEHN, P. Statistical Machine Translation. [s.l.] Cambridge University Press, 2009.\\n\\n\\nKOEHN, P. Neural Machine Translation. [s.l.] Cambridge University Press, 2020.\\n\\n\\nKOEHN, P.; OCH, F. J.; MARCU, D. Statistical phrase-based translation. Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - NAACL ’03. Anais...Association for Computational Linguistics, 2003. Disponível em: <http://dx.doi.org/10.3115/1073445.1073462>\\n\\n\\nKRINGS, H. P. Repairing Texts: Empirical Investigations of Machine Translation Post-editing Processes. [s.l.] Kent State University Press, 2001.\\n\\n\\nLÄUBLI, S. et al. A set of recommendations for assessing human–machine parity in language translation. Journal of Artificial Intelligence Research, v. 67, p. 653–672, 2020.\\n\\n\\nLÄUBLI, S.; SENNRICH, R.; VOLK, M. Has Machine Translation Achieved Human Parity? A Case for Document-level Evaluation. Proceedings of EMNLP. Anais...Brussels, Belgium: 2018.\\n\\n\\nLEE, S. et al. A Survey on Evaluation Metrics for Machine Translation. Mathematics, v. 11, n. 4, 2023.\\n\\n\\nLIKERT, R. A Technique for the Measurement of Attitudes. [s.l.] Archives of Psychology, 1932.\\n\\n\\nLIN, C.-Y. ROUGE: A Package for Automatic Evaluation of Summaries. Text Summarization Branches Out. Anais...Barcelona, Spain: Association for Computational Linguistics, jul. 2004. Disponível em: <https://aclanthology.org/W04-1013>\\n\\n\\nLO, C. YiSi - a Unified Semantic MT Quality Evaluation and Estimation Metric for Languages with Different Levels of Available Resources. Proceedings of the Fourth Conference on Machine Translation, WMT 2019, Florence, Italy, August 1-2, 2019 - Volume 2: Shared Task Papers, Day 1. Anais...2019. Disponível em: <https://doi.org/10.18653/v1/w19-5358>\\n\\n\\nLO, C.; WU, D. MEANT: An inexpensive, high-accuracy, semi-automatic metric for evaluating translation utility based on semantic roles. The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference, 19-24 June, 2011, Portland, Oregon, USA. Anais...2011. Disponível em: <https://aclanthology.org/P11-1023/>\\n\\n\\nLOMMEL, A.; MELBY, A. Tutorial: MQM-DQF: A Good Marriage (Translation Quality for the 21st Century). Proceedings of the 13th Conference of the Association for Machine Translation in the Americas (Volume 2: User Track). Anais...Boston, MA: Association for Machine Translation in the Americas, mar. 2018. Disponível em: <https://aclanthology.org/W18-1925>\\n\\n\\nMA, Q. et al. Blend: a Novel Combined MT Metric Based on Direct Assessment - CASICT-DCU submission to WMT17 Metrics Task. Proceedings of the Second Conference on Machine Translation, WMT 2017, Copenhagen, Denmark, September 7-8, 2017. Anais...2017. Disponível em: <https://doi.org/10.18653/v1/w17-4768>\\n\\n\\nMARIE, B.; FUJITA, A.; RUBINO, R. Scientific Credibility of Machine Translation Research: A Meta-Evaluation of 769 Papers. arXiv:2106.15195 [cs], jun. 2021.\\n\\n\\nMARTINS, D. B. DE J. Pós-edição automática de textos traduzidos automaticamente de inglês para português do Brasil. Mestrado—São Carlos: Universidade Federal de São Carlos, 2014.\\n\\n\\nMARTINS, D. B. DE J.; CASELI, H. DE M. Automatic machine translation error identification. Machine Translation, v. 29, n. 1, p. 1–24, 2015.\\n\\n\\nMARTINS, R. T. et al. An interlingua aiming at communication on the Web: How language-independent can it be? NAACL-ANLP 2000 Workshop: Applied Interlinguas: Practical Applications of Interlingual Approaches to NLP. Anais...2000. Disponível em: <https://aclanthology.org/W00-0204>\\n\\n\\nMOORKENS, J. et al. Correlations of perceived post-editing effort with measurements of actual effort. Machine Translation, v. 29, n. 3/4, p. 267–284, 2015.\\n\\n\\nMOORKENS, J. Under pressure: translation in times of austerity. Perspectives, v. 25, n. 3, p. 464–477, fev. 2017.\\n\\n\\nNAGAO, M. A Framework of a Mechanical Translation between Japanese and English by Analogy Principle. Em: NIRENBURG, S.; SOMERS, H. L.; WILKS, Y. A. (Eds.). Readings in Machine Translation. [s.l.] The MIT Press, 1984.\\n\\n\\nNECO, R. P.; FORCADA, M. L. Asynchronous translations with recurrent neural nets. Proceedings of International Conference on Neural Networks (ICNN’97). Anais...1997.\\n\\n\\nNUNES, M. DAS G. V. et al. O uso de interlı́ngua para comunicação via Internet: a decodificação UNL-português. Revista Tecnologia da Informação, v. 3, n. 1, p. 49–55, 2003.\\n\\n\\nO’BRIEN, S. Towards predicting post-editing productivity. Machine translation, v. 25, p. 197–215, 2011.\\n\\n\\nO’BRIEN, S. et al. Dynamic Quality Evaluation Framework. [s.l.] TAUS Labs Report. The Translation Automation User Society-TAUS, 2011.\\n\\n\\nOCH, F. J.; NEY, H. The Alignment Template Approach to Statistical Machine Translation. Computational Linguistics, v. 30, n. 4, p. 417–449, dez. 2004.\\n\\n\\nPAPINENI, K. et al. BLEU: A Method for Automatic Evaluation of Machine Translation. Proceedings of the 40th Annual Meeting on Association for Computational Linguistics. Anais...: ACL ’02.USA: Association for Computational Linguistics, 2002. Disponível em: <https://doi.org/10.3115/1073083.1073135>\\n\\n\\nPAROUBEK, P.; CHAUDIRON, S.; HIRSCHMAN, L. Principles of Evaluation in Natural Language Processing. Traitement Automatique des Langues, Volume 48, Numéro 1 : Principes de l’évaluation en Traitement Automatique des Langues [Principles of Evaluation in Natural Language Processing]. Anais...France: ATALA (Association pour le Traitement Automatique des Langues), 2007. Disponível em: <https://aclanthology.org/2007.tal-1.1>\\n\\n\\nPOPOVIC, M.; BURCHARDT, A. From Human to Automatic Error Classification for Machine Translation Output. Proceedings of the 15th Conference of the European Association for Machine Translation. Anais...Leuven, Belgium: 2011. Disponível em: <https://aclanthology.org/2011.eamt-1.36.pdf>\\n\\n\\nPOPOVIĆ, M. chrF: character n-gram F-score for automatic MT evaluation. Proceedings of the Tenth Workshop on Statistical Machine Translation. Anais...Lisbon, Portugal: Association for Computational Linguistics, set. 2015. Disponível em: <https://aclanthology.org/W15-3049>\\n\\n\\nREI, R. et al. COMET: A Neural Framework for MT Evaluation. Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). Anais...Online: Association for Computational Linguistics, nov. 2020. Disponível em: <https://aclanthology.org/2020.emnlp-main.213>\\n\\n\\nSARAH HICKEY. Nimdzi 100 - Language Services Industry Market Report 2020.pdf. [s.l: s.n.].\\n\\n\\nSELLAM, T.; DAS, D.; PARIKH, A. P. BLEURT: Learning Robust Metrics for Text Generation. Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020. Anais...2020. Disponível em: <https://doi.org/10.18653/v1/2020.acl-main.704>\\n\\n\\nSENNRICH, R.; HADDOW, B.; BIRCH, A. Improving Neural Machine Translation Models with Monolingual Data. Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016). Anais...2016. Disponível em: <https://arxiv.org/abs/1511.06709>\\n\\n\\nSHIMANAKA, H.; KAJIWARA, T.; KOMACHI, M. Machine Translation Evaluation with BERT Regressor. arXiv, v. abs/1907.12679, 2019.\\n\\n\\nSHTERIONOV, D. et al. Human versus Automatic Quality Evaluation of NMT and PBSMT. Machine Translation, v. 32, n. 3, p. 217–235, 2018.\\n\\n\\nSILVA, I. A. L. DA et al. Translation, post-editing and directionality. Translation in transition: Between cognition, computing and technology, p. 107–134, 2017.\\n\\n\\nSlator 2019 Language Industry Market Report. p. 33, 2019.\\n\\n\\nSMITH, K. S. On Integrating Discourse in Machine Translation. Proceedings of the Third Workshop on Discourse in Machine Translation. Anais...2017.\\n\\n\\nSNOVER, M. G. et al. A Study of Translation Edit Rate with Targeted Human Annotation. Proceedings of the 7th Conference of the Association for Machine Translation in the Americas: Technical Papers, AMTA 2006, Cambridge, Massachusetts, USA, August 8-12, 2006. Anais...2006. Disponível em: <https://aclanthology.org/2006.amta-papers.25/>\\n\\n\\nSTANOJEVIC, M.; SIMA’AN, K. BEER: BEtter Evaluation as Ranking. Proceedings of the Ninth Workshop on Statistical Machine Translation, WMT@ACL 2014, June 26-27, 2014, Baltimore, Maryland, USA. Anais...2014. Disponível em: <https://doi.org/10.3115/v1/w14-3354>\\n\\n\\nSU, K.-Y.; WU, M.-W.; CHANG, J.-S. A new quantitative quality measure for machine translation systems. Proceedings of the 14th conference on Computational linguistics -. Anais...Association for Computational Linguistics, 1992. Disponível em: <http://dx.doi.org/10.3115/992133.992137>\\n\\n\\nTAUS. TAUS - The Translation Industry in 2022 Report., 2020. Disponível em: <https://info.taus.net/translation-industry-2022-report-download>. Acesso em: 19 ago. 2020\\n\\n\\nTORAL, A. et al. Attaining the Unattainable? Reassessing Claims of Human Parity in Neural Machine Translation. Proceedings of WMT. Anais...Brussels, Belgium: 2018.\\n\\n\\nUCHIDA, H.; ZHU, M.; DELLA SENTA, T. A gift for a millennium. IAS/UNU, Tokyo, 1999.\\n\\n\\nUSZKOREIT, H.; LOMMEL, A.'}]\n",
            "\n",
            "\n",
            "Resumo do trecho 5 foi bem-sucedido. Segue o resumo: O livro \"Processamento de Linguagem Natural\" aborda diversos tópicos relacionados à avaliação da qualidade da tradução humana e automática. Alguns dos principais pontos discutidos incluem:\n",
            "\n",
            "- A importância da avaliação da qualidade da tradução humana e automática, tanto do ponto de vista teórico quanto prático.\n",
            "- Diferentes abordagens e métodos para avaliar a qualidade da tradução, incluindo avaliação humana, métricas automáticas e avaliação baseada em contexto.\n",
            "- A utilização de técnicas de processamento de linguagem natural, como modelos de linguagem neural, para melhorar a qualidade da tradução automática.\n",
            "- A influência do contexto na avaliação da qualidade da tradução, tanto em nível de sentença quanto em nível de documento.\n",
            "- O uso de recursos linguísticos, como embeddings de palavras, na pós-edição automática de traduções.\n",
            "- A importância da concordância entre os avaliadores humanos na avaliação da qualidade da tradução.\n",
            "- A análise de erros de tradução e a identificação de características específicas da pós-edição de traduções automáticas.\n",
            "- A comparação entre diferentes métricas de avaliação da qualidade da tradução, como BLEU, chrF e ROUGE.\n",
            "- O desenvolvimento de modelos de avaliação da qualidade da tradução baseados em aprendizado de máquina, como o COMET e o BLEURT.\n",
            "- A busca pela paridade humana na tradução automática e os desafios associados a essa meta.\n",
            "- A evolução da tradução automática ao longo dos anos e os avanços na área, como a utilização de redes neurais e o desenvolvimento do sistema Moses.\n",
            "\n",
            "Esses são apenas alguns dos tópicos abordados no livro, que oferece uma visão abrangente e atualizada do campo do processamento de linguagem natural e da avaliação da qualidade da tradução. \n",
            "\n",
            "\n",
            "[{'role': 'system', 'content': 'Você é uma inteligência artificial que irá resumir pedaços de texto de um livro sobre Processamento de Linguagem Natural, portanto, você deve capturar explicações e pontos importantes para o entendimento e mantendo uma linearidade no resumo.'}, {'role': 'user', 'content': 'Multidimensional Quality Metrics: A New Unified Paradigm for Human and Machine Translation Quality Assessment. [s.l: s.n.].\\n\\n\\nVASWANI, A. et al. Attention is All you Need. (I. Guyon et al., Eds.)Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA. Anais...2017. Disponível em: <https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html>\\n\\n\\nVILAR, D. et al. Error Analysis of Statistical Machine Translation Output. Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC’06). Anais...Genoa, Italy: European Language Resources Association (ELRA), 2006. Disponível em: <http://www.lrec-conf.org/proceedings/lrec2006/pdf/413_pdf.pdf>\\n\\n\\nWAY, A. Quality Expectations of Machine Translation. Em: MOORKENS, J. et al. (Eds.). Translation Quality Assessment: From Principles to Practice. Cham: Springer International Publishing, 2018. p. 159–178.\\n\\n\\nWAY, A.; FORCADA, M. L. Editors’ foreword to the invited issue on SMT and NMT. Machine Translation, v. 32, n. 3, p. 191–194, set. 2018.\\n\\n\\nWU, Y. et al. Google’s neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016.\\n\\n\\nYUAN, W.; NEUBIG, G.; LIU, P. BARTScore: Evaluating Generated Text as Text Generation. Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual. Anais...2021. Disponível em: <https://proceedings.neurips.cc/paper/2021/hash/e4d2b6e6fdeca3e60e0f1a62fee3d9dd-Abstract.html>\\n\\n\\nZHANG, T. et al. BERTScore: Evaluating Text Generation with BERT. 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. Anais...OpenReview.net, 2020. Disponível em: <https://openreview.net/forum?id=SkeHuCVFDr>\\n\\n\\n\\n\\n\\n\\nPara uma descrição abrangente da história da TA, sugere-se consultar alguns livros e capítulos sobre o assunto: (Hutchins, 2001), (Koehn, 2009), (Hutchins, 2010) e (Koehn, 2020).↩︎\\nTodas as aplicações citadas estão disponíveis no Google tradutor. Disponível em: https://translate.google.com.br/.↩︎\\nVejam que aqui estamos usando “palavra” para denotar uma unidade lexical bastante comum no PLN. Contudo, vale ressaltar que outras unidades lexicais (n-grama ou expressão multipalavra, por exemplo) também podem ser usadas na tradução direta. Para saber mais sobre as unidades de processamento, Capítulo\\xa04.↩︎\\nhttps://dl.fbaipublicfiles.com/arrival/dictionaries/pt-en.txt↩︎\\nIsso porque palavras como “a” e “do” não têm entrada no léxico consultado, provavelmente por serem stop words. Para as demais palavras, a tradução foi gerada considerando apenas a primeira ocorrência de equivalente em inglês encontrada. Por exemplo, para “casa” existem as opções “house” e “home”, nesta ordem, e como “house” aparece primeiro, ela foi a escolhida para gerar a saída neste exemplo.↩︎\\nA tradução baseada em regras enquadra-se na estratégia de tradução por transferência, na qual o mapeamento é realizado com base em uma análise da língua-fonte, seguida da aplicação de regras que fazem o mapeamento e a geração de equivalentes na língua-alvo.↩︎\\nPara calcular a quantidade total de tradutores necessários, usamos a fórmula de arranjo de n línguas para combinação em pares (k = 2), An,2 que é (n!)/(n-2)!. Assim, para 5 línguas teríamos que construir 20 tradutores no sentido direita-esquerda e mais 20 tradutores no sentido esquerda-direita, totalizando 40 tradutores distintos!↩︎\\nhttp://www.nilc.icmc.usp.br/↩︎\\nEsse projeto deu origem à UNDL Foundation http://www.undl.org/.↩︎\\nMais informações sobre a linguagem UNL podem ser obtidas em (Uchida; Zhu; Della Senta, 1999). Detalhes sobre o Projeto UNL-Brazil podem ser encontrados em (Martins et al., 2000; Nunes et al., 2003) e http://www.nilc.icmc.usp.br/nilc/projects/unl.htm.↩︎\\nPara uma descrição mais abrangente e para a história dos sistemas de TA baseados em exemplos, consulte (Carl; Way, 2003).↩︎\\nNo contexto da TA estatística, o alinhamento é uma tarefa de encontrar as correspondências entre texto-fonte e texto-alvo. Esse alinhamento pode se dar em nível de palavras (alinhamento lexical), de sentenças (alinhamento sentencial) entre outros.↩︎\\nhttps://www.statmt.org/moses/ e https://github.com/moses-smt/mosesdecoder↩︎\\nObtida por meio da combinação das frases 2, 7, 10 e 12.↩︎\\nObtida por meio da combinação das frases 3, 6, 9, 10 e 13.↩︎\\nObtida por meio da combinação das frases 4, 5, 9, 11 e 12.↩︎\\nhttp://www.nilc.icmc.usp.br/nilc/tools/Fapesp%20Corpora.htm↩︎\\nSegundo informações disponíveis em: https://ai.googleblog.com/2016/09/a-neural-network-for-machine.html.↩︎\\nArtigos como (Chalmers, 1992), (Chrisman, 1991), (Castano; Casacuberta, 1997), (Forcada; Ñeco, 1997), (Neco; Forcada, 1997).↩︎\\nGeralmente são necessárias placas GPU (Graphics Processing Units), originalmente projetadas para processamento gráfico, capazes de fazer diversos cálculos em segundos.↩︎\\nNo contexto das redes neurais artificiais, um ciclo de processamento é chamado de época.↩︎\\nTradução gerada pelo https://translate.google.com.br/ em 23 de agosto de 2023.↩︎\\nhttp://www.nilc.icmc.usp.br/embeddings↩︎\\nhttps://github.com/facebookresearch/MUSE↩︎\\nO modelo foi treinado usando o Google Colab e o código disponível em: https://github.com/brasileiras-pln/minicurso-PLN-SBBD/blob/main/Colabs/5.Aplicacoes_traducao_com_t5.ipynb.↩︎\\nhttps://ai.googleblog.com/2020/06/recent-advances-in-google-translate.html↩︎\\nPara uma descrição abrangente do estado da arte da tradução automática neural, veja “Neural Machine Translation” de Koehn (2020).↩︎\\nA escala Likert é um método de medição criado por Likert (1932) que apresenta ao respondente uma afirmação ou pergunta e solicita que o respondente avalie o grau em que concorda com ela. A escala envolve uma série de itens ou afirmações aos quais os respondentes atribuem níveis de concordância ou discordância.↩︎\\nAs severidades geralmente são classificadas como “Crítico” (critical), “Grave” (major), e “Mínimo” (minor).↩︎\\nAvaliadores contratados via plataformas como Mecanical Turk (https://www.mturk.com/).↩︎\\nhttps://openai.com/↩︎\\n\\n\\n.'}]\n",
            "\n",
            "\n",
            "Resumo do trecho 6 foi bem-sucedido. Segue o resumo: O livro aborda o tema do Processamento de Linguagem Natural (PLN) e apresenta diversos conceitos e aplicações relacionados à área. O capítulo menciona a importância da Tradução Automática (TA) e destaca a necessidade de avaliar a qualidade da tradução, tanto feita por humanos quanto por máquinas.\n",
            "\n",
            "Uma das métricas de qualidade mencionadas é o BARTScore, que avalia a qualidade do texto gerado como uma tarefa de geração de texto. Outra métrica mencionada é o BERTScore, que avalia a qualidade do texto gerado usando o modelo BERT.\n",
            "\n",
            "O capítulo também discute o uso de modelos de tradução neural, como o sistema de tradução neural do Google, que utiliza uma abordagem baseada em redes neurais. Além disso, são mencionadas outras abordagens, como a tradução baseada em regras e a tradução estatística.\n",
            "\n",
            "No contexto da TA estatística, é mencionada a tarefa de alinhamento, que consiste em encontrar as correspondências entre o texto-fonte e o texto-alvo. Também são mencionadas ferramentas como o Moses, que é um sistema de tradução estatística amplamente utilizado.\n",
            "\n",
            "O capítulo destaca a importância de avaliar a qualidade da tradução automática e apresenta diferentes abordagens para realizar essa avaliação. São mencionados métodos de avaliação automática, como a comparação com traduções de referência e o uso de métricas de qualidade.\n",
            "\n",
            "Além disso, são mencionados métodos de avaliação humana, como a escala Likert, em que os avaliadores atribuem níveis de concordância ou discordância a afirmações sobre a qualidade da tradução. Também são mencionados métodos de avaliação por pares, em que os avaliadores comparam diferentes traduções e atribuem uma pontuação de qualidade.\n",
            "\n",
            "O capítulo também menciona a importância de contar com um grande número de avaliadores para obter resultados mais confiáveis. São mencionadas plataformas como o Mechanical Turk, que permitem contratar avaliadores para realizar tarefas de avaliação.\n",
            "\n",
            "Por fim, o capítulo destaca a importância de avanços recentes na área de tradução automática, como o uso de modelos de linguagem pré-treinados e o uso de técnicas de aprendizado de máquina avançadas. São mencionados projetos como o MUSE, que visa melhorar a qualidade da tradução automática usando técnicas de aprendizado de máquina.\n",
            "\n",
            "Em resumo, o capítulo aborda diferentes aspectos relacionados à qualidade da tradução automática, apresentando métricas de qualidade, técnicas de avaliação e avanços recentes na área. \n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Função para visualizar os resumos do Capítulo 8.\n",
        "visualizar_resumo(resumo_Capitulo_8)"
      ],
      "metadata": {
        "id": "YHcwzLHOj3v-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2cf39a8-8fcd-4fda-c0c9-f59536058821"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resumo do trecho 0 :\n",
            "A semântica estuda o significado das palavras e frases na linguagem. No entanto, a definição do significado de uma palavra é um problema complexo e depende da perspectiva teórica adotada. Existem duas\n",
            "principais abordagens no estudo do significado: a representacionista/essencialista e a pragmática. A abordagem representacionista vê as palavras como substitutos de entidades extralinguísticas e busca\n",
            "uma essência comum nos diferentes usos da palavra. Já a abordagem pragmática entende que o significado é variável e depende do contexto e da situação. No processamento de linguagem natural (PLN),\n",
            "essas diferentes perspectivas se refletem em técnicas simbólicas e representações distribuídas. Além disso, existem os datasets com anotação semântica, que buscam criar uma representação estável entre\n",
            "palavras e seus significados em contextos específicos. Essas abordagens têm impacto no desenvolvimento de recursos como wordnets e na anotação de significados em corpora.\n",
            "\n",
            "\n",
            "Resumo do trecho 1 :\n",
            "O termo \"trabalho\" possui diversas acepções de acordo com o dicionário. Essas acepções incluem o emprego da força física ou intelectual para realizar algo, a ocupação profissional, o esmero na\n",
            "confecção de uma obra, o grande esforço, a ação contínua de uma força da natureza, o fenômeno orgânico que ocorre nos tecidos, o resultado do funcionamento de uma máquina, a obrigação ou\n",
            "responsabilidade, o conjunto de atividades humanas empregadas na produção de bens e a tarefa a ser realizada. A anotação semântica dessas diferentes acepções é um desafio, pois é difícil isolar o\n",
            "significado das palavras enquanto estão sendo usadas. Estudos têm mostrado a dificuldade de se obter concordância entre anotadores quanto à escolha do significado utilizado. O uso de representações\n",
            "distribuídas tem sido uma abordagem promissora para lidar com o sentido das palavras, mas ainda há limitações em cada uma das abordagens existentes. O próximo capítulo irá aprofundar essas diferentes\n",
            "maneiras de trabalhar com o significado no Processamento de Linguagem Natural.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Função para visualizar os resumos do Capítulo 18.\n",
        "visualizar_resumo(resumo_Capitulo_18)"
      ],
      "metadata": {
        "id": "G2wCYzICmu_-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f97cc56-dae6-4a1f-fc40-eb873c69ef77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resumo do trecho 0 :\n",
            "A tradução automática (TA) é a tradução de um texto eletrônico de uma língua para outra sem intervenção humana. A TA evoluiu significativamente nos últimos anos com o avanço de modelos estatísticos e\n",
            "neurais. Atualmente, é amplamente utilizada em diversas aplicações de comunicação. Existem diferentes abordagens para a TA, incluindo a tradução direta, baseada em regras, por interlíngua, baseada em\n",
            "exemplos e estatística. A tradução direta mapeia palavras-fonte para palavras-alvo sem análise adicional, mas apresenta limitações. A tradução baseada em regras utiliza regras linguísticas para\n",
            "realizar mapeamentos mais complexos, considerando a sintaxe das línguas envolvidas. A tradução por interlíngua utiliza uma língua intermediária para representar informações de outras línguas. A\n",
            "tradução baseada em exemplos utiliza exemplos de traduções existentes para aprender a traduzir trechos de texto. A tradução estatística utiliza modelos estatísticos para extrair pares de tradução de\n",
            "corpora bilíngues.\n",
            "\n",
            "\n",
            "Resumo do trecho 1 :\n",
            "A tradução automática estatística baseada em frases (PBSMT) alinha fragmentos de frases e palavras no texto-fonte a frases no texto-alvo, utilizando modelos de tradução e de língua para determinar a\n",
            "melhor tradução. Esses modelos são treinados a partir de grandes quantidades de textos paralelos, e a probabilidade de uma frase ser traduzida é calculada com base na co-ocorrência das frases no\n",
            "corpus paralelo. A PBSMT foi amplamente utilizada até a introdução da tradução automática neural (NMT) em 2015. A NMT utiliza redes neurais artificiais para traduzir sentenças, considerando toda a\n",
            "sentença-fonte de uma vez e usando embeddings e modelos de atenção para melhorar a qualidade da tradução. No entanto, a NMT também possui limitações, como a dificuldade de interpretação dos modelos e\n",
            "o desempenho ruim em condições fora do domínio e para idiomas com recursos limitados. A avaliação da tradução automática é um campo em crescimento, mas ainda não há consenso sobre como medir a\n",
            "qualidade da tradução.\n",
            "\n",
            "\n",
            "Resumo do trecho 2 :\n",
            "A avaliação da Tradução Automática (TA) é a prática de analisar a qualidade da tradução gerada por um sistema de TA, utilizando critérios estabelecidos. Existem abordagens automáticas e manuais para a\n",
            "avaliação, e muitas vezes uma combinação das duas é utilizada. A avaliação desempenha um papel essencial na melhoria dos sistemas de TA, fornecendo informações sobre seu desempenho e áreas que\n",
            "precisam de melhorias.  A avaliação da TA pode ser realizada de várias maneiras diferentes, devido à falta de consenso sobre o que constitui uma \"boa\" tradução. Tradicionalmente, a avaliação da TA foi\n",
            "dividida em avaliação \"caixa de vidro\" e \"caixa-preta\". A avaliação \"caixa de vidro\" se concentra nas propriedades internas do sistema de TA, enquanto a avaliação \"caixa-preta\" avalia apenas a saída\n",
            "do sistema, sem considerar seus mecanismos internos.  A avaliação da TA tem uma longa história, com várias avaliações influentes ao longo dos anos. O relatório ALPAC (1966) examinou a qualidade dos\n",
            "sistemas de TA nos Estados Unidos e resultou em uma redução significativa no financiamento para o processamento de linguagem natural e TA na época. A iniciativa DARPA (1992) foi uma das primeiras\n",
            "iniciativas na avaliação de TA e utilizou julgamentos humanos para avaliar as traduções automáticas. Outras avaliações importantes incluem o projeto GALE (2005), EuroMatrix (2006), WMT (2006), TAUS\n",
            "(2005) e QTLaunchPad (2012).  A avaliação de TA é importante para todos os campos relacionados à tradução, mas é um problema complexo devido à falta de consenso sobre o que constitui uma tradução de\n",
            "qualidade. Além disso, a avaliação automática de TA é realizada por meio de métricas automáticas, como BLEU, METEOR e TER, que comparam as traduções geradas com traduções de referência. Essas métricas\n",
            "têm sido aprimoradas ao longo dos anos, mas ainda apresentam limitações.  É importante que a avaliação de TA seja replicável e baseada em evidências. No entanto, muitas vezes a avaliação humana é\n",
            "considerada desnecessária e alguns estudos utilizam apenas uma métrica para avaliar seus sistemas. Com os avanços na qualidade dos sistemas de TA, é necessário aprimorar continuamente a metodologia de\n",
            "avaliação e evitar promessas exageradas sobre a capacidade da TA.  Em resumo, a avaliação da TA é uma prática essencial para melhorar os sistemas de tradução automática. Existem diferentes abordagens\n",
            "e métricas para avaliar a qualidade da tradução, mas é importante considerar o contexto e definir o que constitui uma tradução de qualidade em cada cenário específico.\n",
            "\n",
            "\n",
            "Resumo do trecho 3 :\n",
            "As métricas de avaliação automática (MAAs) são usadas para medir a qualidade da tradução automática (TA) de forma rápida e eficiente. Existem duas categorias de métricas: supervisionadas e não\n",
            "supervisionadas, que podem ser baseadas em word-embeddings ou contextual-embeddings. As MAAs são úteis para comparação entre sistemas de TA, mas têm limitações na predição da qualidade da tradução\n",
            "final.  A avaliação humana é essencial para uma compreensão mais detalhada e abrangente dos sistemas de TA. Ela pode ser feita através de diferentes paradigmas, como fluência-adequação, pós-edição,\n",
            "ranqueamento e anotação de erros. Além disso, métricas secundárias, como legibilidade, compreensibilidade e usabilidade, podem ser consideradas.  A adequação é uma métrica que avalia a fidelidade\n",
            "semântica entre o texto-fonte e a tradução. A fluência, por sua vez, avalia a naturalidade e a estrutura do texto-alvo. O ranqueamento permite estabelecer uma hierarquia de qualidade entre diferentes\n",
            "traduções, enquanto a anotação de erros identifica e classifica os erros presentes nas traduções.  A pós-edição é uma ferramenta importante na avaliação de sistemas de TA, pois oferece uma visão mais\n",
            "aprofundada do esforço envolvido no processo de tradução. O esforço pode ser medido em termos de tempo, operações de edição e esforço cognitivo. A pós-edição também pode ser usada para gerar novos\n",
            "dados de treinamento ou teste.  É importante considerar que a avaliação humana pode variar dependendo dos avaliadores e das línguas envolvidas. Além disso, a anotação de erros é um processo caro e\n",
            "demorado, e a escolha da abordagem de avaliação mais adequada é um ponto de reflexão.\n",
            "\n",
            "\n",
            "Resumo do trecho 4 :\n",
            "A avaliação humana na avaliação de Tradução Automática (TA) levanta várias questões metodológicas e pragmáticas, como a necessidade de avaliar tanto a adequação quanto à fluência, o número e as\n",
            "competências dos avaliadores, a inclusão de tradutores, linguistas ou especialistas no domínio, a definição do número de pontos em uma escala de avaliação e a concordância entre avaliadores. Além\n",
            "disso, considerações pragmáticas, como o custo associado aos avaliadores e à geração de textos de referência traduzidos por humanos, também são importantes.  A avaliação dependente de contexto na TA\n",
            "tem se mostrado necessária para avaliar sistemas neurais mais complexos e que levam em conta o contexto. As métricas automáticas e as métricas humanas são o estado da arte na avaliação, mas a\n",
            "avaliação com contexto é necessária para uma análise mais abrangente. A avaliação em nível de documento também tem sido explorada, com a Conferência de Tradução Automática (WMT) realizando avaliações\n",
            "humanas em nível de documento. Estudos têm mostrado que a avaliação de sentenças individuais no contexto de um documento gera um bom nível de concordância inter-anotadores.  O futuro da TA é\n",
            "promissor, com a tecnologia se tornando cada vez mais importante para aprimorar a produtividade dos tradutores humanos. A qualidade da TA tem melhorado significativamente com o surgimento da TA\n",
            "neural, mas ainda há desafios a serem superados, como preconceitos nos dados de treinamento. Modelos de linguagem em larga escala, como o GPT-3, têm desempenhado um papel importante e prometem\n",
            "melhorar a personalização das traduções. A interação humana com a tradução automática continuará sendo importante, e a avaliação da qualidade dos sistemas de TA é essencial para evitar afirmações\n",
            "exageradas e fornecer feedback honesto.  Em resumo, a avaliação humana na avaliação de TA levanta questões metodológicas e pragmáticas importantes. A avaliação dependente de contexto e em nível de\n",
            "documento tem se mostrado necessária para uma análise mais abrangente. O futuro da TA é promissor, com melhorias na qualidade e a possibilidade de personalização dos sistemas. A interação humana\n",
            "continua sendo crucial, e a avaliação da qualidade dos sistemas de TA é essencial.\n",
            "\n",
            "\n",
            "Resumo do trecho 5 :\n",
            "O livro \"Processamento de Linguagem Natural\" aborda diversos tópicos relacionados à avaliação da qualidade da tradução humana e automática. Alguns dos principais pontos discutidos incluem:  - A\n",
            "importância da avaliação da qualidade da tradução humana e automática, tanto do ponto de vista teórico quanto prático. - Diferentes abordagens e métodos para avaliar a qualidade da tradução, incluindo\n",
            "avaliação humana, métricas automáticas e avaliação baseada em contexto. - A utilização de técnicas de processamento de linguagem natural, como modelos de linguagem neural, para melhorar a qualidade da\n",
            "tradução automática. - A influência do contexto na avaliação da qualidade da tradução, tanto em nível de sentença quanto em nível de documento. - O uso de recursos linguísticos, como embeddings de\n",
            "palavras, na pós-edição automática de traduções. - A importância da concordância entre os avaliadores humanos na avaliação da qualidade da tradução. - A análise de erros de tradução e a identificação\n",
            "de características específicas da pós-edição de traduções automáticas. - A comparação entre diferentes métricas de avaliação da qualidade da tradução, como BLEU, chrF e ROUGE. - O desenvolvimento de\n",
            "modelos de avaliação da qualidade da tradução baseados em aprendizado de máquina, como o COMET e o BLEURT. - A busca pela paridade humana na tradução automática e os desafios associados a essa meta. -\n",
            "A evolução da tradução automática ao longo dos anos e os avanços na área, como a utilização de redes neurais e o desenvolvimento do sistema Moses.  Esses são apenas alguns dos tópicos abordados no\n",
            "livro, que oferece uma visão abrangente e atualizada do campo do processamento de linguagem natural e da avaliação da qualidade da tradução.\n",
            "\n",
            "\n",
            "Resumo do trecho 6 :\n",
            "O livro aborda o tema do Processamento de Linguagem Natural (PLN) e apresenta diversos conceitos e aplicações relacionados à área. O capítulo menciona a importância da Tradução Automática (TA) e\n",
            "destaca a necessidade de avaliar a qualidade da tradução, tanto feita por humanos quanto por máquinas.  Uma das métricas de qualidade mencionadas é o BARTScore, que avalia a qualidade do texto gerado\n",
            "como uma tarefa de geração de texto. Outra métrica mencionada é o BERTScore, que avalia a qualidade do texto gerado usando o modelo BERT.  O capítulo também discute o uso de modelos de tradução\n",
            "neural, como o sistema de tradução neural do Google, que utiliza uma abordagem baseada em redes neurais. Além disso, são mencionadas outras abordagens, como a tradução baseada em regras e a tradução\n",
            "estatística.  No contexto da TA estatística, é mencionada a tarefa de alinhamento, que consiste em encontrar as correspondências entre o texto-fonte e o texto-alvo. Também são mencionadas ferramentas\n",
            "como o Moses, que é um sistema de tradução estatística amplamente utilizado.  O capítulo destaca a importância de avaliar a qualidade da tradução automática e apresenta diferentes abordagens para\n",
            "realizar essa avaliação. São mencionados métodos de avaliação automática, como a comparação com traduções de referência e o uso de métricas de qualidade.  Além disso, são mencionados métodos de\n",
            "avaliação humana, como a escala Likert, em que os avaliadores atribuem níveis de concordância ou discordância a afirmações sobre a qualidade da tradução. Também são mencionados métodos de avaliação\n",
            "por pares, em que os avaliadores comparam diferentes traduções e atribuem uma pontuação de qualidade.  O capítulo também menciona a importância de contar com um grande número de avaliadores para obter\n",
            "resultados mais confiáveis. São mencionadas plataformas como o Mechanical Turk, que permitem contratar avaliadores para realizar tarefas de avaliação.  Por fim, o capítulo destaca a importância de\n",
            "avanços recentes na área de tradução automática, como o uso de modelos de linguagem pré-treinados e o uso de técnicas de aprendizado de máquina avançadas. São mencionados projetos como o MUSE, que\n",
            "visa melhorar a qualidade da tradução automática usando técnicas de aprendizado de máquina.  Em resumo, o capítulo aborda diferentes aspectos relacionados à qualidade da tradução automática,\n",
            "apresentando métricas de qualidade, técnicas de avaliação e avanços recentes na área.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Funções para a tarefa de Similaridade de Textos**"
      ],
      "metadata": {
        "id": "dldTk4Ht5clr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definição da função para dividir um texto em chunks e obter embeddings utilizando OpenAIEmbeddings.\n",
        "def splitter_OpenAIEmbeddings(text):\n",
        "    # Configuração do text splitter para separar o texto por quebras de linha.\n",
        "    text_splitter = CharacterTextSplitter(\n",
        "        separator=\"\\n\",\n",
        "        chunk_size=2500,\n",
        "        chunk_overlap=200,\n",
        "        length_function=len\n",
        "    )\n",
        "\n",
        "    # Divisão do texto em chunks.\n",
        "    chunks = text_splitter.split_text(text)\n",
        "\n",
        "    # Configuração e criação de embeddings utilizando OpenAIEmbeddings.\n",
        "    embeddings = OpenAIEmbeddings(\n",
        "        request_timeout=180,\n",
        "        deployment='text-embedding-ada-002',\n",
        "        chunk_size=10,\n",
        "        openai_api_key=openai.api_key,\n",
        "        show_progress_bar=True\n",
        "    )\n",
        "\n",
        "    return chunks, embeddings"
      ],
      "metadata": {
        "id": "rXONN4k-c5gF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definição da função para buscar trechos similares na base de conhecimento do Capítulo 8.\n",
        "def similarty_base_cap_8(query_user, k_user, chunks_8, embeddings_8, knowledge_base_8):\n",
        "    # Obtém a representação vetorial da consulta do usuário.\n",
        "    query = embeddings_8.embed_query(query_user)\n",
        "\n",
        "    # Realiza a busca por similaridade na base de conhecimento do Capítulo 8.\n",
        "    docs = knowledge_base_8.similarity_search_with_score_by_vector(query, k=k_user)\n",
        "\n",
        "    # Lista para armazenar os resultados em formato JSON.\n",
        "    json_data = []\n",
        "    context = ''\n",
        "\n",
        "    # Itera sobre os documentos retornados.\n",
        "    for index, document in enumerate(docs):\n",
        "        data = {\n",
        "            \"indice\": index,\n",
        "            \"page_content\": document[0].page_content,\n",
        "            \"score\": float(document[1])\n",
        "        }\n",
        "        json_data.append(data)\n",
        "\n",
        "    # Ordena os dados por score em ordem decrescente.\n",
        "    sorted_data = sorted(json_data, key=lambda x: x[\"score\"], reverse=True)\n",
        "\n",
        "    print(\"\\nOs trechos com a semelhança são os seguintes:\\n\\n\")\n",
        "\n",
        "    # Imprime os trechos similares ordenados por score.\n",
        "    for i in sorted_data:\n",
        "        print(f\"O Trecho {i['indice']} com o score {i['score']} com o seguinte conteúdo:\\n\\n{i['page_content']}\")\n",
        "        print('\\n\\n')"
      ],
      "metadata": {
        "id": "q5gk3XRkzENO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definição da função para buscar trechos similares na base de conhecimento do Capítulo 18.\n",
        "def similarty_base_cap_18(query_user, k_user, chunks_18, embeddings_18, knowledge_base_18):\n",
        "    # Obtém a representação vetorial da consulta do usuário.\n",
        "    query = embeddings_18.embed_query(query_user)\n",
        "\n",
        "    # Realiza a busca por similaridade na base de conhecimento do Capítulo 18.\n",
        "    docs = knowledge_base_18.similarity_search_with_score_by_vector(query, k=k_user)\n",
        "\n",
        "    # Lista para armazenar os resultados em formato JSON.\n",
        "    json_data = []\n",
        "    context = ''\n",
        "\n",
        "    # Itera sobre os documentos retornados.\n",
        "    for index, document in enumerate(docs):\n",
        "        data = {\n",
        "            \"indice\": index,\n",
        "            \"page_content\": document[0].page_content,\n",
        "            \"score\": float(document[1])\n",
        "        }\n",
        "        json_data.append(data)\n",
        "\n",
        "    # Ordena os dados por score em ordem decrescente.\n",
        "    sorted_data = sorted(json_data, key=lambda x: x[\"score\"], reverse=True)\n",
        "\n",
        "    print(f\"\\nOs trechos com a semelhança com o texto ({query_user}) são os seguintes:\\n\\n\")\n",
        "\n",
        "    # Imprime os trechos similares ordenados por score.\n",
        "    for i in sorted_data:\n",
        "        print(f\"O Trecho {i['indice']} com o score {i['score']} com o seguinte conteúdo:\\n\\n{i['page_content']}\")\n",
        "        print('\\n\\n')"
      ],
      "metadata": {
        "id": "42G17p8w1_gz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A chamada OpenAIEmbeddings.update_forward_refs() é usada para atualizar as referências internas da classe OpenAIEmbeddings.\n",
        "# Isso pode ser útil em situações específicas, como resolver dependências cíclicas entre classes ou garantir que as referências estejam corretas após mudanças em tempo de execução.\n",
        "# No entanto, é importante revisar a documentação ou contexto específico do código para entender completamente o motivo dessa chamada.\n",
        "OpenAIEmbeddings.update_forward_refs()"
      ],
      "metadata": {
        "id": "-axUbwKCT9Pn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Criação de base de dados para a tarefa de Similaridade de Textos**"
      ],
      "metadata": {
        "id": "n1jLmaf45x0B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtendo chunks e embeddings para o Capítulo 8 utilizando a função splitter_OpenAIEmbeddings.\n",
        "chunks_8, embeddings_8 = splitter_OpenAIEmbeddings(texto[0]['textoOffNormalize'])\n",
        "\n",
        "# Criando a base de conhecimento FAISS a partir dos chunks e embeddings do Capítulo 8.\n",
        "knowledge_base_8 = FAISS.from_texts(chunks_8, embeddings_8)\n"
      ],
      "metadata": {
        "id": "bVq9goM9mVdj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "22bc86fb47924e77937b06939c2117a5",
            "90494973e7f340cb80dd05d2d01d8b54",
            "dacea7ee31fd465887130a52c5b7e33b",
            "fda89f2561a34dbd8cfc1adcc28d7644",
            "f7ab490aefaf467e83d1fb9d4bb0cd16",
            "7d0debbefa24481db0cbb9b5f8f24ae2",
            "20d76e4bcb5f4c0e86828d862be76a70",
            "676282eb36dd41ab9925aef557224cd5",
            "979a4b6d55674c6699873c73d13c1fd9",
            "a291e45ef6204c42a2f77994483cdc72",
            "b5e1dd43f74b4a25b0a2813bc2a933ab"
          ]
        },
        "outputId": "f5ed2eac-c483-454b-ffd9-59f4427bc330"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "22bc86fb47924e77937b06939c2117a5"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtendo chunks e embeddings para o Capítulo 18 utilizando a função splitter_OpenAIEmbeddings.\n",
        "chunks_18, embeddings_18 = splitter_OpenAIEmbeddings(texto[1]['textoOffNormalize'])\n",
        "\n",
        "# Criando a base de conhecimento FAISS a partir dos chunks e embeddings do Capítulo 18.\n",
        "knowledge_base_18 = FAISS.from_texts(chunks_18, embeddings_18)"
      ],
      "metadata": {
        "id": "JYoHilyEmeTb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295,
          "referenced_widgets": [
            "122b83314ad14697b70563c99322c610",
            "03f21dd58db24f169169aa9f8b051124",
            "1ca0eae33f4e44b1bd13e35093a2dfb2",
            "9cf98ca9063e4c6d979f9654e36f16f0",
            "3d42d3ce9e35434a8ff28f6c5b82103a",
            "ca33f5667d064433b7906c3723dc0e26",
            "7f391c1fd72f4dba8bfa4a80c6788113",
            "d103e94fb16842bd91d99e645113bfb3",
            "19a4f31f5f144e208e86149268643bd5",
            "869d90263a7047a69a07183fc35fcc37",
            "e99c76528d9f41859927361c25fd2156"
          ]
        },
        "outputId": "e1c74638-13b1-4b3d-f354-45cbef78989b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "122b83314ad14697b70563c99322c610"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for text-embedding-ada-002 in organization org-IKZseHMKoaK872iSQSu3caB8 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for text-embedding-ada-002 in organization org-IKZseHMKoaK872iSQSu3caB8 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for text-embedding-ada-002 in organization org-IKZseHMKoaK872iSQSu3caB8 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for text-embedding-ada-002 in organization org-IKZseHMKoaK872iSQSu3caB8 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for text-embedding-ada-002 in organization org-IKZseHMKoaK872iSQSu3caB8 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for text-embedding-ada-002 in organization org-IKZseHMKoaK872iSQSu3caB8 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for text-embedding-ada-002 in organization org-IKZseHMKoaK872iSQSu3caB8 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for text-embedding-ada-002 in organization org-IKZseHMKoaK872iSQSu3caB8 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for text-embedding-ada-002 in organization org-IKZseHMKoaK872iSQSu3caB8 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for text-embedding-ada-002 in organization org-IKZseHMKoaK872iSQSu3caB8 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for text-embedding-ada-002 in organization org-IKZseHMKoaK872iSQSu3caB8 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for text-embedding-ada-002 in organization org-IKZseHMKoaK872iSQSu3caB8 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for text-embedding-ada-002 in organization org-IKZseHMKoaK872iSQSu3caB8 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Aplicação da Tarefa de Similaridade de Textos**"
      ],
      "metadata": {
        "id": "sP9i4BnP5516"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop principal para interação contínua com o usuário.\n",
        "while True:\n",
        "    # Solicitação para digitar o número do capítulo desejado.\n",
        "    entrada_usuario = int(input(\"Digite qual capítulo você quer recuperar informação: \"))\n",
        "\n",
        "    # Verifica se o número do capítulo é válido.\n",
        "    if entrada_usuario == 8 or entrada_usuario == 18:\n",
        "        # Solicitação para digitar a informação desejada.\n",
        "        informacao = input(\"\\nDigite qual informação quer recuperar: \")\n",
        "\n",
        "        # Solicitação para informar a quantidade de trechos desejada.\n",
        "        trechos = int(input(\"\\nInforme a quantidade de informações que quer recuperar: \"))\n",
        "\n",
        "        # Verifica qual capítulo foi escolhido e chama a função correspondente.\n",
        "        if entrada_usuario == 8:\n",
        "            similarty_base_cap_8(informacao, trechos, chunks_8, embeddings_8, knowledge_base_8)\n",
        "        elif entrada_usuario == 18:\n",
        "            similarty_base_cap_18(informacao, trechos, chunks_18, embeddings_18, knowledge_base_18)\n",
        "\n",
        "        # Solicitação para digitar 0 para sair ou qualquer outra tecla para continuar.\n",
        "        sair_ou_ficar = int(input(\"\\nDigite 0 para sair ou qualquer tecla para continuar: \"))\n",
        "\n",
        "        if sair_ou_ficar == 0:\n",
        "            break\n",
        "    else:\n",
        "        print(\"\\nVocê digitou um número de capítulo inválido na base de dados.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "abad2a3d42404651ac42cdde9d9848f1",
            "2f4911dce5f64f32842291524e3bfdf1",
            "b55a988e4ad04442b278444509724531",
            "ea6f94aa5e8149f9b1c5640f9a29ce47",
            "8f3c9781566a49a0b43271addb418a27",
            "58105d38560e47cfa329015e5f5e8693",
            "b860beafd56a42c4817392a725b4ea3a",
            "bbad4ef134094b14b907b3d8541c2e00",
            "538debd46ffa4890a1e59000d95589a8",
            "74be88abe6ed4c61a2be56f268436375",
            "ee1471925c534a4da28405d2df144d4b",
            "c6f4762a30904c8d9a3d53c88a129732",
            "0dd37bd71fea47409af806d2bb271183",
            "e8d5440ddaf54122b4b3c97957be705b",
            "ef31a2c9646d477996d054c3a7b4db88",
            "6297ae9c540c49c48960843cf65afbd7",
            "8cf419864fee42cabef2977f8e0b6ad7",
            "5ead95d23be848cdac099d1b5ecfddb1",
            "7779d3c9cbe64f6cb31ab2b487b4d34a",
            "62cdb5019ab34d4a95284ef6fbda73af",
            "23c024a2f0db485f8dd287e7c608f319",
            "ef4a50c0f9cd4a32957ddb0648c87349"
          ]
        },
        "id": "pSmmv4Z-vcm1",
        "outputId": "ce920a1e-53c5-4fe1-a51b-4b4858c9e8a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Digite qual capitulo você quer recuperar informação: 1\n",
            "\n",
            "Digitou um capitulo inválido na base de dados\n",
            "Digite qual capitulo você quer recuperar informação: 8\n",
            "\n",
            "Digite qual informação quer recuperar: o que é pnl\n",
            "\n",
            "Informe quantidade de informações que quer recuperar: 3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "abad2a3d42404651ac42cdde9d9848f1"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Os trechos com a semelhança são os seguintes:\n",
            "\n",
            "\n",
            "O Trecho 2 com o score 0.6119176149368286 com o seguinte contéudo:\n",
            "\n",
            " Entre as técnicas simbólicas e as representações distribuídas existem ainda os datasets (ou corpora) com anotação semântica (veja Capítulo 14), uma terceira maneira de lidar com o significado no PLN. Se, por um lado, tais datasets se alinham às abordagens probabilísticas, uma vez que podem ser usados como fonte para o aprendizado de máquina (para o aprendizado do significado), por outro lado, a atividade de anotação de significado se alinha às abordagens representacionistas. Neste tipo de anotação (também chamada de anotação de word senses), cada palavra (ou segmento de texto) é anotada com informação relativa ao significado de acordo com o contexto específico em que aparece no corpus. A informação relativa ao significado, por sua vez, vem de fontes externas (como dicionários, wordnets, verbnets e framenets) e a tarefa de anotação pode ser descrita como um trabalho de desambiguação, pois consistiria em selecionar, dentre os vários sentidos possíveis de uma palavra, aquele usado no contexto da frase. O que a anotação faz, deste modo, é criar uma representação estável entre a palavra e o seu significado, no contexto em que está sendo usada. Cada ocorrência de uma palavra poderá estar associada a um significado diferente (e aqui vemos uma aproximação com abordagens pragmáticas), desde que este significado esteja presente no inventário de significados usado na anotação (aqui vemos uma aproximação com abordagens representacionistas). Para as pessoas responsáveis pela anotação, a principal dificuldade está na escolha do sentido adequado conforme o contexto, uma vez que os sentidos frequentemente se sobrepõem, como as ondas observadas por Palomar.\n",
            "Por exemplo, tomando a palavra “trabalho” destacada no parágrafo anterior, a tarefa consiste em escolher, dentre opções listadas no quadro abaixo, retiradas do dicionário Caldas-Aulete online9, aquela adequada ao contexto (se a anotação usasse o inventário de uma wordnet como fonte, o inventário de significados poderia ser diferente10).\n",
            "Quadro 8.2 Acepções da palavra trabalho conforme dicionário\n",
            "Emprego da força física ou intelectual para realizar alguma coisa\n",
            "Aplicação dessas forças como ocupação profissional: Seu trabalho é de gari.\n",
            "Local onde isso se realiza: Mora longe do trabalho.\n",
            "Esmero, cuidado que se emprega na confecção ou elaboração de uma obra\n",
            "A confecção, elaboração ou composição de uma obra\n",
            "Obra realizada: Essa cômoda é um belo trabalho de marcenaria.\n",
            "Grande esforço; TRABALHÃO; TRABALHEIRA\n",
            "\n",
            "\n",
            "\n",
            "O Trecho 1 com o score 0.6075577139854431 com o seguinte contéudo:\n",
            "\n",
            " Contudo, o senhor Palomar não perde o ânimo e a cada momento acredita haver conseguido observar tudo o que poderia ver de seu ponto de observação, mas sempre ocorre alguma coisa que não tinha levado em conta. Prestar atenção em um aspecto faz com que este salte para o primeiro plano, invadindo o quadro, como em certos desenhos diante dos quais basta fecharmos os olhos e ao reabri-los a perspectiva já mudou. (…).\n",
            "O vento estaria mudando? É pena que a imagem que o senhor Palomar havia conseguido organizar com tanta minúcia agora se desfigure, se fragmente e se perca. Só conseguindo manter presentes todos os aspectos juntos, ele poderia iniciar a segunda fase da operação: estender esse conhecimento a todo o universo.\n",
            "Bastaria não perder a paciência, coisa que não tarda a acontecer. O senhor Palomar afasta-se ao longo da praia, com os nervos tensos como havia chegado e ainda mais inseguro de tudo.\n",
            "CALVINO, Ítalo. Palomar. São Paulo: Companhia das Letras, 1994. p.7-11.\n",
            "O que vemos, por trás da simplória tarefa de observação de uma única onda, é a dificuldade de Palomar diante de um objeto que se transforma continuamente durante a própria atividade de observação. Ainda que Palomar defina, de modo preciso, seu objetivo e seu objeto (“observar uma simples onda e pronto”) e busque uma abrangência descritiva (“Colher todos os seus componentes simultâneos sem descurar de nenhum”), é difícil, na observação, isolar o objeto de suas “adjacências”, reduzir as diferentes instâncias do objeto a uma essência comum (“sempre ocorre alguma coisa que não tinha levado em conta”), controlar a subjetividade, suspender as pressões externas (“bastaria não perder a paciência”), encontrar o ponto de vista superior ou ideal (“Foi uma dessas línguas baixas de areia que o sr. Palomar escolheu como ponto de observação”). Enfim, definitivamente, Palomar não é bem sucedido em sua empreitada, por mais simples que esta parecesse inicialmente.\n",
            "\n",
            "\n",
            "\n",
            "O Trecho 0 com o score 0.551091194152832 com o seguinte contéudo:\n",
            "\n",
            " O fato de representações distribuídas terem levado a resultados positivos no PLN não deve ser visto como argumento contrário às técnicas simbólicas. São maneiras diferentes de lidar com o sentido das palavras. Como tirar o melhor proveito destas diferentes visões e abordagens, no PLN, é uma das questões que se coloca. O que temos visto é a limitação de cada uma delas, tomada individualmente. Se consideramos o significado como uma entidade estável, como lidar com as mudanças, que inclusive podem ser capturadas pelos dicionários (dicionários, recentemente, mudaram a definição da palavra “família”12)? Por outro lado, se consideramos a instabilidade e a dependência dos dados, como evitar vieses indesejados, como a associação entre os sentidos, por exemplo, de “paraguaio” e “de baixa qualidade”, quando dizemos “uísque paraguaio”?\n",
            "Os próximos capítulos aprofundam cada uma dessas maneiras de trabalhar com o significado no PLN.\n",
            "BAKER, C.; FELLBAUM, C.; PASSONNEAU, R. Semantic Annotation of MASC. Em: Handbook of Linguistic Annotation. [s.l.] Springer Netherlands, 2017. p. 699–717.\n",
            "BREWSTER, C.; WILKS, Y. Ontologies, taxonomies, thesauri:learning from texts. (M. Deegan, Ed.)Proceedings of Use of Computational Linguistics in the Extraction of Keyword Information from Digital Library Content Workshop. Anais...2004. Disponível em: <http://www.cbrewster.com/papers/KeyWord_FMO.pdf>\n",
            "FREITAS, C. Linguística Computacional. [s.l.] Editora Parábola, 2022.\n",
            "ILARI, R.; GERALDI, J. W. Semântica. [s.l.] Ética, 1985.\n",
            "JURAFSKY, D.; MARTIN, J. H. Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition. 3rd. ed. USA: Prentice Hall PTR, 2023.\n",
            "KILGARRIFF, A. I Don’t Believe in Word Senses. Computers and the Humanities, 1997.\n",
            "KILGARRIFF, A. Thesauruses for Natural Language Processing. Proceedings of Natural Language Processing and Knowledge Engineering. Anais...2003. Disponível em: <https://www.kilgarriff.co.uk/Publications/2003-K-Beijing-thes4NLP.pdf>\n",
            "MARTINS, H. Sobre a estabilidade do significado em Wittgenstein. Veredas, v. 4, n. 2, p. 19–42, 2000.\n",
            "MARTINS, H. Três Caminhos na Filosofia da Linguagem. Em: Introdução à Linguística. Volume III. [s.l.] Editora Cortez, 2004.\n",
            "MITKOV, R. The Oxford handbook of Computational Linguistics. [s.l.] Oxford University Press, 2003.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Digite 0 para sair ou qualquer tecla para continuar: 1\n",
            "Digite qual capitulo você quer recuperar informação: 18\n",
            "\n",
            "Digite qual informação quer recuperar: o que é pnl\n",
            "\n",
            "Informe quantidade de informações que quer recuperar: 5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c6f4762a30904c8d9a3d53c88a129732"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Os trechos com a semelhança com o texto(o que é pnl) são os seguintes:\n",
            "\n",
            "\n",
            "O Trecho 4 com o score 0.6026187539100647 com o seguinte contéudo:\n",
            "\n",
            " A adoção de taxonomias de erros na ATA oferece diversos benefícios, tais como identificar tipos específicos de erros nas saídas de TA, fornecer relatórios detalhados de erros para o aprimoramento dos sistemas, e fornecer informações aos clientes sobre a qualidade da tradução. Além disso, provedores de serviços linguísticos utilizam taxonomias e avaliações de severidade29 para monitorar o trabalho de tradutores. A anotação de erro também ajuda a investigar as relações entre tipos específicos de erros e as preferências de usuários ou pós-editores, bem como avaliar o impacto de diferentes tipos de erros em várias etapas do processo de pós-edição.\n",
            "Contudo, entre as principais limitações dessa estratégia, destaca-se que a anotação manual de erros é um processo caro e demorado, demandando um investimento significativo de tempo. Além disso, essa avaliação nem sempre é uma tarefa simples, especialmente quando se trata de diferenciar entre categorias como “tradução literal” e “tradução errada”.\n",
            "Outra complexidade está associada à dependência da língua. Diferentes idiomas possuem particularidades que podem tornar a identificação e a classificação de erros uma tarefa mais desafiadora. Também é relevante considerar que a eficácia da anotação de erros pode variar de acordo com o tipo de abordagem de TA: enquanto ela pode ser mais adequada para sistemas de tradução baseados em regras (RBMT), pode não ser tão precisa para sistemas de tradução estatística (SMT) ou de tradução neural (NMT). Nesse contexto, a seleção da abordagem de avaliação mais adequada torna-se um ponto de reflexão. Além disso, a falta de consenso entre avaliadores é uma questão importante, frequentemente requerendo treinamento e prática para alcançar um nível satisfatório de concordância (Capítulo 14).\n",
            "18.3.4.5 Pós-Edição na Avaliação de TA\n",
            "A pós-edição (PE) (do inglês, post-editing) é definida como “a correção da saída da tradução automática bruta por um tradutor humano, de acordo com instruções e critérios de qualidade específicos” (O’Brien, 2011, p. 197). A PE emerge como uma ferramenta fundamental na avaliação de sistemas de TA, oferecendo uma perspectiva mais aprofundada sobre o esforço envolvido nesse processo. A medição desse esforço pode ser abordada de diferentes perspectivas (Krings, 2001), proporcionando uma visão mais abrangente do desempenho do sistema e do impacto da TA no fluxo de trabalho.\n",
            "\n",
            "\n",
            "\n",
            "O Trecho 3 com o score 0.5864198207855225 com o seguinte contéudo:\n",
            "\n",
            " Nesse momento, os Transformers (Vaswani et al., 2017) são o estado da arte na tradução. A Figura 18.4 ilustra a tradução da sentença de exemplo, em português, para inglês usando Transformers25. Nessa ilustração, quanto mais clara (amarelo, verde claro, azul claro etc.) a célula que une a linha da palavra em inglês com a coluna da palavra em português, maior a “força” da relação entre elas. Por exemplo, observa-se uma forte relação entre “my” e “meu”, “grandfather” e “avô”, “home” e “casa”, e “beautiful” e “linda”.\n",
            "Figura 18.4: Visualização de um modelo de atenção usado para traduzir a sentença de exemplo.\n",
            "Contudo, assim como todas as demais abordagens, a tradução neural também tem suas limitações. Uma delas é que, diferentemente da PBSMT onde é possível “olhar” para os modelos aprendidos e entender o que foi usado na tradução (como as frases da Tabela 18.1), a tradução neural é considerada uma caixa-preta (black box): entender o que pode ter sido considerado para gerar a tradução depende de desvendar uma visualização do modelo de atenção (como o da Figura 18.4), já que as previsões dos modelos neurais consistem em milhões de parâmetros. Isso dificulta a extensão dos modelos previamente treinados e coloca em dúvida a robustez do sistema. Além disso, por ser uma abordagem relativamente nova, a tradução neural ainda enfrenta alguns desafios, como o desempenho ruim em condições fora do domínio e para idiomas com recursos limitados.\n",
            "Além disso, é possível observar que as estratégias de tradução baseadas em corpus são fortemente influenciadas pelo corpus usado no treinamento. Por exemplo, os modelos estatísticos só terão a capacidade de traduzir uma palavra se ela tiver ocorrido um número significativo de vezes no corpus de treinamento, caso contrário não haverá uma frase correspondente contendo essa palavra e o sistema não conseguirá gerar uma tradução completa para a sentença original. No caso da tradução neural, isso é um pouco amenizado pelo uso de embeddings de unidades menores (em inglês, subword units) do que as palavras, as quais conseguem aproximar palavras desconhecidas às possíveis correspondências conhecidas26. Por exemplo, se o “a” for esquecido no “linda” da Figura 18.1 o Google tradutor consegue gerar a mesma saída, sem problema. O tratamento de palavras e subwords é abordado no Capítulo 4.\n",
            "\n",
            "\n",
            "\n",
            "O Trecho 2 com o score 0.57916259765625 com o seguinte contéudo:\n",
            "\n",
            " Em 2007, o sistema open-source PBSMT mais famoso, desenvolvido por Koehn et al. (2007), foi lançado: o Moses SMT toolkit. Ao mesmo tempo, o Google lançou seu famoso Google Tradutor com abordagens estatísticas. Vale ressaltar que os modelos estatísticos conseguiram obter grande sucesso devido ao “aumento do poder de computação e armazenamento de dados, juntamente com a disponibilidade cada vez maior de recursos de texto digital como consequência do crescimento da Internet” (Koehn, 2009, p. 18). Devido à eficiência e precisão da abordagem estatística em relação às anteriores, ela se tornou a abordagem mais amplamente utilizada naquela época. Sistemas de tradução estatística baseada em frases (PBSMT) como os de Koehn; Och; Marcu (2003) e Och; Ney (2004) eram o estado da arte até serem sucedidos pela tradução neural, a partir de 2015. De fato, a estratégia por trás do tradutor automático do Google foi a PBSMT por uma década (aproximadamente de 2006/2007 até 2016/2017)18. Atualmente, o Google e praticamente todos os sistemas de tradução online, bem como pesquisas nesta área usam a tradução neural (neural machine translation, NMT) ou algum sistema híbrido (estatístico e neural).\n",
            "18.2.6 Tradução Automática Neural\n",
            "Os sistemas de TA neural (em inglês, Neural Machine Translation ou NMT) foram introduzidos pela primeira vez na década de 1990 com alguns artigos sugerindo como redes neurais poderiam ser usadas para TA19 (Way; Forcada, 2018). No entanto, a quantidade dos dados usados para treinar esses modelos não era suficiente para produzir resultados razoáveis e, além disso, “a complexidade computacional envolvida excedia em muito os recursos computacionais daquela época, e, portanto, a ideia foi abandonada por quase duas décadas” (Koehn, 2020, p. 39).\n",
            "Em geral, os modelos neurais consistem na construção de redes neurais end-to-end que mapeiam textos paralelos alinhados e são treinados para maximizar a probabilidade de uma sequência alvo Y, dada uma frase de origem X, sem informações linguísticas externas adicionais (Castilho et al., 2017b). Os sistemas neurais podem ser construídos com apenas uma rede em vez de uma sequência de tarefas separadas, como seu predecessor (a tradução estatística).\n",
            "\n",
            "\n",
            "\n",
            "O Trecho 1 com o score 0.5760170221328735 com o seguinte contéudo:\n",
            "\n",
            " Com a publicação de resultados impressionantes em avaliação automática (Bahdanau; Cho; Bengio, 2015; Bojar et al., 2016; Sennrich; Haddow; Birch, 2016), os sistemas neurais geraram grande expectativa, especialmente porque a indústria de tradução busca melhorar a qualidade da TA para minimizar custos (Moorkens, 2017). A adoção dos sistemas neurais nos últimos anos tem sido extensiva, com um número crescente de provedores de TA e grupos de pesquisa concentrando seus esforços e recursos no desenvolvimento e implantação de sistemas NMT (Castilho et al., 2019).\n",
            "Na tradução neural, redes neurais artificiais são usadas para fazer a tradução de uma sentença-fonte para uma sentença-alvo. Uma rede neural artificial pode ser entendida como uma composição de diversas unidades de processamento (os neurônios artificiais) conectadas entre si, em camadas. Cada unidade de processamento recebe uma entrada numérica e gera uma saída numérica. A saída é calculada de acordo com os “pesos” (\\(w\\)) e as “entradas” (\\(x\\)) associados à unidade e uma função que determina como a saída deve ser calculada. Por exemplo, vamos supor que um neurônio artificial seja governado pela função \\(x^2\\). Nesse caso, se a entrada para esse neurônio for o número \\(2\\) então a saída será \\(4\\), se for \\(3\\) a saída será \\(9\\), se for \\(-1\\) a saída será \\(1\\) e assim por diante. Os pesos são usados para ajustar o aprendizado do neurônio e são uma das partes mais importantes da definição de uma rede neural artificial.\n",
            "Na tradução neural, diversas camadas de neurônios são usadas para aprender como traduzir uma sentença-fonte em uma sentença-alvo a partir de um corpus paralelo. Esse aprendizado geralmente demanda muito poder computacional20 e tempo, pois é realizado a passos pequenos em diversos ciclos de processamento21 das sentenças paralelas. E qual é a principal diferença metodológica da tradução neural para a estatística? Na NMT toda a sentença-fonte é considerada no aprendizado, de uma vez, e nos dois sentidos (da esquerda para a direita e da direita para a esquerda), ou seja, não há a quebra em frases como ocorria na PBSMT, nem a divisão clara entre modelo de tradução e modelo de língua. Dessa forma, a tradução gerada por um sistema NMT tende a ser mais fluente e natural, como ilustrado na Figura 18.122.\n",
            "Figura 18.1: Tradução gerada pelo Google tradutor (tradução neural)\n",
            "\n",
            "\n",
            "\n",
            "O Trecho 0 com o score 0.5510388612747192 com o seguinte contéudo:\n",
            "\n",
            " Figura 18.1: Tradução gerada pelo Google tradutor (tradução neural)\n",
            "Os modelos de tradução neural baseiam-se fortemente em duas tecnologias que se tornaram bastante usuais em PLN: embeddings e modelo de atenção. As embeddings (Capítulo 10), são formas de representação de unidades lexicais (geralmente palavras) nas quais as unidades são mapeadas para vetores em um espaço de n (100, 300 ou mais) dimensões. Ao representar palavras como vetores densos notou-se que é possível mapear características linguísticas (morfológicas, sintáticas e semânticas) nesse espaço vetorial. Por exemplo, na Figura 18.2 é possível observar a proximidade semântica da palavra “avô” com outras palavras a partir das word embeddings do NILC23 como “pai”, “tio”, “sobrinho” etc.\n",
            "Figura 18.2: Vizinhos mais próximos da palavra “avô” obtidos via consulta às word embeddings do NILC geradas usando o GloVe e dimensão 300.\n",
            "Usando as word embeddings bilíngues português e inglês do MUSE24 é possível observar as similaridades entre línguas como ilustrado na Figura 18.3, onde as palavras em português aparecem em vermelho e as palavras em inglês, em azul. Essas word embeddings são usadas como forma de representação da língua nos modelos neurais de tradução automática.\n",
            "Figura 18.3: Visualização, em duas dimensões, das palavras em português (em vermelho) das palavras que ocorrem na sentença de exemplo e algumas possíveis traduções para o inglês (em azul).\n",
            "Assim, a tradução neural não se baseia na combinação dos modelos de tradução e de língua, como faz a tradução estatística, mas sim em um modelo sequencial que prediz uma palavra por vez. O potencial deste modelo sequencial está na maneira como ele prediz as palavras: considerando toda a sentença-fonte e também o que já foi produzido para a sentença-alvo. Desde sua proposição, a modelagem sequencial neural passou por várias arquiteturas, indo desde as redes neurais recorrentes (em inglês, recurrent neural network ou RNN) usadas para codificação (em inglês, encoder) e decodificação (em inglês, decoder) até os mecanismos de atenção (em inglês, attention mechanism) (Bahdanau; Cho; Bengio, 2015) que permitem ao decodificador focar em partes específicas da sentença de entrada em seu processo de geração da saída.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Digite 0 para sair ou qualquer tecla para continuar: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Aplicação da Tarefa de Sistemas de Perguntas e Respostas em cima dos capitulos 8 e 18 selecionados**"
      ],
      "metadata": {
        "id": "_HJuBih76UTL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definição da função para dividir um texto em chunks e obter embeddings utilizando OpenAIEmbeddings.\n",
        "def splitter_OpenAIEmbeddings(text):\n",
        "    # Configuração do text splitter para separar o texto por quebras de linha.\n",
        "    text_splitter = CharacterTextSplitter(\n",
        "        separator=\"\\n\",\n",
        "        chunk_size=2500,\n",
        "        chunk_overlap=200,\n",
        "        length_function=len\n",
        "    )\n",
        "\n",
        "    # Divisão do texto em chunks.\n",
        "    chunks = text_splitter.split_text(text)\n",
        "\n",
        "    # Configuração e criação de embeddings utilizando OpenAIEmbeddings.\n",
        "    embeddings = OpenAIEmbeddings(\n",
        "        request_timeout=180,\n",
        "        deployment='text-embedding-ada-002',\n",
        "        chunk_size=10,\n",
        "        openai_api_key=openai.api_key,\n",
        "        show_progress_bar=True\n",
        "    )\n",
        "\n",
        "    return chunks, embeddings"
      ],
      "metadata": {
        "id": "WpprOMhi7Z-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from requests.models import Response\n",
        "\n",
        "def requestOpenAI(query_user, chunks, embeddings, knowledge_base, mensagens):\n",
        "    pergunta = query_user\n",
        "    query = embeddings.embed_query(query_user)\n",
        "    docs = knowledge_base.similarity_search_with_score_by_vector(query, k=4)\n",
        "    json_data = []\n",
        "    context = ''\n",
        "\n",
        "    # Itera sobre os documentos retornados.\n",
        "    for index, document in enumerate(docs):\n",
        "        data = {\n",
        "            \"indice\": index,\n",
        "            \"page_content\": document[0].page_content,\n",
        "            \"score\": float(document[1])\n",
        "        }\n",
        "        json_data.append(data)\n",
        "        context += f'\\nTrecho {index}:\\n {document[0].page_content}'\n",
        "\n",
        "    # Template para criar o prompt com o contexto e a pergunta.\n",
        "    prompt_template = \"Use as seguintes partes dos trechos para responder à pergunta. Contexto:\\n{Context}.\\n\\nPergunta: {Pergunta}\"\n",
        "    prompt = prompt_template.replace(\"{Context}\", context).replace(\"{Pergunta}\", pergunta)\n",
        "\n",
        "    # Adiciona a mensagem do usuário com o prompt ao histórico de mensagens.\n",
        "    mensagens.append({'role': 'user', 'content': prompt})\n",
        "\n",
        "    total_tokens = 0\n",
        "\n",
        "    # Calcula o total de tokens nas mensagens para evitar ultrapassar o limite de 32.000 tokens.\n",
        "    for mensagem in mensagens:\n",
        "        tokens = mensagem['content']\n",
        "        total_tokens += len(mensagem['content'])\n",
        "\n",
        "    # Se ultrapassar o limite, remove algumas mensagens antigas.\n",
        "    if total_tokens > 32000 and len(mensagens) > 1:\n",
        "        del mensagens[1:3]\n",
        "\n",
        "    # Chama a API da OpenAI para obter a resposta.\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo-16k-0613\",\n",
        "        messages=mensagens,\n",
        "        max_tokens=1024,\n",
        "        temperature=0.4,\n",
        "    )\n",
        "\n",
        "    # Remove a última mensagem adicionada ao histórico antes de atualizar com a resposta.\n",
        "    mensagens.pop()\n",
        "    # Adiciona a pergunta do usuário e a resposta do modelo ao histórico de mensagens.\n",
        "    mensagens.append({'role': 'user', 'content': pergunta})\n",
        "    mensagens.append({'role': 'assistant', 'content': response[\"choices\"][0][\"message\"][\"content\"]})\n",
        "\n",
        "    return mensagens, response[\"choices\"][0][\"message\"][\"content\"]"
      ],
      "metadata": {
        "id": "tHXsO6Y27HsW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenando os textos normalizados dos Capítulos 8 e 18 para criar o texto base.\n",
        "texto_base = texto[0]['textoOffNormalize'] + texto[1]['textoOffNormalize']"
      ],
      "metadata": {
        "id": "4q06dyCVWhbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtendo chunks e embeddings para o texto base utilizando a função splitter_OpenAIEmbeddings.\n",
        "chunks, embeddings = splitter_OpenAIEmbeddings(texto_base)\n",
        "\n",
        "# Criando a base de conhecimento FAISS a partir dos chunks e embeddings do texto base.\n",
        "knowledge_base = FAISS.from_texts(chunks, embeddings)"
      ],
      "metadata": {
        "id": "xxmrS9cM69lZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277,
          "referenced_widgets": [
            "1203f073b78340e783587e02f5693630",
            "272a7738021c43b988e49887563de59c",
            "29b5924b04b04855a0ba0268eaba0cc9",
            "f4124d5dd78c499ea4f31b4553b20473",
            "65f071fa699a4a4ca18374c422e102a5",
            "bf34ae246ec94a599ae14ad76fb7badf",
            "693430a580b447a497abbdf030f33fc9",
            "17373778ef874fc0ba26e6643d0f0c3e",
            "87976fa2c43f4289ab233db5a6f1b61b",
            "c5181531d88b480f9c2d2631ebb6fe61",
            "9ed5da709fa342e7b9c8e2d0b82950e4"
          ]
        },
        "outputId": "12032b0d-495f-44be-af74-77f61896adcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/6 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1203f073b78340e783587e02f5693630"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for text-embedding-ada-002 in organization org-IKZseHMKoaK872iSQSu3caB8 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for text-embedding-ada-002 in organization org-IKZseHMKoaK872iSQSu3caB8 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for text-embedding-ada-002 in organization org-IKZseHMKoaK872iSQSu3caB8 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for text-embedding-ada-002 in organization org-IKZseHMKoaK872iSQSu3caB8 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for text-embedding-ada-002 in organization org-IKZseHMKoaK872iSQSu3caB8 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for text-embedding-ada-002 in organization org-IKZseHMKoaK872iSQSu3caB8 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for text-embedding-ada-002 in organization org-IKZseHMKoaK872iSQSu3caB8 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for text-embedding-ada-002 in organization org-IKZseHMKoaK872iSQSu3caB8 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for text-embedding-ada-002 in organization org-IKZseHMKoaK872iSQSu3caB8 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for text-embedding-ada-002 in organization org-IKZseHMKoaK872iSQSu3caB8 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for text-embedding-ada-002 in organization org-IKZseHMKoaK872iSQSu3caB8 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
            "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for text-embedding-ada-002 in organization org-IKZseHMKoaK872iSQSu3caB8 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sou um chatBOT que irei de ajudar sobre dúvidas sobre os capítulos 8 e 18 e sobre o tema de Processamento de Linguagem Natural. Digite 'sair' caso queira encerrar a conversa.\\n\")\n",
        "\n",
        "# Loop principal para interação contínua com o usuário.\n",
        "while True:\n",
        "    mensagens = []\n",
        "    # Mensagem do sistema explicando o papel da inteligência artificial na conversa.\n",
        "    mensagens.append({'role': 'system', 'content': \"Você é uma inteligência artificial especialista sobre o tema de Processamento de Linguagem Natural. Você receberá trechos que podem ser pertinentes à pergunta do usuário, utilize-os somente para complementar a sua resposta, mas nunca utilize somente o contexto deles. Dê prioridade ao contexto da conversa.\"})\n",
        "\n",
        "    # Solicitação para o usuário digitar sua mensagem.\n",
        "    entrada_usuario = input(\"Você: \")\n",
        "\n",
        "    # Verifica se o usuário deseja encerrar a conversa.\n",
        "    if entrada_usuario.lower().strip() == \"sair\":\n",
        "        break\n",
        "\n",
        "    total_tokens = 0\n",
        "\n",
        "    # Calcula o total de tokens nas mensagens para evitar ultrapassar o limite de 32.000 tokens.\n",
        "    for mensagem in mensagens:\n",
        "        tokens = mensagem['content']\n",
        "        total_tokens += len(mensagem['content'])\n",
        "\n",
        "    # Se ultrapassar o limite, remove algumas mensagens antigas.\n",
        "    if total_tokens > 32000 and len(mensagens) > 1:\n",
        "        del mensagens[1:3]\n",
        "\n",
        "    # Chama a função para interagir com a OpenAI e obter a resposta do bot.\n",
        "    mensagens, response_bot = requestOpenAI(entrada_usuario, chunks, embeddings, knowledge_base, mensagens)\n",
        "    print(\"\\nBot: \" + response_bot + \"\\n\")"
      ],
      "metadata": {
        "id": "LsnNb_yX6TkV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532,
          "referenced_widgets": [
            "6c42167995cf48e18a3d0f388148c7d1",
            "88c5c1cca5514f38a94195de6821413e",
            "76667aee4c184eee8b94931062a5bf5f",
            "ef78c533a8c44d7089f633ab5794dfa3",
            "200b1f0dd9ba4c338059a8f84dfb54d6",
            "4472d54a10934846a2fdff87214ba7c6",
            "8d3bf3bc33014abd8e83c719f6f7207a",
            "4435563aedf8487da222a8db99e85dce",
            "ce75788c60a54fb08ee3f49315d51ab4",
            "8c85dfc656f04c009ecfeb146dbfd4f4",
            "ab526ae921614d44865c66de619c0af6",
            "9fa4d15156764615886a15000afef4f7",
            "d8474f39315b45d2af6e8914ecad99d6",
            "0404f0d7f47245599b16ad103153bfbc",
            "723f7ff061cf4c84aa9e3a65693e81dd",
            "6e9b86f9a60443a3aa6619f9f4d993b8",
            "af672877381e4f729843af6408d3f6a3",
            "3e887487c39f48cb9fcbae55ea74448e",
            "1dcd20492f9c404093618bfd7bf1c5a3",
            "445db3bda18a4db3ace04f162f86e03d",
            "f36f13adde324d5c84af416f0aeb86e7",
            "9862070d900b4977870a858c89da8d03",
            "e8f5c6b164cd4adeac01d8c9d33b9097",
            "886f7ee48b844517a582ba969e054e07",
            "9d4a46e7c1ac483ca7c90ebb04789e0f",
            "89fb3c53d72d4d2b89163cf00383d58e",
            "632809c6b83e4d9fb988ace282fa43fd",
            "e1ad40472ffd43cfbf806fc77005d5b4",
            "242f217e45cc4cb99a2661e80973646c",
            "05045ccec19347369f5af9f1f98d5a9e",
            "c22167bbf50b4e89afc553f03ba22814",
            "a5b00bcf4c2e4f94951afa9e917748b1",
            "76407c53c741405aaf4c483e5fcf6b54"
          ]
        },
        "outputId": "e0bf994b-8e26-4fac-dfc1-42aeb4923a82"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sou um chatBOT que irei de ajudar sobre dúvidas sobre os capitulos 8 e 18 e sobre o tema de Processamento de Linguagem Natural. Digite sair, caso queria sair da conversa\n",
            "\n",
            "Você: O que é PNL ?\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6c42167995cf48e18a3d0f388148c7d1"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Bot: PNL é a sigla para Processamento de Linguagem Natural. É uma área da inteligência artificial que se dedica ao estudo e desenvolvimento de técnicas e algoritmos para que os computadores possam compreender, interpretar e gerar linguagem humana de forma natural. O objetivo do PNL é permitir que os computadores possam interagir e comunicar-se com os seres humanos de maneira mais eficiente e eficaz, utilizando a linguagem como meio de comunicação.\n",
            "\n",
            "Você: PLN é do campo de machine learning e ela engloba a tarefa de tradução de texto ?\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9fa4d15156764615886a15000afef4f7"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Bot: Sim, PLN (Processamento de Linguagem Natural) é uma área do campo de machine learning que engloba a tarefa de tradução de texto. O uso de modelos de tradução neural, que são baseados em redes neurais artificiais, é uma abordagem comum no PLN para realizar a tradução automática de uma sentença-fonte para uma sentença-alvo. Esses modelos têm sido amplamente utilizados e têm proporcionado melhorias significativas na qualidade e fluidez das traduções geradas. No entanto, a tradução neural também apresenta desafios, como a compreensão do contexto e nuances linguísticas, além de ser considerada uma caixa-preta, o que dificulta a interpretação dos processos internos do modelo.\n",
            "\n",
            "Você: então fale mais sobre a tarefa de tradução\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e8f5c6b164cd4adeac01d8c9d33b9097"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Bot: A tarefa de tradução consiste em converter um texto escrito em um idioma de origem para um texto equivalente em um idioma de destino. É uma tarefa desafiadora, pois requer o entendimento do significado e da estrutura gramatical das frases em ambos os idiomas, bem como a capacidade de expressar essas informações de forma precisa e fluente.\n",
            "\n",
            "Existem várias abordagens para a tradução automática, desde as mais simples, como a tradução direta palavra-a-palavra, até as mais avançadas, como a tradução neural baseada em redes neurais artificiais. A tradução direta envolve o mapeamento direto de palavras de um idioma para outro, sem passar por outros níveis de análise. Já a tradução neural utiliza modelos de aprendizado de máquina para aprender padrões e relações entre as palavras nos dois idiomas, permitindo uma tradução mais precisa e fluente.\n",
            "\n",
            "Além disso, existem outras abordagens intermediárias, como a tradução baseada em regras, onde o conhecimento linguístico é explicitamente mapeado em regras, e a tradução por interlíngua, que envolve a tradução para uma língua intermediária antes de chegar ao idioma de destino.\n",
            "\n",
            "A avaliação da tradução automática é uma parte importante desse processo, garantindo a qualidade e a precisão das traduções. Existem várias métricas e métodos de avaliação, que visam medir a adequação e a fluência das traduções.\n",
            "\n",
            "No geral, a tradução automática é uma área em constante evolução, impulsionada pelo avanço da tecnologia e do processamento de linguagem natural. Ela desempenha um papel fundamental na comunicação global, permitindo que pessoas de diferentes idiomas se compreendam e interajam de forma mais eficiente.\n",
            "\n",
            "Você: sair\n"
          ]
        }
      ]
    }
  ]
}